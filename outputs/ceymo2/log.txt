[10/03 22:54:03] detectron2 INFO: Rank of current process: 0. World size: 1
[10/03 22:54:04] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0]
numpy                            1.23.1
detectron2                       0.6 @/OneFormer/detectron2/detectron2
Compiler                         GCC 9.4
CUDA compiler                    CUDA 11.7
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.0.0 @/opt/miniconda3/envs/oneformer/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 4090 (arch=8.9)
Driver version                   530.41.03
CUDA_HOME                        /usr/local/cuda
Pillow                           9.4.0
torchvision                      0.15.0 @/opt/miniconda3/envs/oneformer/lib/python3.10/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.8.0
-------------------------------  -------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[10/03 22:54:04] detectron2 INFO: Command line arguments: Namespace(config_file='configs/cityscapes/convnext/mapillary_pretrain_oneformer_convnext_xlarge_bs16_90k.yaml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50165', opts=['OUTPUT_DIR', 'outputs/ceymo2', 'WANDB.NAME', 'ceymo'])
[10/03 22:54:04] detectron2 INFO: Contents of args.config_file=configs/cityscapes/convnext/mapillary_pretrain_oneformer_convnext_xlarge_bs16_90k.yaml:
[38;5;204m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m../oneformer_R50_bs16_90k.yaml[39m
[38;5;204mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;204mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mD2ConvNeXt[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;204mCONVNEXT[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mIN_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;204mDEPTHS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m3[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m3[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m27[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m3[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;204mDIMS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m256[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m512[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1024[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m2048[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;204mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;204mLSIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;204mOUT_INDICES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m0[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m3[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;204mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m/mnt/source/OneFormer/mapillary_pretrain_250_16_convnext_xl_oneformer_mapillary_300k.pth[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;204mPIXEL_MEAN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m123.675[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m116.280[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m103.530[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;204mPIXEL_STD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m58.395[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m57.120[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m57.375[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;204mONE_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m250[39m
[38;5;204mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;204mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m250[39m

[10/03 22:54:04] detectron2 INFO: Running with full config:
[38;5;204mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;204mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;204mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;204mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;204mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;204mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;204mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;204mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;204mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;204mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;204mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;204mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;204mTEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;204mTEST_INSTANCE[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mcityscapes_fine_instance_seg_val[39m
[38;5;15m  [39m[38;5;204mTEST_PANOPTIC[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mcityscapes_fine_panoptic_val[39m
[38;5;15m  [39m[38;5;204mTEST_SEMANTIC[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mcityscapes_fine_sem_seg_val[39m
[38;5;15m  [39m[38;5;204mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mcityscapes_fine_panoptic_train[39m
[38;5;204mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;204mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;204mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;204mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;204mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;204mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;204mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;204mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mabsolute[39m
[38;5;15m  [39m[38;5;204mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141moneformer_unified[39m
[38;5;15m  [39m[38;5;204mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;204mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;204mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;204mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;204mMAX_SEQ_LEN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m77[39m
[38;5;15m  [39m[38;5;204mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;204mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m  [39m[38;5;204mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;204mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;204mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m614[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m716[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m819[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m921[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1126[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1228[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1331[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1433[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1638[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1740[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1843[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1945[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;204mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;204mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;204mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m  [39m[38;5;204mTASK_PROB[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mINSTANCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.66[39m
[38;5;15m    [39m[38;5;204mSEMANTIC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.33[39m
[38;5;15m  [39m[38;5;204mTASK_SEQ_LEN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m77[39m
[38;5;204mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;204mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;204mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;204mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;204mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;204mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;204mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;204mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mD2ConvNeXt[39m
[38;5;15m  [39m[38;5;204mCONVNEXT[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m27[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;204mDIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;204mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;204mIN_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;204mLSIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;204mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;204mOUT_INDICES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m  [39m[38;5;204mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;204mDiNAT[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;204mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5[39m
[38;5;15m    [39m[38;5;204mDILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;204mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.2[39m
[38;5;15m    [39m[38;5;204mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;204mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;204mIN_PATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;204mKERNEL_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m    [39m[38;5;204mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;204mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m    [39m[38;5;204mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;204mOUT_INDICES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;204mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;204mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;204mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;204mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;204mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;204mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;204mIS_DEMO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;204mIS_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;204mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;204mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;204mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;204mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mOneFormer[39m
[38;5;15m  [39m[38;5;204mONE_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mCLASS_DEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;204mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;204mCONTRASTIVE_TEMPERATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.07[39m
[38;5;15m    [39m[38;5;204mCONTRASTIVE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;204mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;204mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;204mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;204mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;204mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;204mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;204mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;204mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;204mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;204mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;204mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;204mNUM_OBJECT_CTX[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m    [39m[38;5;204mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m250[39m
[38;5;15m    [39m[38;5;204mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;204mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;204mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;204mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mContrastiveMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;204mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m    [39m[38;5;204mUSE_TASK_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;204mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;204mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;204mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;204mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;204mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;204mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;204mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;204mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;204mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;204mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;204mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;204mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;15m    [39m[38;5;204mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mSyncBN[39m
[38;5;15m    [39m[38;5;204mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;204mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;204mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;204mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;204mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;204mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;204mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;204mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;204mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;204mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;204mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m&id002[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;204mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;204mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;204mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;204mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;204mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;204mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;204mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;204mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;204mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;204mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;204mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;204mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;204mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;204mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m&id001[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;204mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;204mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;204mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;204mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id001[39m
[38;5;15m    [39m[38;5;204mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;204mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;204mFED_LOSS_FREQ_WEIGHT_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;204mFED_LOSS_NUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;15m    [39m[38;5;204mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;204mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;204mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;204mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;204mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;204mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;204mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;204mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;204mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mUSE_FED_LOSS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mUSE_SIGMOID_CE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;204mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;204mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;204mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;204mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;204mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;204mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;204mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;204mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;204mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;204mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;204mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;204mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;204mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;204mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;204mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;204mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;204mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;204mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;204mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;204mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;204mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;204mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;204mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;204mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;204mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;204mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;204mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;204mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;204mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;204mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id002[39m
[38;5;15m    [39m[38;5;204mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;204mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;204mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;204mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;204mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;204mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;204mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;204mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;204mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;204mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;204mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;204mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;204mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;204mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;204mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;204mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;204mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;204mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;204mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;204mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;204mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;204mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;204mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m255[39m
[38;5;15m    [39m[38;5;204mINST_EMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;204mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;204mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;204mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;204mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;204mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mOneFormerHead[39m
[38;5;15m    [39m[38;5;204mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;204mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m30[39m
[38;5;15m    [39m[38;5;204mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;204mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;204mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;204mSEM_EMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;204mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;204mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;204mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;204mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;204mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;204mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;204mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;204mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;204mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;204mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;204mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;204mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;204mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;204mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;204mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;204mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;204mTEST[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mDETECTION_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;204mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m    [39m[38;5;204mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m    [39m[38;5;204mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;204mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;204mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mTASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpanoptic[39m
[38;5;15m  [39m[38;5;204mTEXT_ENCODER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mCONTEXT_LENGTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m77[39m
[38;5;15m    [39m[38;5;204mNUM_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;204mN_CTX[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m    [39m[38;5;204mPROJ_NUM_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;204mVOCAB_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m49408[39m
[38;5;15m    [39m[38;5;204mWIDTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;204mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/mnt/source/OneFormer/mapillary_pretrain_250_16_convnext_xl_oneformer_mapillary_300k.pth[39m
[38;5;204mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141moutputs/ceymo2[39m
[38;5;204mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;204mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;204mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;204mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;204mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;204mBASE_LR_END[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;204mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;204mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;204mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;204mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;204mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;204mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;204mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;204mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m  [39m[38;5;204mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupPolyLR[39m
[38;5;15m  [39m[38;5;204mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10000[39m
[38;5;15m  [39m[38;5;204mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;204mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;204mNUM_DECAYS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m  [39m[38;5;204mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;204mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;204mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;204mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;204mRESCALE_INTERVAL[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;204mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;204mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;204mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;204mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;204mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;204mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;204mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;204mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;204mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;204mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mCROP_SIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m640[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m640[39m
[38;5;15m    [39m[38;5;204mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;204mIS_SLIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mKEEP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;204mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;204mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1280[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1792[39m
[38;5;15m    [39m[38;5;204mSCALE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m640[39m
[38;5;15m    [39m[38;5;204mSETR_MULTI_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;204mSIZE_DIVISOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;204mSTRIDE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m426[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m426[39m
[38;5;15m  [39m[38;5;204mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m250[39m
[38;5;15m  [39m[38;5;204mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;204mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;204mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;204mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;204mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;204mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;204mWANDB[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;204mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mceymo[39m
[38;5;15m  [39m[38;5;204mPROJECT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mOneFormer[39m

[10/03 22:54:04] detectron2 INFO: Full config saved to outputs/ceymo2/config.yaml
[10/03 22:54:04] d2.utils.env INFO: Using a generated random seed 4332383
[10/03 22:54:11] d2.engine.defaults INFO: Model:
OneFormer(
  (backbone): D2ConvNeXt(
    (downsample_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(3, 256, kernel_size=(4, 4), stride=(4, 4))
        (1): LayerNorm()
      )
      (1): Sequential(
        (0): LayerNorm()
        (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))
      )
      (2): Sequential(
        (0): LayerNorm()
        (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))
      )
      (3): Sequential(
        (0): LayerNorm()
        (1): Conv2d(1024, 2048, kernel_size=(2, 2), stride=(2, 2))
      )
    )
    (stages): ModuleList(
      (0): Sequential(
        (0): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): Identity()
        )
        (1): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): DropPath()
        )
      )
      (1): Sequential(
        (0): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
      )
      (2): Sequential(
        (0): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (3): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (4): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (5): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (6): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (7): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (8): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (9): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (10): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (11): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (12): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (13): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (14): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (15): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (16): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (17): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (18): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (19): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (20): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (21): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (22): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (23): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (24): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (25): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (26): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
      )
      (3): Sequential(
        (0): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
      )
    )
    (norm0): LayerNorm()
    (norm1): LayerNorm()
    (norm2): LayerNorm()
    (norm3): LayerNorm()
  )
  (sem_seg_head): OneFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): ContrastiveMultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (class_transformer): Transformer(
        (encoder): TransformerEncoder(
          (layers): ModuleList()
        )
        (decoder): TransformerDecoder(
          (layers): ModuleList(
            (0-1): 2 x TransformerDecoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (dropout3): Dropout(p=0.1, inplace=False)
            )
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_embed): Embedding(250, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (class_input_proj): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (class_embed): Linear(in_features=256, out_features=31, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (task_mlp): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=77, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
    )
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (text_projector): MLP(
    (layers): ModuleList(
      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
    )
  )
  (prompt_ctx): Embedding(16, 256)
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks', 'contrastive']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_contrastive': 0.5, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_contrastive_0': 0.5, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_contrastive_1': 0.5, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_contrastive_2': 0.5, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_contrastive_3': 0.5, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_contrastive_4': 0.5, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_contrastive_5': 0.5, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_contrastive_6': 0.5, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_contrastive_7': 0.5, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0, 'loss_contrastive_8': 0.5}
      num_classes: 30
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[10/03 22:54:11] oneformer.data.dataset_mappers.oneformer_unified_dataset_mapper INFO: [OneFormerUnifiedDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=..., max_size=4096, sample_style='choice'), RandomCrop_CategoryAreaConstraint(crop_type='absolute', crop_size=[512, 1024], single_category_max_area=1.0, ignored_category=255), <detectron2.projects.point_rend.color_augmentation.ColorAugSSDTransform object at 0x7fbbb4195d50>, RandomFlip()]
[10/03 22:54:12] oneformer.data.datasets.register_cityscapes_panoptic INFO: 2 cities found in '/mnt/source/datasets/cityscapes/leftImg8bit/train'.
[10/03 22:54:12] d2.data.build INFO: Using training sampler TrainingSampler
[10/03 22:54:12] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/03 22:54:12] d2.data.common INFO: Serializing 181 elements to byte tensors and concatenating them all ...
[10/03 22:54:12] d2.data.common INFO: Serialized dataset takes 0.26 MiB
[10/03 22:54:12] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /mnt/source/OneFormer/mapillary_pretrain_250_16_convnext_xl_oneformer_mapillary_300k.pth ...
[10/03 22:54:12] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /mnt/source/OneFormer/mapillary_pretrain_250_16_convnext_xl_oneformer_mapillary_300k.pth ...
[10/03 22:54:13] fvcore.common.checkpoint WARNING: Skip loading parameter 'sem_seg_head.predictor.class_embed.weight' to the model due to incompatible shapes: (66, 256) in the checkpoint but (31, 256) in the model! You might want to double check if this is expected.
[10/03 22:54:13] fvcore.common.checkpoint WARNING: Skip loading parameter 'sem_seg_head.predictor.class_embed.bias' to the model due to incompatible shapes: (66,) in the checkpoint but (31,) in the model! You might want to double check if this is expected.
[10/03 22:54:13] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (66,) in the checkpoint but (31,) in the model! You might want to double check if this is expected.
[10/03 22:54:13] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[10/03 22:54:16] d2.engine.train_loop INFO: Starting training from iteration 0
[10/03 22:54:26] d2.utils.events INFO:  eta: 0:52:49  iter: 19  total_loss: 77.61  loss_ce: 4.793  loss_mask: 0.551  loss_dice: 1.076  loss_contrastive: 0  loss_ce_0: 6.812  loss_mask_0: 0.7152  loss_dice_0: 1.566  loss_ce_1: 6.583  loss_mask_1: 0.6923  loss_dice_1: 1.327  loss_ce_2: 5.499  loss_mask_2: 0.6934  loss_dice_2: 1.172  loss_ce_3: 6.231  loss_mask_3: 0.5844  loss_dice_3: 1.177  loss_ce_4: 5.841  loss_mask_4: 0.5753  loss_dice_4: 1.205  loss_ce_5: 5.583  loss_mask_5: 0.5667  loss_dice_5: 1.166  loss_ce_6: 5.635  loss_mask_6: 0.5638  loss_dice_6: 1.165  loss_ce_7: 5.508  loss_mask_7: 0.5902  loss_dice_7: 1.189  loss_ce_8: 5.039  loss_mask_8: 0.5287  loss_dice_8: 1.083    time: 0.3519  last_time: 0.3085  data_time: 0.0274  last_data_time: 0.0020   lr: 9.9829e-05  max_mem: 14481M
[10/03 22:54:32] d2.utils.events INFO:  eta: 0:53:36  iter: 39  total_loss: 40.01  loss_ce: 1.983  loss_mask: 0.4161  loss_dice: 1.098  loss_contrastive: 0  loss_ce_0: 3.785  loss_mask_0: 0.638  loss_dice_0: 2.037  loss_ce_1: 2.931  loss_mask_1: 0.4787  loss_dice_1: 1.192  loss_ce_2: 2.558  loss_mask_2: 0.4388  loss_dice_2: 1.162  loss_ce_3: 2.41  loss_mask_3: 0.5  loss_dice_3: 1.11  loss_ce_4: 2.351  loss_mask_4: 0.4569  loss_dice_4: 1.074  loss_ce_5: 1.945  loss_mask_5: 0.3983  loss_dice_5: 1.115  loss_ce_6: 1.971  loss_mask_6: 0.445  loss_dice_6: 1.05  loss_ce_7: 1.973  loss_mask_7: 0.3574  loss_dice_7: 0.9503  loss_ce_8: 1.979  loss_mask_8: 0.3954  loss_dice_8: 1.061    time: 0.3366  last_time: 0.3055  data_time: 0.0037  last_data_time: 0.0032   lr: 9.9649e-05  max_mem: 14521M
[10/03 22:54:39] d2.utils.events INFO:  eta: 0:53:01  iter: 59  total_loss: 28.03  loss_ce: 0.9176  loss_mask: 0.3074  loss_dice: 0.9005  loss_contrastive: 0  loss_ce_0: 2.257  loss_mask_0: 0.4756  loss_dice_0: 1.442  loss_ce_1: 1.317  loss_mask_1: 0.3197  loss_dice_1: 1.025  loss_ce_2: 1.279  loss_mask_2: 0.2825  loss_dice_2: 0.9627  loss_ce_3: 1.158  loss_mask_3: 0.2782  loss_dice_3: 0.8098  loss_ce_4: 1.049  loss_mask_4: 0.2982  loss_dice_4: 0.8734  loss_ce_5: 1.08  loss_mask_5: 0.2799  loss_dice_5: 0.8089  loss_ce_6: 0.9975  loss_mask_6: 0.2438  loss_dice_6: 0.9133  loss_ce_7: 0.8942  loss_mask_7: 0.3045  loss_dice_7: 0.8512  loss_ce_8: 0.9684  loss_mask_8: 0.2953  loss_dice_8: 0.8388    time: 0.3311  last_time: 0.3113  data_time: 0.0038  last_data_time: 0.0063   lr: 9.9469e-05  max_mem: 14521M
[10/03 22:54:45] d2.utils.events INFO:  eta: 0:52:13  iter: 79  total_loss: 29.34  loss_ce: 0.9382  loss_mask: 0.3014  loss_dice: 1.013  loss_contrastive: 0  loss_ce_0: 2.453  loss_mask_0: 0.5556  loss_dice_0: 1.939  loss_ce_1: 1.932  loss_mask_1: 0.3152  loss_dice_1: 1.149  loss_ce_2: 1.606  loss_mask_2: 0.3265  loss_dice_2: 1.079  loss_ce_3: 1.287  loss_mask_3: 0.3103  loss_dice_3: 1.035  loss_ce_4: 1.185  loss_mask_4: 0.2803  loss_dice_4: 1.126  loss_ce_5: 1.089  loss_mask_5: 0.2884  loss_dice_5: 1.111  loss_ce_6: 1.001  loss_mask_6: 0.3072  loss_dice_6: 0.9814  loss_ce_7: 0.9632  loss_mask_7: 0.2925  loss_dice_7: 0.9835  loss_ce_8: 0.9073  loss_mask_8: 0.3061  loss_dice_8: 0.8228    time: 0.3286  last_time: 0.3121  data_time: 0.0029  last_data_time: 0.0024   lr: 9.9289e-05  max_mem: 14521M
[10/03 22:54:51] d2.utils.events INFO:  eta: 0:51:49  iter: 99  total_loss: 32.01  loss_ce: 1.063  loss_mask: 0.7102  loss_dice: 1.035  loss_contrastive: 0  loss_ce_0: 2.526  loss_mask_0: 0.7994  loss_dice_0: 1.942  loss_ce_1: 1.896  loss_mask_1: 0.6051  loss_dice_1: 1.263  loss_ce_2: 1.601  loss_mask_2: 0.6505  loss_dice_2: 1.03  loss_ce_3: 1.459  loss_mask_3: 0.5771  loss_dice_3: 1.008  loss_ce_4: 1.336  loss_mask_4: 0.6284  loss_dice_4: 1.019  loss_ce_5: 1.246  loss_mask_5: 0.5816  loss_dice_5: 1.008  loss_ce_6: 1.084  loss_mask_6: 0.6708  loss_dice_6: 0.9979  loss_ce_7: 1.015  loss_mask_7: 0.6506  loss_dice_7: 1.097  loss_ce_8: 1.055  loss_mask_8: 0.6507  loss_dice_8: 1.076    time: 0.3271  last_time: 0.3114  data_time: 0.0031  last_data_time: 0.0029   lr: 9.9109e-05  max_mem: 14521M
[10/03 22:54:58] d2.utils.events INFO:  eta: 0:51:51  iter: 119  total_loss: 29.19  loss_ce: 0.9468  loss_mask: 0.3249  loss_dice: 0.8551  loss_contrastive: 0  loss_ce_0: 2.736  loss_mask_0: 0.5968  loss_dice_0: 1.51  loss_ce_1: 2.188  loss_mask_1: 0.3831  loss_dice_1: 1.047  loss_ce_2: 1.606  loss_mask_2: 0.3772  loss_dice_2: 1.036  loss_ce_3: 1.4  loss_mask_3: 0.3697  loss_dice_3: 0.9351  loss_ce_4: 1.361  loss_mask_4: 0.3553  loss_dice_4: 1.053  loss_ce_5: 1.251  loss_mask_5: 0.3182  loss_dice_5: 1.031  loss_ce_6: 1.082  loss_mask_6: 0.3134  loss_dice_6: 0.8647  loss_ce_7: 1.059  loss_mask_7: 0.3737  loss_dice_7: 0.8994  loss_ce_8: 0.9934  loss_mask_8: 0.3464  loss_dice_8: 0.8913    time: 0.3273  last_time: 0.3265  data_time: 0.0035  last_data_time: 0.0030   lr: 9.8928e-05  max_mem: 14698M
[10/03 22:55:04] d2.utils.events INFO:  eta: 0:51:58  iter: 139  total_loss: 28.77  loss_ce: 0.8591  loss_mask: 0.4737  loss_dice: 0.9491  loss_contrastive: 0  loss_ce_0: 2.746  loss_mask_0: 0.458  loss_dice_0: 1.4  loss_ce_1: 1.915  loss_mask_1: 0.3959  loss_dice_1: 1.137  loss_ce_2: 1.26  loss_mask_2: 0.336  loss_dice_2: 0.9375  loss_ce_3: 1.201  loss_mask_3: 0.4462  loss_dice_3: 0.9158  loss_ce_4: 1.094  loss_mask_4: 0.4106  loss_dice_4: 0.8753  loss_ce_5: 0.9673  loss_mask_5: 0.4183  loss_dice_5: 0.8796  loss_ce_6: 0.9155  loss_mask_6: 0.4233  loss_dice_6: 0.9145  loss_ce_7: 0.7967  loss_mask_7: 0.427  loss_dice_7: 0.9358  loss_ce_8: 0.8389  loss_mask_8: 0.5099  loss_dice_8: 0.9817    time: 0.3263  last_time: 0.3375  data_time: 0.0033  last_data_time: 0.0027   lr: 9.8748e-05  max_mem: 14698M
[10/03 22:55:11] d2.utils.events INFO:  eta: 0:52:09  iter: 159  total_loss: 36.42  loss_ce: 1.186  loss_mask: 0.613  loss_dice: 1.277  loss_contrastive: 0  loss_ce_0: 2.751  loss_mask_0: 0.9006  loss_dice_0: 2.072  loss_ce_1: 1.831  loss_mask_1: 0.68  loss_dice_1: 1.53  loss_ce_2: 1.488  loss_mask_2: 0.7044  loss_dice_2: 1.49  loss_ce_3: 1.467  loss_mask_3: 0.6146  loss_dice_3: 1.343  loss_ce_4: 1.373  loss_mask_4: 0.6346  loss_dice_4: 1.42  loss_ce_5: 1.261  loss_mask_5: 0.656  loss_dice_5: 1.32  loss_ce_6: 1.242  loss_mask_6: 0.595  loss_dice_6: 1.33  loss_ce_7: 1.079  loss_mask_7: 0.5945  loss_dice_7: 1.295  loss_ce_8: 1.058  loss_mask_8: 0.6157  loss_dice_8: 1.355    time: 0.3258  last_time: 0.3086  data_time: 0.0032  last_data_time: 0.0029   lr: 9.8568e-05  max_mem: 14698M
[10/03 22:55:17] d2.utils.events INFO:  eta: 0:52:33  iter: 179  total_loss: 32.97  loss_ce: 0.928  loss_mask: 0.5633  loss_dice: 1.227  loss_contrastive: 0  loss_ce_0: 2.254  loss_mask_0: 0.687  loss_dice_0: 1.734  loss_ce_1: 1.762  loss_mask_1: 0.5077  loss_dice_1: 1.224  loss_ce_2: 1.515  loss_mask_2: 0.5387  loss_dice_2: 1.279  loss_ce_3: 1.44  loss_mask_3: 0.5446  loss_dice_3: 1.229  loss_ce_4: 1.208  loss_mask_4: 0.5276  loss_dice_4: 1.345  loss_ce_5: 1.119  loss_mask_5: 0.4687  loss_dice_5: 1.163  loss_ce_6: 1.039  loss_mask_6: 0.5622  loss_dice_6: 1.171  loss_ce_7: 1.047  loss_mask_7: 0.6393  loss_dice_7: 1.241  loss_ce_8: 1.025  loss_mask_8: 0.5321  loss_dice_8: 1.221    time: 0.3258  last_time: 0.3267  data_time: 0.0034  last_data_time: 0.0047   lr: 9.8388e-05  max_mem: 14698M
[10/03 22:55:24] d2.utils.events INFO:  eta: 0:52:22  iter: 199  total_loss: 28.35  loss_ce: 0.9487  loss_mask: 0.3971  loss_dice: 0.9171  loss_contrastive: 0  loss_ce_0: 2.719  loss_mask_0: 0.5249  loss_dice_0: 1.602  loss_ce_1: 1.842  loss_mask_1: 0.3646  loss_dice_1: 1.026  loss_ce_2: 1.477  loss_mask_2: 0.3728  loss_dice_2: 0.9111  loss_ce_3: 1.349  loss_mask_3: 0.3738  loss_dice_3: 1.008  loss_ce_4: 1.134  loss_mask_4: 0.3706  loss_dice_4: 1.088  loss_ce_5: 1.189  loss_mask_5: 0.3519  loss_dice_5: 0.844  loss_ce_6: 1.221  loss_mask_6: 0.3222  loss_dice_6: 0.9053  loss_ce_7: 1.006  loss_mask_7: 0.4176  loss_dice_7: 0.9928  loss_ce_8: 0.9561  loss_mask_8: 0.4175  loss_dice_8: 0.8719    time: 0.3254  last_time: 0.3319  data_time: 0.0037  last_data_time: 0.0062   lr: 9.8207e-05  max_mem: 14706M
[10/03 22:55:30] d2.utils.events INFO:  eta: 0:52:02  iter: 219  total_loss: 28.82  loss_ce: 0.9214  loss_mask: 0.2872  loss_dice: 1.163  loss_contrastive: 0  loss_ce_0: 2.262  loss_mask_0: 0.4414  loss_dice_0: 1.529  loss_ce_1: 1.396  loss_mask_1: 0.2821  loss_dice_1: 1.251  loss_ce_2: 1.193  loss_mask_2: 0.2806  loss_dice_2: 1.218  loss_ce_3: 1.009  loss_mask_3: 0.3404  loss_dice_3: 1.326  loss_ce_4: 0.8196  loss_mask_4: 0.3857  loss_dice_4: 1.417  loss_ce_5: 0.8073  loss_mask_5: 0.3267  loss_dice_5: 1.254  loss_ce_6: 0.8735  loss_mask_6: 0.2978  loss_dice_6: 1.305  loss_ce_7: 0.8692  loss_mask_7: 0.293  loss_dice_7: 1.211  loss_ce_8: 0.8115  loss_mask_8: 0.2615  loss_dice_8: 1.226    time: 0.3248  last_time: 0.3276  data_time: 0.0036  last_data_time: 0.0033   lr: 9.8027e-05  max_mem: 14706M
[10/03 22:55:37] d2.utils.events INFO:  eta: 0:51:50  iter: 239  total_loss: 30.74  loss_ce: 0.8743  loss_mask: 0.3835  loss_dice: 1.171  loss_contrastive: 0  loss_ce_0: 2.657  loss_mask_0: 0.4774  loss_dice_0: 1.763  loss_ce_1: 1.775  loss_mask_1: 0.3716  loss_dice_1: 1.473  loss_ce_2: 1.315  loss_mask_2: 0.3987  loss_dice_2: 1.298  loss_ce_3: 1.216  loss_mask_3: 0.4301  loss_dice_3: 1.301  loss_ce_4: 1.009  loss_mask_4: 0.4132  loss_dice_4: 1.285  loss_ce_5: 1.011  loss_mask_5: 0.3816  loss_dice_5: 1.338  loss_ce_6: 0.9046  loss_mask_6: 0.3895  loss_dice_6: 1.162  loss_ce_7: 0.9303  loss_mask_7: 0.389  loss_dice_7: 1.286  loss_ce_8: 0.8422  loss_mask_8: 0.3824  loss_dice_8: 1.211    time: 0.3242  last_time: 0.3447  data_time: 0.0032  last_data_time: 0.0029   lr: 9.7846e-05  max_mem: 14706M
[10/03 22:55:43] d2.utils.events INFO:  eta: 0:51:38  iter: 259  total_loss: 33.94  loss_ce: 0.8522  loss_mask: 0.4768  loss_dice: 1.39  loss_contrastive: 0  loss_ce_0: 2.159  loss_mask_0: 0.5959  loss_dice_0: 2.121  loss_ce_1: 1.557  loss_mask_1: 0.4057  loss_dice_1: 1.55  loss_ce_2: 1.253  loss_mask_2: 0.4185  loss_dice_2: 1.418  loss_ce_3: 1.034  loss_mask_3: 0.4243  loss_dice_3: 1.542  loss_ce_4: 0.8764  loss_mask_4: 0.415  loss_dice_4: 1.461  loss_ce_5: 0.9122  loss_mask_5: 0.4513  loss_dice_5: 1.48  loss_ce_6: 0.8751  loss_mask_6: 0.42  loss_dice_6: 1.344  loss_ce_7: 0.7546  loss_mask_7: 0.502  loss_dice_7: 1.317  loss_ce_8: 0.8663  loss_mask_8: 0.5369  loss_dice_8: 1.474    time: 0.3238  last_time: 0.2998  data_time: 0.0038  last_data_time: 0.0029   lr: 9.7666e-05  max_mem: 14706M
[10/03 22:55:49] d2.utils.events INFO:  eta: 0:51:24  iter: 279  total_loss: 31.02  loss_ce: 0.6991  loss_mask: 0.5285  loss_dice: 1.265  loss_contrastive: 0  loss_ce_0: 2.422  loss_mask_0: 0.6854  loss_dice_0: 1.746  loss_ce_1: 1.587  loss_mask_1: 0.6391  loss_dice_1: 1.55  loss_ce_2: 1.296  loss_mask_2: 0.5513  loss_dice_2: 1.444  loss_ce_3: 1.004  loss_mask_3: 0.5951  loss_dice_3: 1.426  loss_ce_4: 0.9302  loss_mask_4: 0.591  loss_dice_4: 1.292  loss_ce_5: 0.8845  loss_mask_5: 0.6266  loss_dice_5: 1.26  loss_ce_6: 0.8231  loss_mask_6: 0.5718  loss_dice_6: 1.28  loss_ce_7: 0.7732  loss_mask_7: 0.5798  loss_dice_7: 1.243  loss_ce_8: 0.6504  loss_mask_8: 0.527  loss_dice_8: 1.282    time: 0.3230  last_time: 0.3026  data_time: 0.0030  last_data_time: 0.0027   lr: 9.7485e-05  max_mem: 14706M
[10/03 22:55:56] d2.utils.events INFO:  eta: 0:51:11  iter: 299  total_loss: 27.83  loss_ce: 0.8744  loss_mask: 0.4439  loss_dice: 1.157  loss_contrastive: 0  loss_ce_0: 1.78  loss_mask_0: 0.5271  loss_dice_0: 1.576  loss_ce_1: 1.405  loss_mask_1: 0.4608  loss_dice_1: 1.269  loss_ce_2: 1.224  loss_mask_2: 0.4916  loss_dice_2: 1.12  loss_ce_3: 1.106  loss_mask_3: 0.4808  loss_dice_3: 1.193  loss_ce_4: 0.9674  loss_mask_4: 0.5273  loss_dice_4: 1.277  loss_ce_5: 0.8673  loss_mask_5: 0.4821  loss_dice_5: 1.13  loss_ce_6: 0.9098  loss_mask_6: 0.4946  loss_dice_6: 1.071  loss_ce_7: 0.8278  loss_mask_7: 0.4906  loss_dice_7: 1.17  loss_ce_8: 0.7697  loss_mask_8: 0.4465  loss_dice_8: 1.127    time: 0.3225  last_time: 0.3025  data_time: 0.0031  last_data_time: 0.0024   lr: 9.7305e-05  max_mem: 14706M
[10/03 22:56:02] d2.utils.events INFO:  eta: 0:51:05  iter: 319  total_loss: 27.35  loss_ce: 0.8157  loss_mask: 0.3246  loss_dice: 1.117  loss_contrastive: 0  loss_ce_0: 2.094  loss_mask_0: 0.538  loss_dice_0: 1.703  loss_ce_1: 1.431  loss_mask_1: 0.3614  loss_dice_1: 1.304  loss_ce_2: 1.001  loss_mask_2: 0.3802  loss_dice_2: 1.277  loss_ce_3: 1.024  loss_mask_3: 0.3744  loss_dice_3: 1.111  loss_ce_4: 0.9029  loss_mask_4: 0.3308  loss_dice_4: 1.311  loss_ce_5: 0.8593  loss_mask_5: 0.3334  loss_dice_5: 1.15  loss_ce_6: 0.8104  loss_mask_6: 0.3828  loss_dice_6: 1.185  loss_ce_7: 0.8564  loss_mask_7: 0.3613  loss_dice_7: 1.103  loss_ce_8: 0.8097  loss_mask_8: 0.346  loss_dice_8: 1.218    time: 0.3220  last_time: 0.3205  data_time: 0.0032  last_data_time: 0.0026   lr: 9.7124e-05  max_mem: 14706M
[10/03 22:56:08] d2.utils.events INFO:  eta: 0:50:49  iter: 339  total_loss: 31.48  loss_ce: 0.7293  loss_mask: 0.6967  loss_dice: 1.183  loss_contrastive: 0  loss_ce_0: 2.027  loss_mask_0: 0.7324  loss_dice_0: 1.508  loss_ce_1: 1.25  loss_mask_1: 0.6651  loss_dice_1: 1.337  loss_ce_2: 1.036  loss_mask_2: 0.6142  loss_dice_2: 1.229  loss_ce_3: 1.123  loss_mask_3: 0.564  loss_dice_3: 1.167  loss_ce_4: 0.9021  loss_mask_4: 0.5938  loss_dice_4: 1.226  loss_ce_5: 0.8799  loss_mask_5: 0.5978  loss_dice_5: 1.292  loss_ce_6: 0.7418  loss_mask_6: 0.6692  loss_dice_6: 1.242  loss_ce_7: 0.7497  loss_mask_7: 0.6285  loss_dice_7: 1.195  loss_ce_8: 0.763  loss_mask_8: 0.6427  loss_dice_8: 1.196    time: 0.3216  last_time: 0.3047  data_time: 0.0031  last_data_time: 0.0034   lr: 9.6944e-05  max_mem: 14706M
[10/03 22:56:15] d2.utils.events INFO:  eta: 0:50:41  iter: 359  total_loss: 26.6  loss_ce: 0.617  loss_mask: 0.5541  loss_dice: 1.312  loss_contrastive: 0  loss_ce_0: 2.164  loss_mask_0: 0.518  loss_dice_0: 1.639  loss_ce_1: 1.283  loss_mask_1: 0.5026  loss_dice_1: 1.358  loss_ce_2: 1.009  loss_mask_2: 0.4644  loss_dice_2: 1.269  loss_ce_3: 0.8932  loss_mask_3: 0.4581  loss_dice_3: 1.286  loss_ce_4: 0.7873  loss_mask_4: 0.464  loss_dice_4: 1.28  loss_ce_5: 0.7491  loss_mask_5: 0.4748  loss_dice_5: 1.331  loss_ce_6: 0.7091  loss_mask_6: 0.5314  loss_dice_6: 1.297  loss_ce_7: 0.6597  loss_mask_7: 0.5263  loss_dice_7: 1.314  loss_ce_8: 0.5755  loss_mask_8: 0.4746  loss_dice_8: 1.319    time: 0.3212  last_time: 0.3327  data_time: 0.0032  last_data_time: 0.0025   lr: 9.6763e-05  max_mem: 14706M
[10/03 22:56:21] d2.utils.events INFO:  eta: 0:50:27  iter: 379  total_loss: 27.31  loss_ce: 0.6097  loss_mask: 0.6184  loss_dice: 1.193  loss_contrastive: 0  loss_ce_0: 2.346  loss_mask_0: 0.6938  loss_dice_0: 1.688  loss_ce_1: 1.579  loss_mask_1: 0.668  loss_dice_1: 1.206  loss_ce_2: 1.155  loss_mask_2: 0.5493  loss_dice_2: 1.21  loss_ce_3: 0.8231  loss_mask_3: 0.5997  loss_dice_3: 1.192  loss_ce_4: 0.7983  loss_mask_4: 0.5379  loss_dice_4: 1.117  loss_ce_5: 0.7577  loss_mask_5: 0.5511  loss_dice_5: 1.016  loss_ce_6: 0.6691  loss_mask_6: 0.6104  loss_dice_6: 1.16  loss_ce_7: 0.6473  loss_mask_7: 0.594  loss_dice_7: 1.208  loss_ce_8: 0.7674  loss_mask_8: 0.6445  loss_dice_8: 1.131    time: 0.3209  last_time: 0.3049  data_time: 0.0032  last_data_time: 0.0027   lr: 9.6582e-05  max_mem: 14706M
[10/03 22:56:27] d2.utils.events INFO:  eta: 0:50:21  iter: 399  total_loss: 30  loss_ce: 0.6922  loss_mask: 0.4088  loss_dice: 1.156  loss_contrastive: 0  loss_ce_0: 2.017  loss_mask_0: 0.5828  loss_dice_0: 1.885  loss_ce_1: 1.301  loss_mask_1: 0.507  loss_dice_1: 1.345  loss_ce_2: 0.9631  loss_mask_2: 0.4573  loss_dice_2: 1.302  loss_ce_3: 0.8225  loss_mask_3: 0.4889  loss_dice_3: 1.138  loss_ce_4: 0.7197  loss_mask_4: 0.4729  loss_dice_4: 1.274  loss_ce_5: 0.6782  loss_mask_5: 0.4584  loss_dice_5: 1.149  loss_ce_6: 0.6882  loss_mask_6: 0.4487  loss_dice_6: 1.226  loss_ce_7: 0.6474  loss_mask_7: 0.4228  loss_dice_7: 1.232  loss_ce_8: 0.6182  loss_mask_8: 0.4023  loss_dice_8: 1.104    time: 0.3208  last_time: 0.3218  data_time: 0.0030  last_data_time: 0.0033   lr: 9.6402e-05  max_mem: 14706M
[10/03 22:56:34] d2.utils.events INFO:  eta: 0:50:18  iter: 419  total_loss: 29.31  loss_ce: 0.7156  loss_mask: 0.3896  loss_dice: 1.186  loss_contrastive: 0  loss_ce_0: 2.172  loss_mask_0: 0.4963  loss_dice_0: 2.024  loss_ce_1: 1.433  loss_mask_1: 0.4506  loss_dice_1: 1.447  loss_ce_2: 1.169  loss_mask_2: 0.3799  loss_dice_2: 1.346  loss_ce_3: 0.7894  loss_mask_3: 0.3903  loss_dice_3: 1.222  loss_ce_4: 0.779  loss_mask_4: 0.4316  loss_dice_4: 1.344  loss_ce_5: 0.7008  loss_mask_5: 0.4122  loss_dice_5: 1.226  loss_ce_6: 0.6673  loss_mask_6: 0.3787  loss_dice_6: 1.091  loss_ce_7: 0.6509  loss_mask_7: 0.414  loss_dice_7: 1.114  loss_ce_8: 0.6724  loss_mask_8: 0.3985  loss_dice_8: 1.192    time: 0.3210  last_time: 0.3306  data_time: 0.0034  last_data_time: 0.0038   lr: 9.6221e-05  max_mem: 14706M
[10/03 22:56:40] d2.utils.events INFO:  eta: 0:50:15  iter: 439  total_loss: 28.5  loss_ce: 0.5927  loss_mask: 0.3394  loss_dice: 1.182  loss_contrastive: 0  loss_ce_0: 1.852  loss_mask_0: 0.6616  loss_dice_0: 2.411  loss_ce_1: 1.123  loss_mask_1: 0.3656  loss_dice_1: 1.378  loss_ce_2: 1.197  loss_mask_2: 0.4228  loss_dice_2: 1.205  loss_ce_3: 1.07  loss_mask_3: 0.41  loss_dice_3: 1.128  loss_ce_4: 0.9595  loss_mask_4: 0.3723  loss_dice_4: 1.216  loss_ce_5: 0.829  loss_mask_5: 0.3727  loss_dice_5: 1.103  loss_ce_6: 0.8935  loss_mask_6: 0.3772  loss_dice_6: 1.076  loss_ce_7: 0.6964  loss_mask_7: 0.365  loss_dice_7: 1.089  loss_ce_8: 0.6811  loss_mask_8: 0.3781  loss_dice_8: 1.177    time: 0.3214  last_time: 0.3104  data_time: 0.0032  last_data_time: 0.0028   lr: 9.604e-05  max_mem: 14706M
[10/03 22:56:47] d2.utils.events INFO:  eta: 0:50:08  iter: 459  total_loss: 29.19  loss_ce: 0.7321  loss_mask: 0.2443  loss_dice: 1.368  loss_contrastive: 0  loss_ce_0: 2.004  loss_mask_0: 0.425  loss_dice_0: 1.979  loss_ce_1: 1.346  loss_mask_1: 0.2941  loss_dice_1: 1.542  loss_ce_2: 1.101  loss_mask_2: 0.2715  loss_dice_2: 1.328  loss_ce_3: 1.085  loss_mask_3: 0.2459  loss_dice_3: 1.309  loss_ce_4: 0.8946  loss_mask_4: 0.2415  loss_dice_4: 1.176  loss_ce_5: 0.8513  loss_mask_5: 0.2342  loss_dice_5: 1.137  loss_ce_6: 0.7808  loss_mask_6: 0.2324  loss_dice_6: 1.106  loss_ce_7: 0.8949  loss_mask_7: 0.2779  loss_dice_7: 1.289  loss_ce_8: 0.8291  loss_mask_8: 0.2641  loss_dice_8: 1.383    time: 0.3212  last_time: 0.3081  data_time: 0.0032  last_data_time: 0.0035   lr: 9.5859e-05  max_mem: 14706M
[10/03 22:56:53] d2.utils.events INFO:  eta: 0:50:03  iter: 479  total_loss: 33.3  loss_ce: 0.9341  loss_mask: 0.4704  loss_dice: 1.464  loss_contrastive: 0  loss_ce_0: 2.396  loss_mask_0: 0.5993  loss_dice_0: 2.239  loss_ce_1: 1.564  loss_mask_1: 0.5187  loss_dice_1: 1.812  loss_ce_2: 1.155  loss_mask_2: 0.4704  loss_dice_2: 1.645  loss_ce_3: 1.136  loss_mask_3: 0.4803  loss_dice_3: 1.465  loss_ce_4: 1.085  loss_mask_4: 0.4684  loss_dice_4: 1.471  loss_ce_5: 0.9416  loss_mask_5: 0.4838  loss_dice_5: 1.401  loss_ce_6: 0.8302  loss_mask_6: 0.4644  loss_dice_6: 1.412  loss_ce_7: 0.9741  loss_mask_7: 0.4759  loss_dice_7: 1.429  loss_ce_8: 0.9069  loss_mask_8: 0.5016  loss_dice_8: 1.433    time: 0.3210  last_time: 0.3316  data_time: 0.0035  last_data_time: 0.0029   lr: 9.5678e-05  max_mem: 14706M
[10/03 22:56:59] d2.utils.events INFO:  eta: 0:49:59  iter: 499  total_loss: 26.36  loss_ce: 0.8303  loss_mask: 0.2606  loss_dice: 1.165  loss_contrastive: 0  loss_ce_0: 1.822  loss_mask_0: 0.3277  loss_dice_0: 1.646  loss_ce_1: 1.237  loss_mask_1: 0.3191  loss_dice_1: 1.29  loss_ce_2: 1.076  loss_mask_2: 0.2627  loss_dice_2: 1.232  loss_ce_3: 0.9207  loss_mask_3: 0.335  loss_dice_3: 1.3  loss_ce_4: 0.8494  loss_mask_4: 0.3005  loss_dice_4: 1.248  loss_ce_5: 0.8612  loss_mask_5: 0.2887  loss_dice_5: 1.25  loss_ce_6: 0.8648  loss_mask_6: 0.2919  loss_dice_6: 1.183  loss_ce_7: 0.793  loss_mask_7: 0.2903  loss_dice_7: 1.204  loss_ce_8: 0.8692  loss_mask_8: 0.2886  loss_dice_8: 1.231    time: 0.3208  last_time: 0.3162  data_time: 0.0032  last_data_time: 0.0024   lr: 9.5498e-05  max_mem: 14706M
[10/03 22:57:06] d2.utils.events INFO:  eta: 0:49:57  iter: 519  total_loss: 23.92  loss_ce: 0.5661  loss_mask: 0.2631  loss_dice: 1.212  loss_contrastive: 0  loss_ce_0: 1.588  loss_mask_0: 0.3399  loss_dice_0: 1.989  loss_ce_1: 1.027  loss_mask_1: 0.2755  loss_dice_1: 1.541  loss_ce_2: 0.9359  loss_mask_2: 0.2542  loss_dice_2: 1.319  loss_ce_3: 0.8166  loss_mask_3: 0.2506  loss_dice_3: 1.128  loss_ce_4: 0.7597  loss_mask_4: 0.2796  loss_dice_4: 1.212  loss_ce_5: 0.6958  loss_mask_5: 0.2831  loss_dice_5: 1.147  loss_ce_6: 0.6394  loss_mask_6: 0.2855  loss_dice_6: 1.118  loss_ce_7: 0.5712  loss_mask_7: 0.2675  loss_dice_7: 1.126  loss_ce_8: 0.5727  loss_mask_8: 0.2464  loss_dice_8: 1.142    time: 0.3209  last_time: 0.3161  data_time: 0.0039  last_data_time: 0.0044   lr: 9.5317e-05  max_mem: 14706M
[10/03 22:57:12] d2.utils.events INFO:  eta: 0:49:53  iter: 539  total_loss: 24.25  loss_ce: 0.6053  loss_mask: 0.3526  loss_dice: 0.8894  loss_contrastive: 0  loss_ce_0: 1.909  loss_mask_0: 0.5048  loss_dice_0: 1.561  loss_ce_1: 1.264  loss_mask_1: 0.38  loss_dice_1: 1.156  loss_ce_2: 1.095  loss_mask_2: 0.3728  loss_dice_2: 0.9862  loss_ce_3: 0.9151  loss_mask_3: 0.3376  loss_dice_3: 1.002  loss_ce_4: 0.731  loss_mask_4: 0.3325  loss_dice_4: 0.983  loss_ce_5: 0.7861  loss_mask_5: 0.3871  loss_dice_5: 0.9308  loss_ce_6: 0.7118  loss_mask_6: 0.3688  loss_dice_6: 0.8575  loss_ce_7: 0.6013  loss_mask_7: 0.3594  loss_dice_7: 0.8835  loss_ce_8: 0.6223  loss_mask_8: 0.3662  loss_dice_8: 0.8876    time: 0.3208  last_time: 0.3090  data_time: 0.0037  last_data_time: 0.0035   lr: 9.5136e-05  max_mem: 14706M
[10/03 22:57:19] d2.utils.events INFO:  eta: 0:49:48  iter: 559  total_loss: 22.28  loss_ce: 0.576  loss_mask: 0.2328  loss_dice: 0.9864  loss_contrastive: 0  loss_ce_0: 1.58  loss_mask_0: 0.3397  loss_dice_0: 1.439  loss_ce_1: 0.9764  loss_mask_1: 0.2907  loss_dice_1: 1.325  loss_ce_2: 0.8921  loss_mask_2: 0.2797  loss_dice_2: 1.103  loss_ce_3: 0.6689  loss_mask_3: 0.2582  loss_dice_3: 0.9949  loss_ce_4: 0.6513  loss_mask_4: 0.2628  loss_dice_4: 1.099  loss_ce_5: 0.5944  loss_mask_5: 0.269  loss_dice_5: 0.9992  loss_ce_6: 0.6676  loss_mask_6: 0.2459  loss_dice_6: 0.9496  loss_ce_7: 0.6433  loss_mask_7: 0.242  loss_dice_7: 1.024  loss_ce_8: 0.5726  loss_mask_8: 0.2471  loss_dice_8: 0.9598    time: 0.3207  last_time: 0.3041  data_time: 0.0033  last_data_time: 0.0030   lr: 9.4955e-05  max_mem: 14706M
[10/03 22:57:25] d2.utils.events INFO:  eta: 0:49:43  iter: 579  total_loss: 28.7  loss_ce: 0.6497  loss_mask: 0.5532  loss_dice: 1.097  loss_contrastive: 0  loss_ce_0: 1.7  loss_mask_0: 0.6601  loss_dice_0: 1.54  loss_ce_1: 1.162  loss_mask_1: 0.5742  loss_dice_1: 1.311  loss_ce_2: 0.8932  loss_mask_2: 0.5206  loss_dice_2: 1.123  loss_ce_3: 0.8928  loss_mask_3: 0.503  loss_dice_3: 1.048  loss_ce_4: 0.7676  loss_mask_4: 0.492  loss_dice_4: 1.167  loss_ce_5: 0.6823  loss_mask_5: 0.5341  loss_dice_5: 1.11  loss_ce_6: 0.6925  loss_mask_6: 0.5454  loss_dice_6: 1.069  loss_ce_7: 0.6571  loss_mask_7: 0.5386  loss_dice_7: 1.041  loss_ce_8: 0.5827  loss_mask_8: 0.5417  loss_dice_8: 1.179    time: 0.3207  last_time: 0.3202  data_time: 0.0035  last_data_time: 0.0032   lr: 9.4774e-05  max_mem: 14706M
[10/03 22:57:32] d2.utils.events INFO:  eta: 0:49:38  iter: 599  total_loss: 23.99  loss_ce: 0.5032  loss_mask: 0.3728  loss_dice: 1.007  loss_contrastive: 0  loss_ce_0: 1.923  loss_mask_0: 0.4839  loss_dice_0: 1.422  loss_ce_1: 1.215  loss_mask_1: 0.4059  loss_dice_1: 1.026  loss_ce_2: 0.8269  loss_mask_2: 0.3912  loss_dice_2: 0.9881  loss_ce_3: 0.7087  loss_mask_3: 0.3693  loss_dice_3: 0.9907  loss_ce_4: 0.5992  loss_mask_4: 0.3493  loss_dice_4: 0.9711  loss_ce_5: 0.6281  loss_mask_5: 0.3785  loss_dice_5: 0.9215  loss_ce_6: 0.5728  loss_mask_6: 0.3942  loss_dice_6: 0.9467  loss_ce_7: 0.5475  loss_mask_7: 0.3456  loss_dice_7: 0.9272  loss_ce_8: 0.5257  loss_mask_8: 0.3346  loss_dice_8: 0.9699    time: 0.3207  last_time: 0.3092  data_time: 0.0038  last_data_time: 0.0031   lr: 9.4592e-05  max_mem: 14706M
[10/03 22:57:38] d2.utils.events INFO:  eta: 0:49:31  iter: 619  total_loss: 27.09  loss_ce: 0.6154  loss_mask: 0.3568  loss_dice: 1.175  loss_contrastive: 0  loss_ce_0: 1.711  loss_mask_0: 0.5087  loss_dice_0: 1.614  loss_ce_1: 1.309  loss_mask_1: 0.3696  loss_dice_1: 1.204  loss_ce_2: 0.9097  loss_mask_2: 0.3744  loss_dice_2: 1.234  loss_ce_3: 0.8065  loss_mask_3: 0.3631  loss_dice_3: 1.196  loss_ce_4: 0.7436  loss_mask_4: 0.3711  loss_dice_4: 1.332  loss_ce_5: 0.7494  loss_mask_5: 0.3299  loss_dice_5: 1.165  loss_ce_6: 0.646  loss_mask_6: 0.298  loss_dice_6: 1.097  loss_ce_7: 0.6565  loss_mask_7: 0.359  loss_dice_7: 1.281  loss_ce_8: 0.676  loss_mask_8: 0.3588  loss_dice_8: 1.277    time: 0.3207  last_time: 0.3341  data_time: 0.0034  last_data_time: 0.0028   lr: 9.4411e-05  max_mem: 14706M
[10/03 22:57:44] d2.utils.events INFO:  eta: 0:49:24  iter: 639  total_loss: 19.87  loss_ce: 0.2424  loss_mask: 0.278  loss_dice: 0.8176  loss_contrastive: 0  loss_ce_0: 0.9553  loss_mask_0: 0.3384  loss_dice_0: 1.279  loss_ce_1: 0.866  loss_mask_1: 0.3172  loss_dice_1: 1.073  loss_ce_2: 0.5349  loss_mask_2: 0.2778  loss_dice_2: 0.8721  loss_ce_3: 0.4554  loss_mask_3: 0.2589  loss_dice_3: 0.7444  loss_ce_4: 0.2824  loss_mask_4: 0.2555  loss_dice_4: 0.8075  loss_ce_5: 0.3305  loss_mask_5: 0.2608  loss_dice_5: 0.6904  loss_ce_6: 0.2264  loss_mask_6: 0.2737  loss_dice_6: 0.7875  loss_ce_7: 0.2167  loss_mask_7: 0.2699  loss_dice_7: 0.8889  loss_ce_8: 0.2966  loss_mask_8: 0.2645  loss_dice_8: 0.8664    time: 0.3207  last_time: 0.3351  data_time: 0.0029  last_data_time: 0.0042   lr: 9.423e-05  max_mem: 14706M
[10/03 22:57:51] d2.utils.events INFO:  eta: 0:49:18  iter: 659  total_loss: 22.39  loss_ce: 0.5589  loss_mask: 0.3365  loss_dice: 0.9537  loss_contrastive: 0  loss_ce_0: 1.645  loss_mask_0: 0.3151  loss_dice_0: 1.628  loss_ce_1: 1.131  loss_mask_1: 0.2945  loss_dice_1: 1.177  loss_ce_2: 0.9848  loss_mask_2: 0.3246  loss_dice_2: 1.037  loss_ce_3: 0.809  loss_mask_3: 0.296  loss_dice_3: 1.017  loss_ce_4: 0.7186  loss_mask_4: 0.2504  loss_dice_4: 1.049  loss_ce_5: 0.7091  loss_mask_5: 0.2818  loss_dice_5: 0.9764  loss_ce_6: 0.6323  loss_mask_6: 0.2982  loss_dice_6: 0.91  loss_ce_7: 0.6172  loss_mask_7: 0.3201  loss_dice_7: 0.9561  loss_ce_8: 0.5574  loss_mask_8: 0.3145  loss_dice_8: 0.9168    time: 0.3206  last_time: 0.3064  data_time: 0.0031  last_data_time: 0.0034   lr: 9.4049e-05  max_mem: 14706M
[10/03 22:57:57] d2.utils.events INFO:  eta: 0:49:10  iter: 679  total_loss: 26.25  loss_ce: 0.7487  loss_mask: 0.4263  loss_dice: 1.039  loss_contrastive: 0  loss_ce_0: 1.656  loss_mask_0: 0.5507  loss_dice_0: 1.555  loss_ce_1: 1.109  loss_mask_1: 0.4722  loss_dice_1: 1.26  loss_ce_2: 1.039  loss_mask_2: 0.4435  loss_dice_2: 1.141  loss_ce_3: 0.8198  loss_mask_3: 0.4216  loss_dice_3: 1.104  loss_ce_4: 0.9934  loss_mask_4: 0.4082  loss_dice_4: 1.017  loss_ce_5: 0.7519  loss_mask_5: 0.4565  loss_dice_5: 1.046  loss_ce_6: 0.7316  loss_mask_6: 0.4204  loss_dice_6: 1.042  loss_ce_7: 0.8041  loss_mask_7: 0.4451  loss_dice_7: 1.127  loss_ce_8: 0.7028  loss_mask_8: 0.4391  loss_dice_8: 1.063    time: 0.3205  last_time: 0.3075  data_time: 0.0033  last_data_time: 0.0031   lr: 9.3868e-05  max_mem: 14706M
[10/03 22:58:04] d2.utils.events INFO:  eta: 0:49:02  iter: 699  total_loss: 29.12  loss_ce: 0.7042  loss_mask: 0.3907  loss_dice: 1.209  loss_contrastive: 0  loss_ce_0: 1.944  loss_mask_0: 0.5826  loss_dice_0: 1.666  loss_ce_1: 1.389  loss_mask_1: 0.4792  loss_dice_1: 1.413  loss_ce_2: 1.017  loss_mask_2: 0.3835  loss_dice_2: 1.467  loss_ce_3: 0.7692  loss_mask_3: 0.3695  loss_dice_3: 1.232  loss_ce_4: 0.5931  loss_mask_4: 0.3757  loss_dice_4: 1.299  loss_ce_5: 0.7025  loss_mask_5: 0.4035  loss_dice_5: 1.352  loss_ce_6: 0.678  loss_mask_6: 0.4159  loss_dice_6: 1.277  loss_ce_7: 0.6885  loss_mask_7: 0.399  loss_dice_7: 1.298  loss_ce_8: 0.7409  loss_mask_8: 0.3701  loss_dice_8: 1.276    time: 0.3205  last_time: 0.2934  data_time: 0.0041  last_data_time: 0.0019   lr: 9.3686e-05  max_mem: 14706M
[10/03 22:58:10] d2.utils.events INFO:  eta: 0:48:54  iter: 719  total_loss: 24.68  loss_ce: 0.5852  loss_mask: 0.5273  loss_dice: 1.104  loss_contrastive: 0  loss_ce_0: 2.102  loss_mask_0: 0.6194  loss_dice_0: 1.767  loss_ce_1: 1.2  loss_mask_1: 0.5428  loss_dice_1: 1.425  loss_ce_2: 0.9113  loss_mask_2: 0.4628  loss_dice_2: 1.139  loss_ce_3: 0.7881  loss_mask_3: 0.5256  loss_dice_3: 0.9903  loss_ce_4: 0.6372  loss_mask_4: 0.514  loss_dice_4: 1.106  loss_ce_5: 0.5909  loss_mask_5: 0.5544  loss_dice_5: 1.114  loss_ce_6: 0.5975  loss_mask_6: 0.4933  loss_dice_6: 1.067  loss_ce_7: 0.5284  loss_mask_7: 0.5098  loss_dice_7: 1.063  loss_ce_8: 0.6182  loss_mask_8: 0.541  loss_dice_8: 1.096    time: 0.3205  last_time: 0.3139  data_time: 0.0034  last_data_time: 0.0029   lr: 9.3505e-05  max_mem: 14706M
[10/03 22:58:16] d2.utils.events INFO:  eta: 0:48:45  iter: 739  total_loss: 26.68  loss_ce: 0.5794  loss_mask: 0.5415  loss_dice: 1.048  loss_contrastive: 0  loss_ce_0: 1.747  loss_mask_0: 0.6317  loss_dice_0: 1.943  loss_ce_1: 1.105  loss_mask_1: 0.5075  loss_dice_1: 1.191  loss_ce_2: 0.8623  loss_mask_2: 0.4353  loss_dice_2: 1.135  loss_ce_3: 0.6774  loss_mask_3: 0.4971  loss_dice_3: 1.227  loss_ce_4: 0.6235  loss_mask_4: 0.4545  loss_dice_4: 1.112  loss_ce_5: 0.5942  loss_mask_5: 0.452  loss_dice_5: 1.052  loss_ce_6: 0.6462  loss_mask_6: 0.4425  loss_dice_6: 0.9691  loss_ce_7: 0.5922  loss_mask_7: 0.4631  loss_dice_7: 1.237  loss_ce_8: 0.5367  loss_mask_8: 0.5259  loss_dice_8: 1.031    time: 0.3204  last_time: 0.3096  data_time: 0.0033  last_data_time: 0.0026   lr: 9.3324e-05  max_mem: 14706M
[10/03 22:58:23] d2.utils.events INFO:  eta: 0:48:41  iter: 759  total_loss: 31.22  loss_ce: 0.6881  loss_mask: 0.4068  loss_dice: 1.292  loss_contrastive: 0  loss_ce_0: 2.04  loss_mask_0: 0.5772  loss_dice_0: 1.685  loss_ce_1: 1.329  loss_mask_1: 0.3517  loss_dice_1: 1.4  loss_ce_2: 1.114  loss_mask_2: 0.2975  loss_dice_2: 1.461  loss_ce_3: 0.9558  loss_mask_3: 0.3084  loss_dice_3: 1.333  loss_ce_4: 0.9474  loss_mask_4: 0.339  loss_dice_4: 1.268  loss_ce_5: 0.9087  loss_mask_5: 0.3182  loss_dice_5: 1.294  loss_ce_6: 0.825  loss_mask_6: 0.2798  loss_dice_6: 1.203  loss_ce_7: 0.6653  loss_mask_7: 0.3468  loss_dice_7: 1.278  loss_ce_8: 0.6951  loss_mask_8: 0.3391  loss_dice_8: 1.247    time: 0.3208  last_time: 0.3061  data_time: 0.0036  last_data_time: 0.0032   lr: 9.3142e-05  max_mem: 14733M
[10/03 22:58:30] d2.utils.events INFO:  eta: 0:48:35  iter: 779  total_loss: 29.28  loss_ce: 0.55  loss_mask: 0.4101  loss_dice: 1.153  loss_contrastive: 0  loss_ce_0: 1.61  loss_mask_0: 0.4692  loss_dice_0: 1.694  loss_ce_1: 0.8786  loss_mask_1: 0.4492  loss_dice_1: 1.442  loss_ce_2: 0.6898  loss_mask_2: 0.3686  loss_dice_2: 1.328  loss_ce_3: 0.6501  loss_mask_3: 0.3863  loss_dice_3: 1.2  loss_ce_4: 0.6  loss_mask_4: 0.4135  loss_dice_4: 1.25  loss_ce_5: 0.5928  loss_mask_5: 0.4028  loss_dice_5: 1.285  loss_ce_6: 0.5993  loss_mask_6: 0.4439  loss_dice_6: 1.266  loss_ce_7: 0.6482  loss_mask_7: 0.4095  loss_dice_7: 1.167  loss_ce_8: 0.6042  loss_mask_8: 0.4107  loss_dice_8: 1.156    time: 0.3208  last_time: 0.3096  data_time: 0.0032  last_data_time: 0.0040   lr: 9.2961e-05  max_mem: 14733M
[10/03 22:58:36] d2.utils.events INFO:  eta: 0:48:32  iter: 799  total_loss: 30.24  loss_ce: 0.5899  loss_mask: 0.5164  loss_dice: 1.273  loss_contrastive: 0  loss_ce_0: 1.743  loss_mask_0: 0.6184  loss_dice_0: 1.676  loss_ce_1: 1.273  loss_mask_1: 0.7255  loss_dice_1: 1.943  loss_ce_2: 0.6872  loss_mask_2: 0.5514  loss_dice_2: 1.455  loss_ce_3: 0.6597  loss_mask_3: 0.5333  loss_dice_3: 1.272  loss_ce_4: 0.5843  loss_mask_4: 0.5892  loss_dice_4: 1.427  loss_ce_5: 0.6821  loss_mask_5: 0.6651  loss_dice_5: 1.382  loss_ce_6: 0.7884  loss_mask_6: 0.597  loss_dice_6: 1.364  loss_ce_7: 0.6421  loss_mask_7: 0.4921  loss_dice_7: 1.282  loss_ce_8: 0.668  loss_mask_8: 0.4873  loss_dice_8: 1.356    time: 0.3210  last_time: 0.3037  data_time: 0.0034  last_data_time: 0.0036   lr: 9.2779e-05  max_mem: 14733M
[10/03 22:58:42] d2.utils.events INFO:  eta: 0:48:25  iter: 819  total_loss: 35.04  loss_ce: 0.8929  loss_mask: 0.4118  loss_dice: 1.377  loss_contrastive: 0  loss_ce_0: 2.16  loss_mask_0: 0.6083  loss_dice_0: 2.171  loss_ce_1: 1.68  loss_mask_1: 0.5237  loss_dice_1: 2.091  loss_ce_2: 1.195  loss_mask_2: 0.4679  loss_dice_2: 1.627  loss_ce_3: 0.998  loss_mask_3: 0.3899  loss_dice_3: 1.397  loss_ce_4: 0.9307  loss_mask_4: 0.4268  loss_dice_4: 1.546  loss_ce_5: 0.9423  loss_mask_5: 0.3916  loss_dice_5: 1.353  loss_ce_6: 0.9644  loss_mask_6: 0.4267  loss_dice_6: 1.477  loss_ce_7: 0.9704  loss_mask_7: 0.4717  loss_dice_7: 1.362  loss_ce_8: 0.8751  loss_mask_8: 0.483  loss_dice_8: 1.377    time: 0.3209  last_time: 0.3268  data_time: 0.0037  last_data_time: 0.0029   lr: 9.2598e-05  max_mem: 14733M
[10/03 22:58:49] d2.utils.events INFO:  eta: 0:48:22  iter: 839  total_loss: 28.39  loss_ce: 0.725  loss_mask: 0.4941  loss_dice: 1.289  loss_contrastive: 0  loss_ce_0: 1.686  loss_mask_0: 0.6746  loss_dice_0: 1.79  loss_ce_1: 1.249  loss_mask_1: 0.557  loss_dice_1: 1.456  loss_ce_2: 0.9735  loss_mask_2: 0.5031  loss_dice_2: 1.301  loss_ce_3: 0.9629  loss_mask_3: 0.4772  loss_dice_3: 1.222  loss_ce_4: 0.8168  loss_mask_4: 0.467  loss_dice_4: 1.39  loss_ce_5: 0.6876  loss_mask_5: 0.439  loss_dice_5: 1.257  loss_ce_6: 0.7616  loss_mask_6: 0.4524  loss_dice_6: 1.092  loss_ce_7: 0.7703  loss_mask_7: 0.4682  loss_dice_7: 1.121  loss_ce_8: 0.8285  loss_mask_8: 0.4362  loss_dice_8: 1.188    time: 0.3212  last_time: 0.3326  data_time: 0.0032  last_data_time: 0.0036   lr: 9.2416e-05  max_mem: 14733M
[10/03 22:58:56] d2.utils.events INFO:  eta: 0:48:16  iter: 859  total_loss: 30.94  loss_ce: 0.8037  loss_mask: 0.6756  loss_dice: 1.431  loss_contrastive: 0  loss_ce_0: 1.836  loss_mask_0: 0.8044  loss_dice_0: 2.115  loss_ce_1: 1.072  loss_mask_1: 0.7817  loss_dice_1: 1.659  loss_ce_2: 1.055  loss_mask_2: 0.6718  loss_dice_2: 1.287  loss_ce_3: 0.9293  loss_mask_3: 0.7679  loss_dice_3: 1.383  loss_ce_4: 0.843  loss_mask_4: 0.6905  loss_dice_4: 1.545  loss_ce_5: 0.7788  loss_mask_5: 0.7811  loss_dice_5: 1.499  loss_ce_6: 0.7616  loss_mask_6: 0.7192  loss_dice_6: 1.53  loss_ce_7: 0.8036  loss_mask_7: 0.8902  loss_dice_7: 1.517  loss_ce_8: 0.7658  loss_mask_8: 0.6621  loss_dice_8: 1.531    time: 0.3212  last_time: 0.3097  data_time: 0.0031  last_data_time: 0.0024   lr: 9.2235e-05  max_mem: 14733M
[10/03 22:59:02] d2.utils.events INFO:  eta: 0:48:09  iter: 879  total_loss: 31.94  loss_ce: 0.815  loss_mask: 0.4696  loss_dice: 1.368  loss_contrastive: 0  loss_ce_0: 2.042  loss_mask_0: 0.5699  loss_dice_0: 1.665  loss_ce_1: 1.424  loss_mask_1: 0.4625  loss_dice_1: 1.487  loss_ce_2: 1.184  loss_mask_2: 0.405  loss_dice_2: 1.277  loss_ce_3: 1.103  loss_mask_3: 0.4093  loss_dice_3: 1.344  loss_ce_4: 0.9329  loss_mask_4: 0.3987  loss_dice_4: 1.441  loss_ce_5: 1  loss_mask_5: 0.3944  loss_dice_5: 1.474  loss_ce_6: 0.843  loss_mask_6: 0.4183  loss_dice_6: 1.413  loss_ce_7: 0.761  loss_mask_7: 0.4801  loss_dice_7: 1.283  loss_ce_8: 0.9206  loss_mask_8: 0.457  loss_dice_8: 1.381    time: 0.3212  last_time: 0.3015  data_time: 0.0033  last_data_time: 0.0031   lr: 9.2053e-05  max_mem: 14733M
[10/03 22:59:09] d2.utils.events INFO:  eta: 0:48:04  iter: 899  total_loss: 32.53  loss_ce: 0.8264  loss_mask: 0.5147  loss_dice: 1.308  loss_contrastive: 0  loss_ce_0: 1.456  loss_mask_0: 0.511  loss_dice_0: 1.721  loss_ce_1: 1.109  loss_mask_1: 0.4237  loss_dice_1: 1.517  loss_ce_2: 1.111  loss_mask_2: 0.517  loss_dice_2: 1.474  loss_ce_3: 0.8851  loss_mask_3: 0.5335  loss_dice_3: 1.178  loss_ce_4: 0.8423  loss_mask_4: 0.4231  loss_dice_4: 1.477  loss_ce_5: 0.8855  loss_mask_5: 0.4552  loss_dice_5: 1.306  loss_ce_6: 0.7761  loss_mask_6: 0.4274  loss_dice_6: 1.336  loss_ce_7: 0.8093  loss_mask_7: 0.4435  loss_dice_7: 1.274  loss_ce_8: 0.8751  loss_mask_8: 0.4063  loss_dice_8: 1.229    time: 0.3212  last_time: 0.3051  data_time: 0.0032  last_data_time: 0.0030   lr: 9.1871e-05  max_mem: 14733M
[10/03 22:59:15] d2.utils.events INFO:  eta: 0:47:56  iter: 919  total_loss: 32.75  loss_ce: 1.25  loss_mask: 0.4006  loss_dice: 1.455  loss_contrastive: 0  loss_ce_0: 1.79  loss_mask_0: 0.5331  loss_dice_0: 2.045  loss_ce_1: 1.301  loss_mask_1: 0.5181  loss_dice_1: 1.806  loss_ce_2: 1.151  loss_mask_2: 0.5053  loss_dice_2: 1.488  loss_ce_3: 1.01  loss_mask_3: 0.4738  loss_dice_3: 1.571  loss_ce_4: 0.8615  loss_mask_4: 0.483  loss_dice_4: 1.419  loss_ce_5: 0.8387  loss_mask_5: 0.4468  loss_dice_5: 1.401  loss_ce_6: 0.8147  loss_mask_6: 0.4226  loss_dice_6: 1.398  loss_ce_7: 0.803  loss_mask_7: 0.472  loss_dice_7: 1.328  loss_ce_8: 0.9548  loss_mask_8: 0.3969  loss_dice_8: 1.244    time: 0.3211  last_time: 0.3133  data_time: 0.0033  last_data_time: 0.0031   lr: 9.169e-05  max_mem: 14733M
[10/03 22:59:21] d2.utils.events INFO:  eta: 0:47:50  iter: 939  total_loss: 29.29  loss_ce: 0.9241  loss_mask: 0.5138  loss_dice: 1.455  loss_contrastive: 0  loss_ce_0: 1.691  loss_mask_0: 0.5192  loss_dice_0: 2.014  loss_ce_1: 1.061  loss_mask_1: 0.4557  loss_dice_1: 1.625  loss_ce_2: 0.896  loss_mask_2: 0.4687  loss_dice_2: 1.379  loss_ce_3: 0.8683  loss_mask_3: 0.4623  loss_dice_3: 1.32  loss_ce_4: 0.8798  loss_mask_4: 0.4591  loss_dice_4: 1.239  loss_ce_5: 0.7984  loss_mask_5: 0.4344  loss_dice_5: 1.232  loss_ce_6: 0.6834  loss_mask_6: 0.5068  loss_dice_6: 1.354  loss_ce_7: 0.5615  loss_mask_7: 0.5242  loss_dice_7: 1.327  loss_ce_8: 0.7653  loss_mask_8: 0.518  loss_dice_8: 1.42    time: 0.3210  last_time: 0.3354  data_time: 0.0030  last_data_time: 0.0026   lr: 9.1508e-05  max_mem: 14733M
[10/03 22:59:28] d2.utils.events INFO:  eta: 0:47:44  iter: 959  total_loss: 25.43  loss_ce: 0.6951  loss_mask: 0.4544  loss_dice: 0.9128  loss_contrastive: 0  loss_ce_0: 1.667  loss_mask_0: 0.6073  loss_dice_0: 1.831  loss_ce_1: 0.9794  loss_mask_1: 0.4378  loss_dice_1: 1.311  loss_ce_2: 0.9215  loss_mask_2: 0.4502  loss_dice_2: 0.9315  loss_ce_3: 0.7958  loss_mask_3: 0.454  loss_dice_3: 1.058  loss_ce_4: 0.7853  loss_mask_4: 0.4214  loss_dice_4: 0.9676  loss_ce_5: 0.7463  loss_mask_5: 0.459  loss_dice_5: 1.086  loss_ce_6: 0.7533  loss_mask_6: 0.4806  loss_dice_6: 0.9259  loss_ce_7: 0.6222  loss_mask_7: 0.4703  loss_dice_7: 0.9795  loss_ce_8: 0.5776  loss_mask_8: 0.4387  loss_dice_8: 1.097    time: 0.3209  last_time: 0.3517  data_time: 0.0035  last_data_time: 0.0026   lr: 9.1326e-05  max_mem: 14733M
[10/03 22:59:34] d2.utils.events INFO:  eta: 0:47:38  iter: 979  total_loss: 27.3  loss_ce: 0.4505  loss_mask: 0.6077  loss_dice: 0.9562  loss_contrastive: 0  loss_ce_0: 1.686  loss_mask_0: 0.5711  loss_dice_0: 1.522  loss_ce_1: 1.064  loss_mask_1: 0.5362  loss_dice_1: 1.172  loss_ce_2: 0.8845  loss_mask_2: 0.5302  loss_dice_2: 1.152  loss_ce_3: 0.792  loss_mask_3: 0.6209  loss_dice_3: 1.002  loss_ce_4: 0.6803  loss_mask_4: 0.5762  loss_dice_4: 1.005  loss_ce_5: 0.6613  loss_mask_5: 0.5128  loss_dice_5: 0.9998  loss_ce_6: 0.7264  loss_mask_6: 0.636  loss_dice_6: 0.9474  loss_ce_7: 0.5842  loss_mask_7: 0.6308  loss_dice_7: 0.9983  loss_ce_8: 0.5261  loss_mask_8: 0.5357  loss_dice_8: 1.062    time: 0.3209  last_time: 0.3037  data_time: 0.0031  last_data_time: 0.0020   lr: 9.1144e-05  max_mem: 14733M
[10/03 22:59:40] d2.utils.events INFO:  eta: 0:47:32  iter: 999  total_loss: 29.45  loss_ce: 0.7129  loss_mask: 0.539  loss_dice: 1.251  loss_contrastive: 0  loss_ce_0: 1.715  loss_mask_0: 0.5398  loss_dice_0: 1.81  loss_ce_1: 1.067  loss_mask_1: 0.5557  loss_dice_1: 1.518  loss_ce_2: 0.8845  loss_mask_2: 0.5133  loss_dice_2: 1.328  loss_ce_3: 0.7434  loss_mask_3: 0.4868  loss_dice_3: 1.281  loss_ce_4: 0.6673  loss_mask_4: 0.4886  loss_dice_4: 1.235  loss_ce_5: 0.702  loss_mask_5: 0.535  loss_dice_5: 1.189  loss_ce_6: 0.7293  loss_mask_6: 0.4479  loss_dice_6: 1.116  loss_ce_7: 0.7416  loss_mask_7: 0.5326  loss_dice_7: 1.244  loss_ce_8: 0.6305  loss_mask_8: 0.5018  loss_dice_8: 1.246    time: 0.3209  last_time: 0.3051  data_time: 0.0035  last_data_time: 0.0031   lr: 9.0962e-05  max_mem: 14733M
[10/03 22:59:47] d2.utils.events INFO:  eta: 0:47:26  iter: 1019  total_loss: 26.24  loss_ce: 0.4718  loss_mask: 0.2987  loss_dice: 1.04  loss_contrastive: 0  loss_ce_0: 1.676  loss_mask_0: 0.7021  loss_dice_0: 1.786  loss_ce_1: 0.9432  loss_mask_1: 0.4  loss_dice_1: 1.23  loss_ce_2: 0.723  loss_mask_2: 0.5106  loss_dice_2: 1.296  loss_ce_3: 0.5643  loss_mask_3: 0.4948  loss_dice_3: 1.06  loss_ce_4: 0.5168  loss_mask_4: 0.3539  loss_dice_4: 1.212  loss_ce_5: 0.4299  loss_mask_5: 0.3355  loss_dice_5: 1.133  loss_ce_6: 0.5047  loss_mask_6: 0.3611  loss_dice_6: 1.065  loss_ce_7: 0.4302  loss_mask_7: 0.3702  loss_dice_7: 1.088  loss_ce_8: 0.4034  loss_mask_8: 0.304  loss_dice_8: 1.073    time: 0.3208  last_time: 0.3063  data_time: 0.0030  last_data_time: 0.0036   lr: 9.078e-05  max_mem: 14733M
[10/03 22:59:53] d2.utils.events INFO:  eta: 0:47:20  iter: 1039  total_loss: 31.99  loss_ce: 0.7896  loss_mask: 0.3109  loss_dice: 1.52  loss_contrastive: 0  loss_ce_0: 2.273  loss_mask_0: 0.4784  loss_dice_0: 2.309  loss_ce_1: 1.362  loss_mask_1: 0.3539  loss_dice_1: 1.836  loss_ce_2: 1.1  loss_mask_2: 0.3612  loss_dice_2: 1.574  loss_ce_3: 0.9147  loss_mask_3: 0.3532  loss_dice_3: 1.577  loss_ce_4: 0.875  loss_mask_4: 0.3293  loss_dice_4: 1.493  loss_ce_5: 0.8093  loss_mask_5: 0.333  loss_dice_5: 1.545  loss_ce_6: 0.9048  loss_mask_6: 0.2919  loss_dice_6: 1.511  loss_ce_7: 0.8812  loss_mask_7: 0.3292  loss_dice_7: 1.426  loss_ce_8: 0.8069  loss_mask_8: 0.321  loss_dice_8: 1.523    time: 0.3210  last_time: 0.3348  data_time: 0.0040  last_data_time: 0.0037   lr: 9.0598e-05  max_mem: 14733M
[10/03 23:00:00] d2.utils.events INFO:  eta: 0:47:16  iter: 1059  total_loss: 36.11  loss_ce: 0.9036  loss_mask: 0.5156  loss_dice: 1.589  loss_contrastive: 0  loss_ce_0: 2.031  loss_mask_0: 0.6629  loss_dice_0: 2.097  loss_ce_1: 1.514  loss_mask_1: 0.5323  loss_dice_1: 1.949  loss_ce_2: 1.3  loss_mask_2: 0.62  loss_dice_2: 1.756  loss_ce_3: 1.297  loss_mask_3: 0.769  loss_dice_3: 1.744  loss_ce_4: 1.077  loss_mask_4: 0.6172  loss_dice_4: 1.734  loss_ce_5: 1.045  loss_mask_5: 0.6451  loss_dice_5: 1.661  loss_ce_6: 0.9359  loss_mask_6: 0.5561  loss_dice_6: 1.544  loss_ce_7: 0.847  loss_mask_7: 0.5301  loss_dice_7: 1.524  loss_ce_8: 0.9147  loss_mask_8: 0.4328  loss_dice_8: 1.603    time: 0.3211  last_time: 0.3314  data_time: 0.0033  last_data_time: 0.0060   lr: 9.0416e-05  max_mem: 14733M
[10/03 23:00:06] d2.utils.events INFO:  eta: 0:47:11  iter: 1079  total_loss: 32.44  loss_ce: 0.8467  loss_mask: 0.6612  loss_dice: 1.359  loss_contrastive: 0  loss_ce_0: 1.938  loss_mask_0: 1.008  loss_dice_0: 2.708  loss_ce_1: 1.512  loss_mask_1: 0.5309  loss_dice_1: 1.874  loss_ce_2: 1.056  loss_mask_2: 0.6388  loss_dice_2: 1.651  loss_ce_3: 1.117  loss_mask_3: 0.7002  loss_dice_3: 1.459  loss_ce_4: 1.101  loss_mask_4: 0.5638  loss_dice_4: 1.446  loss_ce_5: 0.8999  loss_mask_5: 0.5733  loss_dice_5: 1.481  loss_ce_6: 0.8378  loss_mask_6: 0.5711  loss_dice_6: 1.391  loss_ce_7: 0.7875  loss_mask_7: 0.6536  loss_dice_7: 1.415  loss_ce_8: 0.9864  loss_mask_8: 0.6959  loss_dice_8: 1.446    time: 0.3210  last_time: 0.3205  data_time: 0.0030  last_data_time: 0.0029   lr: 9.0234e-05  max_mem: 14733M
[10/03 23:00:13] d2.utils.events INFO:  eta: 0:47:06  iter: 1099  total_loss: 31.03  loss_ce: 0.5468  loss_mask: 0.5689  loss_dice: 1.298  loss_contrastive: 0  loss_ce_0: 2.243  loss_mask_0: 0.7057  loss_dice_0: 2.516  loss_ce_1: 1.389  loss_mask_1: 0.5802  loss_dice_1: 1.461  loss_ce_2: 0.8799  loss_mask_2: 0.6017  loss_dice_2: 1.352  loss_ce_3: 0.7938  loss_mask_3: 0.6435  loss_dice_3: 1.463  loss_ce_4: 0.739  loss_mask_4: 0.5808  loss_dice_4: 1.374  loss_ce_5: 0.6792  loss_mask_5: 0.5386  loss_dice_5: 1.358  loss_ce_6: 0.6262  loss_mask_6: 0.6177  loss_dice_6: 1.371  loss_ce_7: 0.5806  loss_mask_7: 0.5826  loss_dice_7: 1.395  loss_ce_8: 0.7346  loss_mask_8: 0.5784  loss_dice_8: 1.354    time: 0.3210  last_time: 0.3091  data_time: 0.0033  last_data_time: 0.0029   lr: 9.0052e-05  max_mem: 14733M
[10/03 23:00:19] d2.utils.events INFO:  eta: 0:47:00  iter: 1119  total_loss: 31.26  loss_ce: 0.6995  loss_mask: 0.4157  loss_dice: 1.423  loss_contrastive: 0  loss_ce_0: 2.316  loss_mask_0: 0.5983  loss_dice_0: 2.345  loss_ce_1: 1.407  loss_mask_1: 0.4016  loss_dice_1: 1.691  loss_ce_2: 1.115  loss_mask_2: 0.4148  loss_dice_2: 1.563  loss_ce_3: 0.9633  loss_mask_3: 0.3848  loss_dice_3: 1.512  loss_ce_4: 0.8312  loss_mask_4: 0.3831  loss_dice_4: 1.517  loss_ce_5: 0.8105  loss_mask_5: 0.386  loss_dice_5: 1.507  loss_ce_6: 0.771  loss_mask_6: 0.392  loss_dice_6: 1.399  loss_ce_7: 0.726  loss_mask_7: 0.3671  loss_dice_7: 1.399  loss_ce_8: 0.7097  loss_mask_8: 0.4128  loss_dice_8: 1.405    time: 0.3210  last_time: 0.3285  data_time: 0.0034  last_data_time: 0.0028   lr: 8.987e-05  max_mem: 14733M
[10/03 23:00:26] d2.utils.events INFO:  eta: 0:46:53  iter: 1139  total_loss: 23.71  loss_ce: 0.4766  loss_mask: 0.4297  loss_dice: 1.006  loss_contrastive: 0  loss_ce_0: 1.536  loss_mask_0: 0.523  loss_dice_0: 1.834  loss_ce_1: 1.072  loss_mask_1: 0.4597  loss_dice_1: 1.074  loss_ce_2: 0.804  loss_mask_2: 0.4683  loss_dice_2: 1.041  loss_ce_3: 0.6198  loss_mask_3: 0.4385  loss_dice_3: 0.9405  loss_ce_4: 0.5591  loss_mask_4: 0.4294  loss_dice_4: 1.091  loss_ce_5: 0.4508  loss_mask_5: 0.4248  loss_dice_5: 1.067  loss_ce_6: 0.4784  loss_mask_6: 0.4471  loss_dice_6: 0.9831  loss_ce_7: 0.4259  loss_mask_7: 0.4338  loss_dice_7: 1.055  loss_ce_8: 0.4833  loss_mask_8: 0.4315  loss_dice_8: 1.07    time: 0.3210  last_time: 0.3304  data_time: 0.0031  last_data_time: 0.0043   lr: 8.9688e-05  max_mem: 14733M
[10/03 23:00:32] d2.utils.events INFO:  eta: 0:46:45  iter: 1159  total_loss: 32.25  loss_ce: 0.8394  loss_mask: 0.4032  loss_dice: 1.273  loss_contrastive: 0  loss_ce_0: 1.577  loss_mask_0: 0.4548  loss_dice_0: 1.856  loss_ce_1: 1.15  loss_mask_1: 0.4117  loss_dice_1: 1.689  loss_ce_2: 0.9834  loss_mask_2: 0.43  loss_dice_2: 1.443  loss_ce_3: 0.9893  loss_mask_3: 0.3904  loss_dice_3: 1.347  loss_ce_4: 0.8378  loss_mask_4: 0.3983  loss_dice_4: 1.348  loss_ce_5: 0.7669  loss_mask_5: 0.3529  loss_dice_5: 1.321  loss_ce_6: 0.7413  loss_mask_6: 0.3878  loss_dice_6: 1.274  loss_ce_7: 0.7703  loss_mask_7: 0.3635  loss_dice_7: 1.278  loss_ce_8: 0.8417  loss_mask_8: 0.3658  loss_dice_8: 1.281    time: 0.3210  last_time: 0.3265  data_time: 0.0034  last_data_time: 0.0042   lr: 8.9506e-05  max_mem: 14733M
[10/03 23:00:38] d2.utils.events INFO:  eta: 0:46:34  iter: 1179  total_loss: 26.78  loss_ce: 0.4932  loss_mask: 0.404  loss_dice: 1.182  loss_contrastive: 0  loss_ce_0: 1.564  loss_mask_0: 0.4226  loss_dice_0: 1.672  loss_ce_1: 0.9003  loss_mask_1: 0.4207  loss_dice_1: 1.319  loss_ce_2: 0.9763  loss_mask_2: 0.3794  loss_dice_2: 1.225  loss_ce_3: 0.9043  loss_mask_3: 0.3767  loss_dice_3: 1.134  loss_ce_4: 0.7837  loss_mask_4: 0.4065  loss_dice_4: 1.209  loss_ce_5: 0.6722  loss_mask_5: 0.3835  loss_dice_5: 1.097  loss_ce_6: 0.7834  loss_mask_6: 0.3799  loss_dice_6: 1.092  loss_ce_7: 0.6701  loss_mask_7: 0.3716  loss_dice_7: 1.115  loss_ce_8: 0.5396  loss_mask_8: 0.3722  loss_dice_8: 1.108    time: 0.3210  last_time: 0.3080  data_time: 0.0030  last_data_time: 0.0024   lr: 8.9324e-05  max_mem: 14733M
[10/03 23:00:45] d2.utils.events INFO:  eta: 0:46:29  iter: 1199  total_loss: 27.2  loss_ce: 0.6484  loss_mask: 0.4442  loss_dice: 1.139  loss_contrastive: 0  loss_ce_0: 1.594  loss_mask_0: 0.4199  loss_dice_0: 1.698  loss_ce_1: 1.157  loss_mask_1: 0.4066  loss_dice_1: 1.38  loss_ce_2: 0.7937  loss_mask_2: 0.529  loss_dice_2: 1.273  loss_ce_3: 0.6229  loss_mask_3: 0.4004  loss_dice_3: 1.169  loss_ce_4: 0.689  loss_mask_4: 0.5556  loss_dice_4: 1.218  loss_ce_5: 0.634  loss_mask_5: 0.5136  loss_dice_5: 1.171  loss_ce_6: 0.6597  loss_mask_6: 0.4509  loss_dice_6: 1.072  loss_ce_7: 0.6231  loss_mask_7: 0.4866  loss_dice_7: 1.233  loss_ce_8: 0.5754  loss_mask_8: 0.4122  loss_dice_8: 1.188    time: 0.3210  last_time: 0.3215  data_time: 0.0034  last_data_time: 0.0036   lr: 8.9141e-05  max_mem: 14733M
[10/03 23:00:51] d2.utils.events INFO:  eta: 0:46:23  iter: 1219  total_loss: 23.54  loss_ce: 0.5305  loss_mask: 0.3564  loss_dice: 1.37  loss_contrastive: 0  loss_ce_0: 1.675  loss_mask_0: 0.3287  loss_dice_0: 1.618  loss_ce_1: 1.067  loss_mask_1: 0.2943  loss_dice_1: 1.237  loss_ce_2: 0.7955  loss_mask_2: 0.3057  loss_dice_2: 1.288  loss_ce_3: 0.701  loss_mask_3: 0.279  loss_dice_3: 1.263  loss_ce_4: 0.5604  loss_mask_4: 0.3449  loss_dice_4: 1.364  loss_ce_5: 0.5608  loss_mask_5: 0.2934  loss_dice_5: 1.325  loss_ce_6: 0.585  loss_mask_6: 0.2671  loss_dice_6: 1.34  loss_ce_7: 0.6034  loss_mask_7: 0.2819  loss_dice_7: 1.299  loss_ce_8: 0.5462  loss_mask_8: 0.3204  loss_dice_8: 1.287    time: 0.3210  last_time: 0.3261  data_time: 0.0033  last_data_time: 0.0055   lr: 8.8959e-05  max_mem: 14733M
[10/03 23:00:58] d2.utils.events INFO:  eta: 0:46:17  iter: 1239  total_loss: 23.34  loss_ce: 0.3248  loss_mask: 0.31  loss_dice: 1.033  loss_contrastive: 0  loss_ce_0: 1.642  loss_mask_0: 0.328  loss_dice_0: 1.292  loss_ce_1: 0.8462  loss_mask_1: 0.3759  loss_dice_1: 1.054  loss_ce_2: 0.6518  loss_mask_2: 0.3647  loss_dice_2: 1.048  loss_ce_3: 0.5546  loss_mask_3: 0.3189  loss_dice_3: 1.024  loss_ce_4: 0.4608  loss_mask_4: 0.3318  loss_dice_4: 0.9306  loss_ce_5: 0.3744  loss_mask_5: 0.2943  loss_dice_5: 0.919  loss_ce_6: 0.4175  loss_mask_6: 0.3015  loss_dice_6: 0.9159  loss_ce_7: 0.3346  loss_mask_7: 0.2999  loss_dice_7: 0.9757  loss_ce_8: 0.3158  loss_mask_8: 0.2676  loss_dice_8: 1.023    time: 0.3210  last_time: 0.3024  data_time: 0.0032  last_data_time: 0.0035   lr: 8.8777e-05  max_mem: 14733M
[10/03 23:01:04] d2.utils.events INFO:  eta: 0:46:13  iter: 1259  total_loss: 26.68  loss_ce: 0.4222  loss_mask: 0.5605  loss_dice: 1.277  loss_contrastive: 0  loss_ce_0: 1.335  loss_mask_0: 0.5498  loss_dice_0: 1.429  loss_ce_1: 0.927  loss_mask_1: 0.4978  loss_dice_1: 1.385  loss_ce_2: 0.7127  loss_mask_2: 0.5169  loss_dice_2: 1.381  loss_ce_3: 0.5397  loss_mask_3: 0.501  loss_dice_3: 1.246  loss_ce_4: 0.4723  loss_mask_4: 0.5244  loss_dice_4: 1.241  loss_ce_5: 0.504  loss_mask_5: 0.5046  loss_dice_5: 1.105  loss_ce_6: 0.531  loss_mask_6: 0.5061  loss_dice_6: 1.256  loss_ce_7: 0.4909  loss_mask_7: 0.5154  loss_dice_7: 1.159  loss_ce_8: 0.6071  loss_mask_8: 0.52  loss_dice_8: 1.267    time: 0.3209  last_time: 0.2949  data_time: 0.0031  last_data_time: 0.0023   lr: 8.8594e-05  max_mem: 14733M
[10/03 23:01:10] d2.utils.events INFO:  eta: 0:46:04  iter: 1279  total_loss: 24.41  loss_ce: 0.5436  loss_mask: 0.4915  loss_dice: 1.045  loss_contrastive: 0  loss_ce_0: 1.468  loss_mask_0: 0.5635  loss_dice_0: 1.291  loss_ce_1: 0.8727  loss_mask_1: 0.5256  loss_dice_1: 1.032  loss_ce_2: 0.6439  loss_mask_2: 0.4333  loss_dice_2: 1.155  loss_ce_3: 0.5576  loss_mask_3: 0.4769  loss_dice_3: 1.217  loss_ce_4: 0.5277  loss_mask_4: 0.5217  loss_dice_4: 0.9998  loss_ce_5: 0.5833  loss_mask_5: 0.5041  loss_dice_5: 1.207  loss_ce_6: 0.6342  loss_mask_6: 0.4951  loss_dice_6: 1.067  loss_ce_7: 0.5228  loss_mask_7: 0.4915  loss_dice_7: 1.011  loss_ce_8: 0.5441  loss_mask_8: 0.5109  loss_dice_8: 1.138    time: 0.3209  last_time: 0.3036  data_time: 0.0032  last_data_time: 0.0027   lr: 8.8412e-05  max_mem: 14733M
[10/03 23:01:17] d2.utils.events INFO:  eta: 0:46:00  iter: 1299  total_loss: 28.8  loss_ce: 0.6285  loss_mask: 0.3791  loss_dice: 1.154  loss_contrastive: 0  loss_ce_0: 1.358  loss_mask_0: 0.4151  loss_dice_0: 1.406  loss_ce_1: 0.9517  loss_mask_1: 0.415  loss_dice_1: 1.267  loss_ce_2: 0.6981  loss_mask_2: 0.3752  loss_dice_2: 1.213  loss_ce_3: 0.6674  loss_mask_3: 0.3782  loss_dice_3: 1.212  loss_ce_4: 0.6176  loss_mask_4: 0.4381  loss_dice_4: 1.264  loss_ce_5: 0.523  loss_mask_5: 0.3881  loss_dice_5: 1.246  loss_ce_6: 0.5261  loss_mask_6: 0.3818  loss_dice_6: 1.134  loss_ce_7: 0.6452  loss_mask_7: 0.381  loss_dice_7: 1.132  loss_ce_8: 0.6854  loss_mask_8: 0.3927  loss_dice_8: 1.267    time: 0.3208  last_time: 0.2991  data_time: 0.0029  last_data_time: 0.0019   lr: 8.8229e-05  max_mem: 14733M
[10/03 23:01:23] d2.utils.events INFO:  eta: 0:45:55  iter: 1319  total_loss: 27.08  loss_ce: 0.7867  loss_mask: 0.5162  loss_dice: 1.083  loss_contrastive: 0  loss_ce_0: 1.655  loss_mask_0: 0.4897  loss_dice_0: 1.472  loss_ce_1: 1.102  loss_mask_1: 0.4012  loss_dice_1: 1.118  loss_ce_2: 0.923  loss_mask_2: 0.4091  loss_dice_2: 1.048  loss_ce_3: 0.7939  loss_mask_3: 0.4437  loss_dice_3: 1.076  loss_ce_4: 0.745  loss_mask_4: 0.4556  loss_dice_4: 1.115  loss_ce_5: 0.7489  loss_mask_5: 0.3919  loss_dice_5: 1.171  loss_ce_6: 0.7071  loss_mask_6: 0.4179  loss_dice_6: 1.148  loss_ce_7: 0.6764  loss_mask_7: 0.4363  loss_dice_7: 1.063  loss_ce_8: 0.6399  loss_mask_8: 0.4135  loss_dice_8: 1.125    time: 0.3208  last_time: 0.2948  data_time: 0.0034  last_data_time: 0.0020   lr: 8.8047e-05  max_mem: 14733M
[10/03 23:01:30] d2.utils.events INFO:  eta: 0:45:50  iter: 1339  total_loss: 25.47  loss_ce: 0.4352  loss_mask: 0.3425  loss_dice: 1.186  loss_contrastive: 0  loss_ce_0: 1.593  loss_mask_0: 0.4636  loss_dice_0: 1.666  loss_ce_1: 0.752  loss_mask_1: 0.4573  loss_dice_1: 1.252  loss_ce_2: 0.5783  loss_mask_2: 0.3864  loss_dice_2: 1.217  loss_ce_3: 0.4442  loss_mask_3: 0.3733  loss_dice_3: 1.161  loss_ce_4: 0.434  loss_mask_4: 0.4253  loss_dice_4: 1.365  loss_ce_5: 0.3599  loss_mask_5: 0.413  loss_dice_5: 1.242  loss_ce_6: 0.4393  loss_mask_6: 0.3446  loss_dice_6: 1.296  loss_ce_7: 0.4529  loss_mask_7: 0.3538  loss_dice_7: 1.203  loss_ce_8: 0.4497  loss_mask_8: 0.3522  loss_dice_8: 1.208    time: 0.3208  last_time: 0.3044  data_time: 0.0034  last_data_time: 0.0031   lr: 8.7864e-05  max_mem: 14733M
[10/03 23:01:36] d2.utils.events INFO:  eta: 0:45:47  iter: 1359  total_loss: 21.77  loss_ce: 0.4861  loss_mask: 0.2817  loss_dice: 1.098  loss_contrastive: 0  loss_ce_0: 1.491  loss_mask_0: 0.3936  loss_dice_0: 1.418  loss_ce_1: 1.056  loss_mask_1: 0.3123  loss_dice_1: 1.117  loss_ce_2: 0.8986  loss_mask_2: 0.3051  loss_dice_2: 1.084  loss_ce_3: 0.6873  loss_mask_3: 0.3016  loss_dice_3: 1.01  loss_ce_4: 0.5627  loss_mask_4: 0.3058  loss_dice_4: 1.096  loss_ce_5: 0.4904  loss_mask_5: 0.3591  loss_dice_5: 1.026  loss_ce_6: 0.5646  loss_mask_6: 0.3001  loss_dice_6: 1.038  loss_ce_7: 0.5325  loss_mask_7: 0.3085  loss_dice_7: 1.029  loss_ce_8: 0.5796  loss_mask_8: 0.3251  loss_dice_8: 0.9976    time: 0.3208  last_time: 0.3223  data_time: 0.0034  last_data_time: 0.0027   lr: 8.7681e-05  max_mem: 14733M
[10/03 23:01:42] d2.utils.events INFO:  eta: 0:45:39  iter: 1379  total_loss: 25.31  loss_ce: 0.4175  loss_mask: 0.4025  loss_dice: 1.235  loss_contrastive: 0  loss_ce_0: 1.652  loss_mask_0: 0.4296  loss_dice_0: 1.599  loss_ce_1: 0.9616  loss_mask_1: 0.3602  loss_dice_1: 1.306  loss_ce_2: 0.7255  loss_mask_2: 0.4008  loss_dice_2: 1.278  loss_ce_3: 0.6317  loss_mask_3: 0.3965  loss_dice_3: 1.23  loss_ce_4: 0.5091  loss_mask_4: 0.3909  loss_dice_4: 1.248  loss_ce_5: 0.643  loss_mask_5: 0.3936  loss_dice_5: 1.281  loss_ce_6: 0.4534  loss_mask_6: 0.4347  loss_dice_6: 1.238  loss_ce_7: 0.4463  loss_mask_7: 0.4235  loss_dice_7: 1.124  loss_ce_8: 0.4861  loss_mask_8: 0.3814  loss_dice_8: 1.322    time: 0.3207  last_time: 0.3103  data_time: 0.0031  last_data_time: 0.0035   lr: 8.7499e-05  max_mem: 14733M
[10/03 23:01:49] d2.utils.events INFO:  eta: 0:45:31  iter: 1399  total_loss: 18.57  loss_ce: 0.3657  loss_mask: 0.3503  loss_dice: 0.8377  loss_contrastive: 0  loss_ce_0: 1.336  loss_mask_0: 0.3877  loss_dice_0: 1.007  loss_ce_1: 0.6889  loss_mask_1: 0.3601  loss_dice_1: 0.8512  loss_ce_2: 0.5461  loss_mask_2: 0.3392  loss_dice_2: 0.8598  loss_ce_3: 0.4774  loss_mask_3: 0.3371  loss_dice_3: 0.8586  loss_ce_4: 0.4145  loss_mask_4: 0.3436  loss_dice_4: 0.8179  loss_ce_5: 0.3619  loss_mask_5: 0.3372  loss_dice_5: 0.8566  loss_ce_6: 0.4583  loss_mask_6: 0.3372  loss_dice_6: 0.8288  loss_ce_7: 0.4237  loss_mask_7: 0.3225  loss_dice_7: 0.7994  loss_ce_8: 0.3293  loss_mask_8: 0.3341  loss_dice_8: 0.8578    time: 0.3207  last_time: 0.3033  data_time: 0.0029  last_data_time: 0.0028   lr: 8.7316e-05  max_mem: 14733M
[10/03 23:01:55] d2.utils.events INFO:  eta: 0:45:24  iter: 1419  total_loss: 22.16  loss_ce: 0.4197  loss_mask: 0.3411  loss_dice: 1.002  loss_contrastive: 0  loss_ce_0: 1.229  loss_mask_0: 0.3954  loss_dice_0: 1.281  loss_ce_1: 1.036  loss_mask_1: 0.3885  loss_dice_1: 0.9918  loss_ce_2: 0.7187  loss_mask_2: 0.329  loss_dice_2: 1.002  loss_ce_3: 0.5641  loss_mask_3: 0.3446  loss_dice_3: 0.9595  loss_ce_4: 0.5601  loss_mask_4: 0.3527  loss_dice_4: 1.062  loss_ce_5: 0.4965  loss_mask_5: 0.3688  loss_dice_5: 1.038  loss_ce_6: 0.4673  loss_mask_6: 0.324  loss_dice_6: 0.9828  loss_ce_7: 0.4509  loss_mask_7: 0.3164  loss_dice_7: 1.026  loss_ce_8: 0.4459  loss_mask_8: 0.3194  loss_dice_8: 1.02    time: 0.3207  last_time: 0.3105  data_time: 0.0032  last_data_time: 0.0029   lr: 8.7133e-05  max_mem: 14733M
[10/03 23:02:02] d2.utils.events INFO:  eta: 0:45:18  iter: 1439  total_loss: 20.6  loss_ce: 0.3062  loss_mask: 0.4115  loss_dice: 1.09  loss_contrastive: 0  loss_ce_0: 1.577  loss_mask_0: 0.4597  loss_dice_0: 1.347  loss_ce_1: 0.9626  loss_mask_1: 0.4488  loss_dice_1: 1.085  loss_ce_2: 0.6209  loss_mask_2: 0.4473  loss_dice_2: 1.064  loss_ce_3: 0.4936  loss_mask_3: 0.4166  loss_dice_3: 1.068  loss_ce_4: 0.4903  loss_mask_4: 0.396  loss_dice_4: 1.016  loss_ce_5: 0.3456  loss_mask_5: 0.3738  loss_dice_5: 1.088  loss_ce_6: 0.3554  loss_mask_6: 0.3819  loss_dice_6: 1.058  loss_ce_7: 0.3319  loss_mask_7: 0.4104  loss_dice_7: 1.085  loss_ce_8: 0.2961  loss_mask_8: 0.4045  loss_dice_8: 1.088    time: 0.3208  last_time: 0.3050  data_time: 0.0031  last_data_time: 0.0031   lr: 8.695e-05  max_mem: 14733M
[10/03 23:02:08] d2.utils.events INFO:  eta: 0:45:12  iter: 1459  total_loss: 21.16  loss_ce: 0.5089  loss_mask: 0.361  loss_dice: 0.9733  loss_contrastive: 0  loss_ce_0: 1.538  loss_mask_0: 0.4164  loss_dice_0: 1.394  loss_ce_1: 1.014  loss_mask_1: 0.3572  loss_dice_1: 1.1  loss_ce_2: 0.7242  loss_mask_2: 0.3421  loss_dice_2: 1.164  loss_ce_3: 0.6823  loss_mask_3: 0.325  loss_dice_3: 1.139  loss_ce_4: 0.567  loss_mask_4: 0.3621  loss_dice_4: 1.119  loss_ce_5: 0.5007  loss_mask_5: 0.3142  loss_dice_5: 1.089  loss_ce_6: 0.4521  loss_mask_6: 0.3484  loss_dice_6: 1.019  loss_ce_7: 0.462  loss_mask_7: 0.346  loss_dice_7: 1.059  loss_ce_8: 0.5311  loss_mask_8: 0.3196  loss_dice_8: 1.023    time: 0.3208  last_time: 0.3034  data_time: 0.0036  last_data_time: 0.0030   lr: 8.6768e-05  max_mem: 14733M
[10/03 23:02:15] d2.utils.events INFO:  eta: 0:45:06  iter: 1479  total_loss: 22.16  loss_ce: 0.46  loss_mask: 0.3005  loss_dice: 0.8476  loss_contrastive: 0  loss_ce_0: 1.422  loss_mask_0: 0.4939  loss_dice_0: 1.357  loss_ce_1: 0.9755  loss_mask_1: 0.2361  loss_dice_1: 0.948  loss_ce_2: 0.6903  loss_mask_2: 0.2581  loss_dice_2: 0.8888  loss_ce_3: 0.5065  loss_mask_3: 0.3617  loss_dice_3: 0.8077  loss_ce_4: 0.4955  loss_mask_4: 0.2987  loss_dice_4: 1  loss_ce_5: 0.5198  loss_mask_5: 0.3503  loss_dice_5: 1.065  loss_ce_6: 0.5418  loss_mask_6: 0.2966  loss_dice_6: 0.8935  loss_ce_7: 0.4724  loss_mask_7: 0.3221  loss_dice_7: 0.9594  loss_ce_8: 0.426  loss_mask_8: 0.3521  loss_dice_8: 0.9561    time: 0.3208  last_time: 0.3426  data_time: 0.0029  last_data_time: 0.0038   lr: 8.6585e-05  max_mem: 14733M
[10/03 23:02:21] d2.utils.events INFO:  eta: 0:44:59  iter: 1499  total_loss: 20.31  loss_ce: 0.3223  loss_mask: 0.4073  loss_dice: 0.8923  loss_contrastive: 0  loss_ce_0: 1.207  loss_mask_0: 0.4581  loss_dice_0: 1.142  loss_ce_1: 0.6775  loss_mask_1: 0.4064  loss_dice_1: 1.087  loss_ce_2: 0.4931  loss_mask_2: 0.4598  loss_dice_2: 1.069  loss_ce_3: 0.4486  loss_mask_3: 0.4319  loss_dice_3: 0.8569  loss_ce_4: 0.4313  loss_mask_4: 0.4451  loss_dice_4: 0.9292  loss_ce_5: 0.5172  loss_mask_5: 0.4456  loss_dice_5: 0.9952  loss_ce_6: 0.3211  loss_mask_6: 0.4303  loss_dice_6: 0.8758  loss_ce_7: 0.309  loss_mask_7: 0.425  loss_dice_7: 0.8477  loss_ce_8: 0.3399  loss_mask_8: 0.4201  loss_dice_8: 0.8267    time: 0.3207  last_time: 0.3039  data_time: 0.0032  last_data_time: 0.0026   lr: 8.6402e-05  max_mem: 14733M
[10/03 23:02:27] d2.utils.events INFO:  eta: 0:44:51  iter: 1519  total_loss: 23.73  loss_ce: 0.5453  loss_mask: 0.518  loss_dice: 1.124  loss_contrastive: 0  loss_ce_0: 1.742  loss_mask_0: 0.5861  loss_dice_0: 1.549  loss_ce_1: 0.9182  loss_mask_1: 0.5147  loss_dice_1: 1.244  loss_ce_2: 0.7045  loss_mask_2: 0.4899  loss_dice_2: 1.087  loss_ce_3: 0.6002  loss_mask_3: 0.4792  loss_dice_3: 1.105  loss_ce_4: 0.651  loss_mask_4: 0.5028  loss_dice_4: 1.154  loss_ce_5: 0.6967  loss_mask_5: 0.5085  loss_dice_5: 1.142  loss_ce_6: 0.594  loss_mask_6: 0.4882  loss_dice_6: 1.087  loss_ce_7: 0.4364  loss_mask_7: 0.5003  loss_dice_7: 1.126  loss_ce_8: 0.598  loss_mask_8: 0.4751  loss_dice_8: 1.096    time: 0.3206  last_time: 0.3231  data_time: 0.0033  last_data_time: 0.0026   lr: 8.6219e-05  max_mem: 14733M
[10/03 23:02:34] d2.utils.events INFO:  eta: 0:44:41  iter: 1539  total_loss: 25.56  loss_ce: 0.6513  loss_mask: 0.408  loss_dice: 0.9683  loss_contrastive: 0  loss_ce_0: 1.552  loss_mask_0: 0.526  loss_dice_0: 1.52  loss_ce_1: 0.9695  loss_mask_1: 0.416  loss_dice_1: 1.255  loss_ce_2: 0.9267  loss_mask_2: 0.4325  loss_dice_2: 1.035  loss_ce_3: 0.7298  loss_mask_3: 0.4006  loss_dice_3: 1.041  loss_ce_4: 0.7415  loss_mask_4: 0.3709  loss_dice_4: 1.087  loss_ce_5: 0.7852  loss_mask_5: 0.3673  loss_dice_5: 1.012  loss_ce_6: 0.8463  loss_mask_6: 0.3842  loss_dice_6: 0.9009  loss_ce_7: 0.6681  loss_mask_7: 0.3504  loss_dice_7: 0.9537  loss_ce_8: 0.6325  loss_mask_8: 0.3882  loss_dice_8: 1.083    time: 0.3206  last_time: 0.3121  data_time: 0.0030  last_data_time: 0.0041   lr: 8.6036e-05  max_mem: 14733M
[10/03 23:02:40] d2.utils.events INFO:  eta: 0:44:36  iter: 1559  total_loss: 21.99  loss_ce: 0.497  loss_mask: 0.5298  loss_dice: 0.9221  loss_contrastive: 0  loss_ce_0: 1.747  loss_mask_0: 0.5432  loss_dice_0: 1.366  loss_ce_1: 1.004  loss_mask_1: 0.5336  loss_dice_1: 1.036  loss_ce_2: 0.6506  loss_mask_2: 0.5353  loss_dice_2: 0.9715  loss_ce_3: 0.5712  loss_mask_3: 0.4689  loss_dice_3: 0.9366  loss_ce_4: 0.5914  loss_mask_4: 0.4936  loss_dice_4: 0.8517  loss_ce_5: 0.5227  loss_mask_5: 0.445  loss_dice_5: 1.052  loss_ce_6: 0.4784  loss_mask_6: 0.4647  loss_dice_6: 0.9019  loss_ce_7: 0.4987  loss_mask_7: 0.4787  loss_dice_7: 0.9282  loss_ce_8: 0.5647  loss_mask_8: 0.4905  loss_dice_8: 0.8216    time: 0.3206  last_time: 0.3074  data_time: 0.0039  last_data_time: 0.0035   lr: 8.5853e-05  max_mem: 14733M
[10/03 23:02:46] d2.utils.events INFO:  eta: 0:44:27  iter: 1579  total_loss: 23.8  loss_ce: 0.5253  loss_mask: 0.3924  loss_dice: 1.146  loss_contrastive: 0  loss_ce_0: 1.301  loss_mask_0: 0.5142  loss_dice_0: 1.535  loss_ce_1: 0.7963  loss_mask_1: 0.4097  loss_dice_1: 1.266  loss_ce_2: 0.5737  loss_mask_2: 0.4818  loss_dice_2: 1.238  loss_ce_3: 0.5792  loss_mask_3: 0.4365  loss_dice_3: 1.145  loss_ce_4: 0.458  loss_mask_4: 0.4587  loss_dice_4: 1.143  loss_ce_5: 0.5053  loss_mask_5: 0.4105  loss_dice_5: 1.23  loss_ce_6: 0.5141  loss_mask_6: 0.4013  loss_dice_6: 1.137  loss_ce_7: 0.4618  loss_mask_7: 0.3811  loss_dice_7: 1.146  loss_ce_8: 0.5111  loss_mask_8: 0.3793  loss_dice_8: 1.18    time: 0.3205  last_time: 0.3109  data_time: 0.0031  last_data_time: 0.0028   lr: 8.567e-05  max_mem: 14733M
[10/03 23:02:53] d2.utils.events INFO:  eta: 0:44:20  iter: 1599  total_loss: 25.06  loss_ce: 0.4914  loss_mask: 0.3796  loss_dice: 1.089  loss_contrastive: 0  loss_ce_0: 1.321  loss_mask_0: 0.5363  loss_dice_0: 1.416  loss_ce_1: 0.9162  loss_mask_1: 0.4351  loss_dice_1: 1.216  loss_ce_2: 0.6616  loss_mask_2: 0.573  loss_dice_2: 1.196  loss_ce_3: 0.5839  loss_mask_3: 0.484  loss_dice_3: 0.9529  loss_ce_4: 0.5498  loss_mask_4: 0.4748  loss_dice_4: 1.095  loss_ce_5: 0.5011  loss_mask_5: 0.4427  loss_dice_5: 1  loss_ce_6: 0.5487  loss_mask_6: 0.3866  loss_dice_6: 0.9542  loss_ce_7: 0.5016  loss_mask_7: 0.3708  loss_dice_7: 1.057  loss_ce_8: 0.5248  loss_mask_8: 0.4425  loss_dice_8: 0.9969    time: 0.3206  last_time: 0.3055  data_time: 0.0031  last_data_time: 0.0030   lr: 8.5487e-05  max_mem: 14733M
[10/03 23:02:59] d2.utils.events INFO:  eta: 0:44:14  iter: 1619  total_loss: 23.41  loss_ce: 0.4292  loss_mask: 0.3519  loss_dice: 1.063  loss_contrastive: 0  loss_ce_0: 1.52  loss_mask_0: 0.4615  loss_dice_0: 1.767  loss_ce_1: 1.028  loss_mask_1: 0.3828  loss_dice_1: 1.328  loss_ce_2: 0.7798  loss_mask_2: 0.3905  loss_dice_2: 1.159  loss_ce_3: 0.6882  loss_mask_3: 0.3459  loss_dice_3: 1.164  loss_ce_4: 0.4162  loss_mask_4: 0.3716  loss_dice_4: 1.163  loss_ce_5: 0.4767  loss_mask_5: 0.3815  loss_dice_5: 1.116  loss_ce_6: 0.5228  loss_mask_6: 0.3778  loss_dice_6: 1.033  loss_ce_7: 0.4322  loss_mask_7: 0.3655  loss_dice_7: 1.08  loss_ce_8: 0.5504  loss_mask_8: 0.301  loss_dice_8: 1.142    time: 0.3205  last_time: 0.3113  data_time: 0.0034  last_data_time: 0.0038   lr: 8.5303e-05  max_mem: 14733M
[10/03 23:03:06] d2.utils.events INFO:  eta: 0:44:12  iter: 1639  total_loss: 24.71  loss_ce: 0.5827  loss_mask: 0.2685  loss_dice: 1.158  loss_contrastive: 0  loss_ce_0: 1.804  loss_mask_0: 0.3706  loss_dice_0: 1.64  loss_ce_1: 1.086  loss_mask_1: 0.3333  loss_dice_1: 1.275  loss_ce_2: 0.8608  loss_mask_2: 0.3004  loss_dice_2: 1.252  loss_ce_3: 0.6626  loss_mask_3: 0.2965  loss_dice_3: 1.248  loss_ce_4: 0.6442  loss_mask_4: 0.2843  loss_dice_4: 1.284  loss_ce_5: 0.626  loss_mask_5: 0.2599  loss_dice_5: 1.25  loss_ce_6: 0.6806  loss_mask_6: 0.3736  loss_dice_6: 1.213  loss_ce_7: 0.6474  loss_mask_7: 0.2589  loss_dice_7: 1.218  loss_ce_8: 0.5945  loss_mask_8: 0.3044  loss_dice_8: 1.299    time: 0.3206  last_time: 0.3559  data_time: 0.0031  last_data_time: 0.0032   lr: 8.512e-05  max_mem: 14733M
[10/03 23:03:12] d2.utils.events INFO:  eta: 0:44:03  iter: 1659  total_loss: 26.56  loss_ce: 0.7211  loss_mask: 0.4476  loss_dice: 1.087  loss_contrastive: 0  loss_ce_0: 1.549  loss_mask_0: 0.4866  loss_dice_0: 1.631  loss_ce_1: 1.008  loss_mask_1: 0.4791  loss_dice_1: 1.371  loss_ce_2: 0.8126  loss_mask_2: 0.4254  loss_dice_2: 1.324  loss_ce_3: 0.7238  loss_mask_3: 0.5165  loss_dice_3: 1.201  loss_ce_4: 0.7783  loss_mask_4: 0.4875  loss_dice_4: 1.243  loss_ce_5: 0.6604  loss_mask_5: 0.4851  loss_dice_5: 1.272  loss_ce_6: 0.6766  loss_mask_6: 0.4454  loss_dice_6: 1.201  loss_ce_7: 0.6659  loss_mask_7: 0.4035  loss_dice_7: 1.143  loss_ce_8: 0.5678  loss_mask_8: 0.469  loss_dice_8: 1.228    time: 0.3206  last_time: 0.2997  data_time: 0.0040  last_data_time: 0.0026   lr: 8.4937e-05  max_mem: 14733M
[10/03 23:03:19] d2.utils.events INFO:  eta: 0:43:56  iter: 1679  total_loss: 24.25  loss_ce: 0.5272  loss_mask: 0.5138  loss_dice: 1.038  loss_contrastive: 0  loss_ce_0: 1.516  loss_mask_0: 0.5286  loss_dice_0: 1.507  loss_ce_1: 0.9877  loss_mask_1: 0.5443  loss_dice_1: 1.122  loss_ce_2: 0.685  loss_mask_2: 0.5047  loss_dice_2: 1.212  loss_ce_3: 0.6614  loss_mask_3: 0.5456  loss_dice_3: 1.228  loss_ce_4: 0.6095  loss_mask_4: 0.5482  loss_dice_4: 1.108  loss_ce_5: 0.5792  loss_mask_5: 0.5092  loss_dice_5: 1.075  loss_ce_6: 0.5725  loss_mask_6: 0.5207  loss_dice_6: 1.061  loss_ce_7: 0.5336  loss_mask_7: 0.5236  loss_dice_7: 1.116  loss_ce_8: 0.5737  loss_mask_8: 0.4947  loss_dice_8: 1.064    time: 0.3206  last_time: 0.3177  data_time: 0.0030  last_data_time: 0.0027   lr: 8.4754e-05  max_mem: 14733M
[10/03 23:03:25] d2.utils.events INFO:  eta: 0:43:49  iter: 1699  total_loss: 21.85  loss_ce: 0.5491  loss_mask: 0.4258  loss_dice: 1.055  loss_contrastive: 0  loss_ce_0: 1.451  loss_mask_0: 0.5061  loss_dice_0: 1.916  loss_ce_1: 0.8634  loss_mask_1: 0.4931  loss_dice_1: 1.398  loss_ce_2: 0.6905  loss_mask_2: 0.4377  loss_dice_2: 1.174  loss_ce_3: 0.6553  loss_mask_3: 0.412  loss_dice_3: 1.283  loss_ce_4: 0.5517  loss_mask_4: 0.408  loss_dice_4: 1.197  loss_ce_5: 0.5097  loss_mask_5: 0.4192  loss_dice_5: 1.072  loss_ce_6: 0.5183  loss_mask_6: 0.4089  loss_dice_6: 1.165  loss_ce_7: 0.5027  loss_mask_7: 0.3987  loss_dice_7: 1.003  loss_ce_8: 0.5948  loss_mask_8: 0.4209  loss_dice_8: 1.193    time: 0.3206  last_time: 0.3096  data_time: 0.0032  last_data_time: 0.0032   lr: 8.457e-05  max_mem: 14733M
[10/03 23:03:32] d2.utils.events INFO:  eta: 0:43:43  iter: 1719  total_loss: 22.8  loss_ce: 0.5799  loss_mask: 0.37  loss_dice: 0.9971  loss_contrastive: 0  loss_ce_0: 1.128  loss_mask_0: 0.3429  loss_dice_0: 1.376  loss_ce_1: 0.7826  loss_mask_1: 0.3408  loss_dice_1: 1.195  loss_ce_2: 0.7483  loss_mask_2: 0.3506  loss_dice_2: 1.213  loss_ce_3: 0.6226  loss_mask_3: 0.3841  loss_dice_3: 1.152  loss_ce_4: 0.546  loss_mask_4: 0.3652  loss_dice_4: 1.181  loss_ce_5: 0.6028  loss_mask_5: 0.4107  loss_dice_5: 1.095  loss_ce_6: 0.5822  loss_mask_6: 0.3692  loss_dice_6: 1.097  loss_ce_7: 0.5372  loss_mask_7: 0.3945  loss_dice_7: 1.15  loss_ce_8: 0.5363  loss_mask_8: 0.3947  loss_dice_8: 1.125    time: 0.3206  last_time: 0.3314  data_time: 0.0033  last_data_time: 0.0027   lr: 8.4387e-05  max_mem: 14733M
[10/03 23:03:38] d2.utils.events INFO:  eta: 0:43:41  iter: 1739  total_loss: 20.5  loss_ce: 0.3492  loss_mask: 0.4117  loss_dice: 0.9273  loss_contrastive: 0  loss_ce_0: 1.248  loss_mask_0: 0.5024  loss_dice_0: 1.026  loss_ce_1: 0.7171  loss_mask_1: 0.3841  loss_dice_1: 1.027  loss_ce_2: 0.4849  loss_mask_2: 0.4037  loss_dice_2: 0.9392  loss_ce_3: 0.3958  loss_mask_3: 0.4186  loss_dice_3: 0.8379  loss_ce_4: 0.3414  loss_mask_4: 0.4084  loss_dice_4: 0.8591  loss_ce_5: 0.3655  loss_mask_5: 0.391  loss_dice_5: 0.8702  loss_ce_6: 0.3312  loss_mask_6: 0.3583  loss_dice_6: 0.9383  loss_ce_7: 0.3466  loss_mask_7: 0.4003  loss_dice_7: 0.929  loss_ce_8: 0.3699  loss_mask_8: 0.4226  loss_dice_8: 0.9412    time: 0.3206  last_time: 0.3357  data_time: 0.0031  last_data_time: 0.0031   lr: 8.4203e-05  max_mem: 14733M
[10/03 23:03:44] d2.utils.events INFO:  eta: 0:43:30  iter: 1759  total_loss: 24.58  loss_ce: 0.3976  loss_mask: 0.4277  loss_dice: 1.346  loss_contrastive: 0  loss_ce_0: 1.278  loss_mask_0: 0.4508  loss_dice_0: 1.465  loss_ce_1: 0.9129  loss_mask_1: 0.3567  loss_dice_1: 1.483  loss_ce_2: 0.7057  loss_mask_2: 0.3566  loss_dice_2: 1.376  loss_ce_3: 0.588  loss_mask_3: 0.4325  loss_dice_3: 1.338  loss_ce_4: 0.4816  loss_mask_4: 0.4273  loss_dice_4: 1.417  loss_ce_5: 0.5374  loss_mask_5: 0.3447  loss_dice_5: 1.332  loss_ce_6: 0.4051  loss_mask_6: 0.3531  loss_dice_6: 1.254  loss_ce_7: 0.3736  loss_mask_7: 0.4442  loss_dice_7: 1.285  loss_ce_8: 0.3966  loss_mask_8: 0.4015  loss_dice_8: 1.438    time: 0.3206  last_time: 0.3301  data_time: 0.0036  last_data_time: 0.0039   lr: 8.402e-05  max_mem: 14733M
[10/03 23:03:51] d2.utils.events INFO:  eta: 0:43:26  iter: 1779  total_loss: 19.91  loss_ce: 0.5907  loss_mask: 0.442  loss_dice: 1.185  loss_contrastive: 0  loss_ce_0: 1.154  loss_mask_0: 0.4316  loss_dice_0: 1.319  loss_ce_1: 0.8363  loss_mask_1: 0.4849  loss_dice_1: 1.213  loss_ce_2: 0.7056  loss_mask_2: 0.4543  loss_dice_2: 1.177  loss_ce_3: 0.5345  loss_mask_3: 0.489  loss_dice_3: 1.211  loss_ce_4: 0.5908  loss_mask_4: 0.4119  loss_dice_4: 1.089  loss_ce_5: 0.5568  loss_mask_5: 0.462  loss_dice_5: 1.09  loss_ce_6: 0.5398  loss_mask_6: 0.4262  loss_dice_6: 1.14  loss_ce_7: 0.5403  loss_mask_7: 0.4531  loss_dice_7: 1.094  loss_ce_8: 0.5344  loss_mask_8: 0.4799  loss_dice_8: 1.237    time: 0.3206  last_time: 0.2945  data_time: 0.0032  last_data_time: 0.0019   lr: 8.3836e-05  max_mem: 14733M
[10/03 23:03:57] d2.utils.events INFO:  eta: 0:43:17  iter: 1799  total_loss: 23.6  loss_ce: 0.413  loss_mask: 0.3401  loss_dice: 1.138  loss_contrastive: 0  loss_ce_0: 1.381  loss_mask_0: 0.4725  loss_dice_0: 1.425  loss_ce_1: 0.8762  loss_mask_1: 0.4165  loss_dice_1: 1.229  loss_ce_2: 0.5796  loss_mask_2: 0.3796  loss_dice_2: 1.108  loss_ce_3: 0.5018  loss_mask_3: 0.3646  loss_dice_3: 0.9896  loss_ce_4: 0.5629  loss_mask_4: 0.3957  loss_dice_4: 1.116  loss_ce_5: 0.5715  loss_mask_5: 0.3866  loss_dice_5: 1.111  loss_ce_6: 0.5134  loss_mask_6: 0.4449  loss_dice_6: 1.032  loss_ce_7: 0.4231  loss_mask_7: 0.386  loss_dice_7: 1.041  loss_ce_8: 0.4063  loss_mask_8: 0.3844  loss_dice_8: 1.069    time: 0.3206  last_time: 0.3033  data_time: 0.0031  last_data_time: 0.0028   lr: 8.3653e-05  max_mem: 14733M
[10/03 23:04:03] d2.utils.events INFO:  eta: 0:43:11  iter: 1819  total_loss: 25.77  loss_ce: 0.4079  loss_mask: 0.3666  loss_dice: 1.316  loss_contrastive: 0  loss_ce_0: 1.247  loss_mask_0: 0.3547  loss_dice_0: 1.62  loss_ce_1: 0.8041  loss_mask_1: 0.3077  loss_dice_1: 1.375  loss_ce_2: 0.6318  loss_mask_2: 0.2994  loss_dice_2: 1.279  loss_ce_3: 0.6297  loss_mask_3: 0.3253  loss_dice_3: 1.213  loss_ce_4: 0.554  loss_mask_4: 0.3203  loss_dice_4: 1.408  loss_ce_5: 0.4859  loss_mask_5: 0.3379  loss_dice_5: 1.315  loss_ce_6: 0.4458  loss_mask_6: 0.3394  loss_dice_6: 1.379  loss_ce_7: 0.4298  loss_mask_7: 0.2931  loss_dice_7: 1.346  loss_ce_8: 0.4502  loss_mask_8: 0.3067  loss_dice_8: 1.394    time: 0.3205  last_time: 0.3351  data_time: 0.0034  last_data_time: 0.0050   lr: 8.3469e-05  max_mem: 14733M
[10/03 23:04:10] d2.utils.events INFO:  eta: 0:43:02  iter: 1839  total_loss: 21.34  loss_ce: 0.5459  loss_mask: 0.3255  loss_dice: 1.035  loss_contrastive: 0  loss_ce_0: 1.309  loss_mask_0: 0.3928  loss_dice_0: 1.235  loss_ce_1: 0.9403  loss_mask_1: 0.3098  loss_dice_1: 1.177  loss_ce_2: 0.8245  loss_mask_2: 0.3575  loss_dice_2: 1.151  loss_ce_3: 0.6778  loss_mask_3: 0.3129  loss_dice_3: 1.061  loss_ce_4: 0.7082  loss_mask_4: 0.3105  loss_dice_4: 1.064  loss_ce_5: 0.6206  loss_mask_5: 0.2953  loss_dice_5: 1.009  loss_ce_6: 0.5411  loss_mask_6: 0.2998  loss_dice_6: 1.05  loss_ce_7: 0.4706  loss_mask_7: 0.2877  loss_dice_7: 1.079  loss_ce_8: 0.5079  loss_mask_8: 0.2913  loss_dice_8: 1.09    time: 0.3205  last_time: 0.3295  data_time: 0.0032  last_data_time: 0.0033   lr: 8.3285e-05  max_mem: 14733M
[10/03 23:04:16] d2.utils.events INFO:  eta: 0:42:55  iter: 1859  total_loss: 24.31  loss_ce: 0.6075  loss_mask: 0.3851  loss_dice: 1.234  loss_contrastive: 0  loss_ce_0: 1.511  loss_mask_0: 0.4827  loss_dice_0: 1.436  loss_ce_1: 0.8552  loss_mask_1: 0.3815  loss_dice_1: 1.348  loss_ce_2: 0.6711  loss_mask_2: 0.3499  loss_dice_2: 1.191  loss_ce_3: 0.6282  loss_mask_3: 0.3641  loss_dice_3: 1.139  loss_ce_4: 0.5716  loss_mask_4: 0.3252  loss_dice_4: 1.213  loss_ce_5: 0.5042  loss_mask_5: 0.3352  loss_dice_5: 1.215  loss_ce_6: 0.4644  loss_mask_6: 0.3238  loss_dice_6: 1.134  loss_ce_7: 0.5933  loss_mask_7: 0.3508  loss_dice_7: 1.203  loss_ce_8: 0.5744  loss_mask_8: 0.3475  loss_dice_8: 1.238    time: 0.3205  last_time: 0.3371  data_time: 0.0034  last_data_time: 0.0032   lr: 8.3102e-05  max_mem: 14733M
[10/03 23:04:23] d2.utils.events INFO:  eta: 0:42:49  iter: 1879  total_loss: 19.7  loss_ce: 0.285  loss_mask: 0.4155  loss_dice: 0.8229  loss_contrastive: 0  loss_ce_0: 1.074  loss_mask_0: 0.366  loss_dice_0: 1.031  loss_ce_1: 0.6252  loss_mask_1: 0.2986  loss_dice_1: 1.076  loss_ce_2: 0.5417  loss_mask_2: 0.3552  loss_dice_2: 0.898  loss_ce_3: 0.5079  loss_mask_3: 0.3824  loss_dice_3: 0.7806  loss_ce_4: 0.3508  loss_mask_4: 0.3664  loss_dice_4: 0.874  loss_ce_5: 0.2933  loss_mask_5: 0.3247  loss_dice_5: 0.8422  loss_ce_6: 0.3022  loss_mask_6: 0.3579  loss_dice_6: 0.9049  loss_ce_7: 0.2588  loss_mask_7: 0.3781  loss_dice_7: 0.8288  loss_ce_8: 0.3367  loss_mask_8: 0.3257  loss_dice_8: 0.8158    time: 0.3204  last_time: 0.3135  data_time: 0.0033  last_data_time: 0.0029   lr: 8.2918e-05  max_mem: 14733M
[10/03 23:04:29] d2.utils.events INFO:  eta: 0:42:42  iter: 1899  total_loss: 22.35  loss_ce: 0.51  loss_mask: 0.4494  loss_dice: 0.9722  loss_contrastive: 0  loss_ce_0: 1.088  loss_mask_0: 0.5326  loss_dice_0: 1.249  loss_ce_1: 0.7806  loss_mask_1: 0.4694  loss_dice_1: 1.081  loss_ce_2: 0.6238  loss_mask_2: 0.4593  loss_dice_2: 1.093  loss_ce_3: 0.6542  loss_mask_3: 0.4425  loss_dice_3: 0.9679  loss_ce_4: 0.7002  loss_mask_4: 0.4521  loss_dice_4: 0.9459  loss_ce_5: 0.6073  loss_mask_5: 0.4338  loss_dice_5: 0.9814  loss_ce_6: 0.6354  loss_mask_6: 0.4631  loss_dice_6: 1.015  loss_ce_7: 0.5463  loss_mask_7: 0.4746  loss_dice_7: 0.9677  loss_ce_8: 0.6326  loss_mask_8: 0.4716  loss_dice_8: 0.971    time: 0.3205  last_time: 0.3268  data_time: 0.0034  last_data_time: 0.0052   lr: 8.2734e-05  max_mem: 14733M
[10/03 23:04:35] d2.utils.events INFO:  eta: 0:42:36  iter: 1919  total_loss: 20.03  loss_ce: 0.2377  loss_mask: 0.4377  loss_dice: 0.9522  loss_contrastive: 0  loss_ce_0: 1.276  loss_mask_0: 0.4974  loss_dice_0: 1.274  loss_ce_1: 0.8092  loss_mask_1: 0.4588  loss_dice_1: 1.047  loss_ce_2: 0.4749  loss_mask_2: 0.412  loss_dice_2: 0.8517  loss_ce_3: 0.3415  loss_mask_3: 0.4046  loss_dice_3: 0.9057  loss_ce_4: 0.2671  loss_mask_4: 0.4074  loss_dice_4: 0.7915  loss_ce_5: 0.2556  loss_mask_5: 0.4132  loss_dice_5: 0.8694  loss_ce_6: 0.2956  loss_mask_6: 0.4121  loss_dice_6: 0.8963  loss_ce_7: 0.2869  loss_mask_7: 0.4378  loss_dice_7: 0.8694  loss_ce_8: 0.3012  loss_mask_8: 0.4195  loss_dice_8: 0.8078    time: 0.3204  last_time: 0.3078  data_time: 0.0029  last_data_time: 0.0040   lr: 8.255e-05  max_mem: 14733M
[10/03 23:04:42] d2.utils.events INFO:  eta: 0:42:31  iter: 1939  total_loss: 24.67  loss_ce: 0.6023  loss_mask: 0.3172  loss_dice: 1.051  loss_contrastive: 0  loss_ce_0: 1.386  loss_mask_0: 0.3608  loss_dice_0: 1.475  loss_ce_1: 1.012  loss_mask_1: 0.3041  loss_dice_1: 1.295  loss_ce_2: 0.9347  loss_mask_2: 0.3129  loss_dice_2: 1.179  loss_ce_3: 0.7809  loss_mask_3: 0.3182  loss_dice_3: 1.043  loss_ce_4: 0.7773  loss_mask_4: 0.2726  loss_dice_4: 1.074  loss_ce_5: 0.8301  loss_mask_5: 0.2773  loss_dice_5: 1.072  loss_ce_6: 0.6738  loss_mask_6: 0.2998  loss_dice_6: 1.008  loss_ce_7: 0.5706  loss_mask_7: 0.3404  loss_dice_7: 1.079  loss_ce_8: 0.6197  loss_mask_8: 0.3692  loss_dice_8: 1.153    time: 0.3204  last_time: 0.3133  data_time: 0.0034  last_data_time: 0.0031   lr: 8.2366e-05  max_mem: 14733M
[10/03 23:04:48] d2.utils.events INFO:  eta: 0:42:23  iter: 1959  total_loss: 27.11  loss_ce: 0.5956  loss_mask: 0.3257  loss_dice: 1.395  loss_contrastive: 0  loss_ce_0: 1.519  loss_mask_0: 0.4034  loss_dice_0: 1.84  loss_ce_1: 0.8933  loss_mask_1: 0.3413  loss_dice_1: 1.621  loss_ce_2: 0.7563  loss_mask_2: 0.3952  loss_dice_2: 1.432  loss_ce_3: 0.6893  loss_mask_3: 0.3561  loss_dice_3: 1.353  loss_ce_4: 0.6141  loss_mask_4: 0.3326  loss_dice_4: 1.398  loss_ce_5: 0.6657  loss_mask_5: 0.359  loss_dice_5: 1.349  loss_ce_6: 0.5042  loss_mask_6: 0.3406  loss_dice_6: 1.321  loss_ce_7: 0.5062  loss_mask_7: 0.3466  loss_dice_7: 1.271  loss_ce_8: 0.5929  loss_mask_8: 0.3404  loss_dice_8: 1.285    time: 0.3205  last_time: 0.3126  data_time: 0.0034  last_data_time: 0.0044   lr: 8.2182e-05  max_mem: 14733M
[10/03 23:04:55] d2.utils.events INFO:  eta: 0:42:15  iter: 1979  total_loss: 19.9  loss_ce: 0.4316  loss_mask: 0.3388  loss_dice: 0.9345  loss_contrastive: 0  loss_ce_0: 1.452  loss_mask_0: 0.3788  loss_dice_0: 1.199  loss_ce_1: 0.9447  loss_mask_1: 0.3705  loss_dice_1: 1.112  loss_ce_2: 0.7529  loss_mask_2: 0.3377  loss_dice_2: 1.048  loss_ce_3: 0.5697  loss_mask_3: 0.3677  loss_dice_3: 0.9144  loss_ce_4: 0.5541  loss_mask_4: 0.346  loss_dice_4: 0.8036  loss_ce_5: 0.5635  loss_mask_5: 0.3641  loss_dice_5: 0.9501  loss_ce_6: 0.5642  loss_mask_6: 0.3231  loss_dice_6: 0.8408  loss_ce_7: 0.5086  loss_mask_7: 0.35  loss_dice_7: 0.9154  loss_ce_8: 0.4645  loss_mask_8: 0.3562  loss_dice_8: 1.048    time: 0.3205  last_time: 0.3280  data_time: 0.0035  last_data_time: 0.0048   lr: 8.1998e-05  max_mem: 14733M
[10/03 23:05:01] d2.utils.events INFO:  eta: 0:42:08  iter: 1999  total_loss: 22.34  loss_ce: 0.3288  loss_mask: 0.4373  loss_dice: 1.122  loss_contrastive: 0  loss_ce_0: 1.292  loss_mask_0: 0.4574  loss_dice_0: 1.105  loss_ce_1: 0.8216  loss_mask_1: 0.425  loss_dice_1: 1.028  loss_ce_2: 0.6335  loss_mask_2: 0.4192  loss_dice_2: 0.949  loss_ce_3: 0.4901  loss_mask_3: 0.4857  loss_dice_3: 1.04  loss_ce_4: 0.4443  loss_mask_4: 0.4853  loss_dice_4: 0.9841  loss_ce_5: 0.4907  loss_mask_5: 0.451  loss_dice_5: 1.011  loss_ce_6: 0.3968  loss_mask_6: 0.4515  loss_dice_6: 0.9359  loss_ce_7: 0.3295  loss_mask_7: 0.4437  loss_dice_7: 0.9448  loss_ce_8: 0.3288  loss_mask_8: 0.4491  loss_dice_8: 0.9885    time: 0.3204  last_time: 0.3056  data_time: 0.0032  last_data_time: 0.0030   lr: 8.1814e-05  max_mem: 14733M
[10/03 23:05:08] d2.utils.events INFO:  eta: 0:42:04  iter: 2019  total_loss: 24.06  loss_ce: 0.5276  loss_mask: 0.4192  loss_dice: 1.213  loss_contrastive: 0  loss_ce_0: 1.307  loss_mask_0: 0.4734  loss_dice_0: 1.62  loss_ce_1: 0.963  loss_mask_1: 0.4511  loss_dice_1: 1.55  loss_ce_2: 0.6788  loss_mask_2: 0.3987  loss_dice_2: 1.409  loss_ce_3: 0.5725  loss_mask_3: 0.4176  loss_dice_3: 1.233  loss_ce_4: 0.5384  loss_mask_4: 0.4318  loss_dice_4: 1.292  loss_ce_5: 0.4944  loss_mask_5: 0.4503  loss_dice_5: 1.327  loss_ce_6: 0.553  loss_mask_6: 0.459  loss_dice_6: 1.225  loss_ce_7: 0.49  loss_mask_7: 0.4497  loss_dice_7: 1.339  loss_ce_8: 0.6211  loss_mask_8: 0.4566  loss_dice_8: 1.294    time: 0.3205  last_time: 0.3156  data_time: 0.0039  last_data_time: 0.0026   lr: 8.163e-05  max_mem: 14733M
[10/03 23:05:14] d2.utils.events INFO:  eta: 0:41:57  iter: 2039  total_loss: 22.98  loss_ce: 0.4383  loss_mask: 0.4535  loss_dice: 1.189  loss_contrastive: 0  loss_ce_0: 1.238  loss_mask_0: 0.4738  loss_dice_0: 1.175  loss_ce_1: 0.8569  loss_mask_1: 0.4209  loss_dice_1: 1.266  loss_ce_2: 0.5517  loss_mask_2: 0.4395  loss_dice_2: 1.098  loss_ce_3: 0.6013  loss_mask_3: 0.4075  loss_dice_3: 1.244  loss_ce_4: 0.4779  loss_mask_4: 0.4293  loss_dice_4: 1.14  loss_ce_5: 0.4025  loss_mask_5: 0.438  loss_dice_5: 1.234  loss_ce_6: 0.4751  loss_mask_6: 0.4176  loss_dice_6: 1.208  loss_ce_7: 0.4144  loss_mask_7: 0.4376  loss_dice_7: 1.201  loss_ce_8: 0.4737  loss_mask_8: 0.4208  loss_dice_8: 1.191    time: 0.3205  last_time: 0.3511  data_time: 0.0039  last_data_time: 0.0072   lr: 8.1446e-05  max_mem: 14733M
[10/03 23:05:21] d2.utils.events INFO:  eta: 0:41:52  iter: 2059  total_loss: 22.69  loss_ce: 0.467  loss_mask: 0.2204  loss_dice: 1.354  loss_contrastive: 0  loss_ce_0: 1.032  loss_mask_0: 0.2529  loss_dice_0: 1.635  loss_ce_1: 0.8484  loss_mask_1: 0.24  loss_dice_1: 1.249  loss_ce_2: 0.9022  loss_mask_2: 0.2506  loss_dice_2: 1.312  loss_ce_3: 0.5431  loss_mask_3: 0.2542  loss_dice_3: 1.308  loss_ce_4: 0.4919  loss_mask_4: 0.2204  loss_dice_4: 1.254  loss_ce_5: 0.4795  loss_mask_5: 0.2138  loss_dice_5: 1.346  loss_ce_6: 0.5733  loss_mask_6: 0.2169  loss_dice_6: 1.248  loss_ce_7: 0.5754  loss_mask_7: 0.223  loss_dice_7: 1.258  loss_ce_8: 0.5509  loss_mask_8: 0.2156  loss_dice_8: 1.231    time: 0.3205  last_time: 0.3072  data_time: 0.0034  last_data_time: 0.0030   lr: 8.1262e-05  max_mem: 14733M
[10/03 23:05:27] d2.utils.events INFO:  eta: 0:41:46  iter: 2079  total_loss: 22.75  loss_ce: 0.5635  loss_mask: 0.4476  loss_dice: 1.056  loss_contrastive: 0  loss_ce_0: 1.381  loss_mask_0: 0.5098  loss_dice_0: 1.342  loss_ce_1: 0.9649  loss_mask_1: 0.4237  loss_dice_1: 1.177  loss_ce_2: 0.7744  loss_mask_2: 0.4252  loss_dice_2: 1.14  loss_ce_3: 0.5887  loss_mask_3: 0.3916  loss_dice_3: 1.125  loss_ce_4: 0.5803  loss_mask_4: 0.4108  loss_dice_4: 1.103  loss_ce_5: 0.5872  loss_mask_5: 0.4306  loss_dice_5: 1.17  loss_ce_6: 0.443  loss_mask_6: 0.4435  loss_dice_6: 1.074  loss_ce_7: 0.4667  loss_mask_7: 0.4505  loss_dice_7: 1.098  loss_ce_8: 0.4779  loss_mask_8: 0.4396  loss_dice_8: 1.107    time: 0.3206  last_time: 0.3152  data_time: 0.0036  last_data_time: 0.0046   lr: 8.1078e-05  max_mem: 14733M
[10/03 23:05:34] d2.utils.events INFO:  eta: 0:41:42  iter: 2099  total_loss: 21.5  loss_ce: 0.3989  loss_mask: 0.1984  loss_dice: 0.9847  loss_contrastive: 0  loss_ce_0: 1.501  loss_mask_0: 0.293  loss_dice_0: 1.369  loss_ce_1: 0.8984  loss_mask_1: 0.2372  loss_dice_1: 1.131  loss_ce_2: 0.8518  loss_mask_2: 0.2077  loss_dice_2: 1.031  loss_ce_3: 0.6048  loss_mask_3: 0.193  loss_dice_3: 1.17  loss_ce_4: 0.6548  loss_mask_4: 0.2162  loss_dice_4: 1.078  loss_ce_5: 0.6155  loss_mask_5: 0.1934  loss_dice_5: 0.9533  loss_ce_6: 0.5242  loss_mask_6: 0.1933  loss_dice_6: 0.8806  loss_ce_7: 0.5262  loss_mask_7: 0.1873  loss_dice_7: 1.048  loss_ce_8: 0.4587  loss_mask_8: 0.1881  loss_dice_8: 1.02    time: 0.3206  last_time: 0.3251  data_time: 0.0032  last_data_time: 0.0032   lr: 8.0894e-05  max_mem: 14733M
[10/03 23:05:40] d2.utils.events INFO:  eta: 0:41:36  iter: 2119  total_loss: 19.42  loss_ce: 0.3784  loss_mask: 0.2149  loss_dice: 0.8817  loss_contrastive: 0  loss_ce_0: 1.262  loss_mask_0: 0.3381  loss_dice_0: 1.43  loss_ce_1: 0.8068  loss_mask_1: 0.2296  loss_dice_1: 0.9329  loss_ce_2: 0.6595  loss_mask_2: 0.2218  loss_dice_2: 0.8826  loss_ce_3: 0.5236  loss_mask_3: 0.2133  loss_dice_3: 0.8286  loss_ce_4: 0.5262  loss_mask_4: 0.2223  loss_dice_4: 0.8887  loss_ce_5: 0.4371  loss_mask_5: 0.2195  loss_dice_5: 0.8718  loss_ce_6: 0.3955  loss_mask_6: 0.2197  loss_dice_6: 0.7637  loss_ce_7: 0.4056  loss_mask_7: 0.224  loss_dice_7: 0.8871  loss_ce_8: 0.4928  loss_mask_8: 0.2298  loss_dice_8: 0.9103    time: 0.3206  last_time: 0.3259  data_time: 0.0036  last_data_time: 0.0038   lr: 8.0709e-05  max_mem: 14733M
[10/03 23:05:46] d2.utils.events INFO:  eta: 0:41:28  iter: 2139  total_loss: 24.09  loss_ce: 0.6045  loss_mask: 0.4795  loss_dice: 1.204  loss_contrastive: 0  loss_ce_0: 1.425  loss_mask_0: 0.5065  loss_dice_0: 1.287  loss_ce_1: 1.068  loss_mask_1: 0.4971  loss_dice_1: 1.339  loss_ce_2: 0.8855  loss_mask_2: 0.4392  loss_dice_2: 1.129  loss_ce_3: 0.711  loss_mask_3: 0.3865  loss_dice_3: 1.144  loss_ce_4: 0.6506  loss_mask_4: 0.391  loss_dice_4: 1.113  loss_ce_5: 0.7049  loss_mask_5: 0.4039  loss_dice_5: 1.091  loss_ce_6: 0.6633  loss_mask_6: 0.4098  loss_dice_6: 1.069  loss_ce_7: 0.6333  loss_mask_7: 0.4486  loss_dice_7: 1.144  loss_ce_8: 0.6419  loss_mask_8: 0.4318  loss_dice_8: 1.076    time: 0.3206  last_time: 0.3198  data_time: 0.0031  last_data_time: 0.0028   lr: 8.0525e-05  max_mem: 14733M
[10/03 23:05:53] d2.utils.events INFO:  eta: 0:41:22  iter: 2159  total_loss: 21.11  loss_ce: 0.386  loss_mask: 0.3381  loss_dice: 1.246  loss_contrastive: 0  loss_ce_0: 1.328  loss_mask_0: 0.3804  loss_dice_0: 1.348  loss_ce_1: 0.8004  loss_mask_1: 0.3531  loss_dice_1: 1.262  loss_ce_2: 0.595  loss_mask_2: 0.3389  loss_dice_2: 1.13  loss_ce_3: 0.4927  loss_mask_3: 0.3627  loss_dice_3: 1.136  loss_ce_4: 0.4333  loss_mask_4: 0.354  loss_dice_4: 1.139  loss_ce_5: 0.4019  loss_mask_5: 0.359  loss_dice_5: 1.146  loss_ce_6: 0.3116  loss_mask_6: 0.3515  loss_dice_6: 1.127  loss_ce_7: 0.3867  loss_mask_7: 0.3361  loss_dice_7: 1.192  loss_ce_8: 0.3809  loss_mask_8: 0.3633  loss_dice_8: 1.232    time: 0.3205  last_time: 0.3060  data_time: 0.0033  last_data_time: 0.0035   lr: 8.034e-05  max_mem: 14733M
[10/03 23:05:59] d2.utils.events INFO:  eta: 0:41:15  iter: 2179  total_loss: 22.72  loss_ce: 0.5276  loss_mask: 0.3764  loss_dice: 1.1  loss_contrastive: 0  loss_ce_0: 1.135  loss_mask_0: 0.4621  loss_dice_0: 1.258  loss_ce_1: 0.9911  loss_mask_1: 0.4173  loss_dice_1: 0.9728  loss_ce_2: 0.806  loss_mask_2: 0.3481  loss_dice_2: 0.8869  loss_ce_3: 0.6669  loss_mask_3: 0.362  loss_dice_3: 0.8117  loss_ce_4: 0.7644  loss_mask_4: 0.3798  loss_dice_4: 0.8555  loss_ce_5: 0.6896  loss_mask_5: 0.3831  loss_dice_5: 0.8349  loss_ce_6: 0.7049  loss_mask_6: 0.3433  loss_dice_6: 0.9244  loss_ce_7: 0.6238  loss_mask_7: 0.3708  loss_dice_7: 1.037  loss_ce_8: 0.561  loss_mask_8: 0.3666  loss_dice_8: 0.9719    time: 0.3205  last_time: 0.3031  data_time: 0.0034  last_data_time: 0.0026   lr: 8.0156e-05  max_mem: 14733M
[10/03 23:06:06] d2.utils.events INFO:  eta: 0:41:09  iter: 2199  total_loss: 27.77  loss_ce: 0.7269  loss_mask: 0.499  loss_dice: 1.204  loss_contrastive: 0  loss_ce_0: 1.265  loss_mask_0: 0.4542  loss_dice_0: 1.459  loss_ce_1: 0.8452  loss_mask_1: 0.5088  loss_dice_1: 1.261  loss_ce_2: 0.7421  loss_mask_2: 0.4829  loss_dice_2: 1.303  loss_ce_3: 0.6149  loss_mask_3: 0.4338  loss_dice_3: 1.098  loss_ce_4: 0.5388  loss_mask_4: 0.4285  loss_dice_4: 1.184  loss_ce_5: 0.4872  loss_mask_5: 0.4374  loss_dice_5: 1.25  loss_ce_6: 0.6133  loss_mask_6: 0.4999  loss_dice_6: 1.2  loss_ce_7: 0.6399  loss_mask_7: 0.4408  loss_dice_7: 1.181  loss_ce_8: 0.6528  loss_mask_8: 0.5162  loss_dice_8: 1.168    time: 0.3205  last_time: 0.3248  data_time: 0.0036  last_data_time: 0.0030   lr: 7.9972e-05  max_mem: 14733M
[10/03 23:06:12] d2.utils.events INFO:  eta: 0:41:03  iter: 2219  total_loss: 21.75  loss_ce: 0.5979  loss_mask: 0.4172  loss_dice: 1.136  loss_contrastive: 0  loss_ce_0: 1.114  loss_mask_0: 0.5279  loss_dice_0: 1.202  loss_ce_1: 0.9208  loss_mask_1: 0.466  loss_dice_1: 1.101  loss_ce_2: 0.7828  loss_mask_2: 0.5587  loss_dice_2: 1.057  loss_ce_3: 0.6386  loss_mask_3: 0.4855  loss_dice_3: 1.035  loss_ce_4: 0.6745  loss_mask_4: 0.4413  loss_dice_4: 0.9879  loss_ce_5: 0.7796  loss_mask_5: 0.3852  loss_dice_5: 1.031  loss_ce_6: 0.7207  loss_mask_6: 0.3975  loss_dice_6: 1.084  loss_ce_7: 0.6125  loss_mask_7: 0.403  loss_dice_7: 1.069  loss_ce_8: 0.627  loss_mask_8: 0.415  loss_dice_8: 1.104    time: 0.3205  last_time: 0.3113  data_time: 0.0031  last_data_time: 0.0039   lr: 7.9787e-05  max_mem: 14733M
[10/03 23:06:19] d2.utils.events INFO:  eta: 0:40:57  iter: 2239  total_loss: 24.29  loss_ce: 0.429  loss_mask: 0.5103  loss_dice: 1.258  loss_contrastive: 0  loss_ce_0: 1.172  loss_mask_0: 0.55  loss_dice_0: 1.218  loss_ce_1: 0.8016  loss_mask_1: 0.5162  loss_dice_1: 1.235  loss_ce_2: 0.836  loss_mask_2: 0.6621  loss_dice_2: 1.221  loss_ce_3: 0.7597  loss_mask_3: 0.498  loss_dice_3: 1.107  loss_ce_4: 0.8112  loss_mask_4: 0.4512  loss_dice_4: 1.147  loss_ce_5: 0.7027  loss_mask_5: 0.4386  loss_dice_5: 1.105  loss_ce_6: 0.6172  loss_mask_6: 0.4828  loss_dice_6: 1.146  loss_ce_7: 0.578  loss_mask_7: 0.5567  loss_dice_7: 1.154  loss_ce_8: 0.4527  loss_mask_8: 0.5371  loss_dice_8: 1.205    time: 0.3205  last_time: 0.3220  data_time: 0.0031  last_data_time: 0.0027   lr: 7.9602e-05  max_mem: 14733M
[10/03 23:06:25] d2.utils.events INFO:  eta: 0:40:50  iter: 2259  total_loss: 25.64  loss_ce: 0.6422  loss_mask: 0.2603  loss_dice: 1.223  loss_contrastive: 0  loss_ce_0: 1.487  loss_mask_0: 0.4234  loss_dice_0: 1.754  loss_ce_1: 1.151  loss_mask_1: 0.2647  loss_dice_1: 1.383  loss_ce_2: 1.155  loss_mask_2: 0.3627  loss_dice_2: 1.275  loss_ce_3: 0.8225  loss_mask_3: 0.3162  loss_dice_3: 1.522  loss_ce_4: 0.7099  loss_mask_4: 0.3044  loss_dice_4: 1.522  loss_ce_5: 0.7477  loss_mask_5: 0.3163  loss_dice_5: 1.305  loss_ce_6: 0.7665  loss_mask_6: 0.3073  loss_dice_6: 1.235  loss_ce_7: 0.6512  loss_mask_7: 0.275  loss_dice_7: 1.194  loss_ce_8: 0.5884  loss_mask_8: 0.306  loss_dice_8: 1.215    time: 0.3205  last_time: 0.3323  data_time: 0.0036  last_data_time: 0.0028   lr: 7.9418e-05  max_mem: 14733M
[10/03 23:06:31] d2.utils.events INFO:  eta: 0:40:45  iter: 2279  total_loss: 24.83  loss_ce: 0.4495  loss_mask: 0.3845  loss_dice: 1.21  loss_contrastive: 0  loss_ce_0: 1.512  loss_mask_0: 0.4356  loss_dice_0: 1.687  loss_ce_1: 1.026  loss_mask_1: 0.388  loss_dice_1: 1.454  loss_ce_2: 0.7222  loss_mask_2: 0.3815  loss_dice_2: 1.29  loss_ce_3: 0.6763  loss_mask_3: 0.4162  loss_dice_3: 1.284  loss_ce_4: 0.5717  loss_mask_4: 0.363  loss_dice_4: 1.235  loss_ce_5: 0.6044  loss_mask_5: 0.3895  loss_dice_5: 1.213  loss_ce_6: 0.5477  loss_mask_6: 0.385  loss_dice_6: 1.164  loss_ce_7: 0.5478  loss_mask_7: 0.3707  loss_dice_7: 1.162  loss_ce_8: 0.4564  loss_mask_8: 0.3726  loss_dice_8: 1.205    time: 0.3205  last_time: 0.3062  data_time: 0.0041  last_data_time: 0.0030   lr: 7.9233e-05  max_mem: 14733M
[10/03 23:06:38] d2.utils.events INFO:  eta: 0:40:39  iter: 2299  total_loss: 27.21  loss_ce: 0.643  loss_mask: 0.455  loss_dice: 1.262  loss_contrastive: 0  loss_ce_0: 1.611  loss_mask_0: 0.4917  loss_dice_0: 1.839  loss_ce_1: 1  loss_mask_1: 0.4573  loss_dice_1: 1.481  loss_ce_2: 1.044  loss_mask_2: 0.4506  loss_dice_2: 1.426  loss_ce_3: 0.9035  loss_mask_3: 0.4587  loss_dice_3: 1.358  loss_ce_4: 0.7431  loss_mask_4: 0.4317  loss_dice_4: 1.274  loss_ce_5: 0.6951  loss_mask_5: 0.4562  loss_dice_5: 1.298  loss_ce_6: 0.6323  loss_mask_6: 0.4275  loss_dice_6: 1.296  loss_ce_7: 0.5867  loss_mask_7: 0.4539  loss_dice_7: 1.267  loss_ce_8: 0.5492  loss_mask_8: 0.4809  loss_dice_8: 1.41    time: 0.3205  last_time: 0.3070  data_time: 0.0034  last_data_time: 0.0025   lr: 7.9048e-05  max_mem: 14733M
[10/03 23:06:44] d2.utils.events INFO:  eta: 0:40:32  iter: 2319  total_loss: 23.28  loss_ce: 0.3311  loss_mask: 0.4075  loss_dice: 1.061  loss_contrastive: 0  loss_ce_0: 1.26  loss_mask_0: 0.4556  loss_dice_0: 1.277  loss_ce_1: 0.7915  loss_mask_1: 0.4396  loss_dice_1: 1.115  loss_ce_2: 0.6271  loss_mask_2: 0.4389  loss_dice_2: 1.023  loss_ce_3: 0.494  loss_mask_3: 0.3979  loss_dice_3: 1.134  loss_ce_4: 0.429  loss_mask_4: 0.4094  loss_dice_4: 1.076  loss_ce_5: 0.5221  loss_mask_5: 0.3899  loss_dice_5: 1.019  loss_ce_6: 0.4047  loss_mask_6: 0.3909  loss_dice_6: 1.024  loss_ce_7: 0.3641  loss_mask_7: 0.3702  loss_dice_7: 1.022  loss_ce_8: 0.3501  loss_mask_8: 0.3708  loss_dice_8: 1.051    time: 0.3205  last_time: 0.3119  data_time: 0.0032  last_data_time: 0.0035   lr: 7.8863e-05  max_mem: 14733M
[10/03 23:06:51] d2.utils.events INFO:  eta: 0:40:25  iter: 2339  total_loss: 22.93  loss_ce: 0.28  loss_mask: 0.4904  loss_dice: 1.188  loss_contrastive: 0  loss_ce_0: 1.092  loss_mask_0: 0.4004  loss_dice_0: 1.402  loss_ce_1: 0.789  loss_mask_1: 0.4499  loss_dice_1: 1.265  loss_ce_2: 0.5144  loss_mask_2: 0.3878  loss_dice_2: 1.184  loss_ce_3: 0.4732  loss_mask_3: 0.4054  loss_dice_3: 1.267  loss_ce_4: 0.3911  loss_mask_4: 0.4424  loss_dice_4: 1.236  loss_ce_5: 0.3866  loss_mask_5: 0.4415  loss_dice_5: 1.198  loss_ce_6: 0.3364  loss_mask_6: 0.4409  loss_dice_6: 1.215  loss_ce_7: 0.2956  loss_mask_7: 0.4154  loss_dice_7: 1.215  loss_ce_8: 0.268  loss_mask_8: 0.4339  loss_dice_8: 1.232    time: 0.3205  last_time: 0.3587  data_time: 0.0037  last_data_time: 0.0066   lr: 7.8679e-05  max_mem: 14733M
[10/03 23:06:57] d2.utils.events INFO:  eta: 0:40:18  iter: 2359  total_loss: 23.35  loss_ce: 0.5646  loss_mask: 0.2696  loss_dice: 1.034  loss_contrastive: 0  loss_ce_0: 1.474  loss_mask_0: 0.3592  loss_dice_0: 1.311  loss_ce_1: 0.8495  loss_mask_1: 0.3066  loss_dice_1: 1.152  loss_ce_2: 0.8074  loss_mask_2: 0.3649  loss_dice_2: 1.028  loss_ce_3: 0.7408  loss_mask_3: 0.2824  loss_dice_3: 1.039  loss_ce_4: 0.6728  loss_mask_4: 0.3161  loss_dice_4: 0.9879  loss_ce_5: 0.6078  loss_mask_5: 0.2689  loss_dice_5: 1.013  loss_ce_6: 0.5908  loss_mask_6: 0.2893  loss_dice_6: 0.9733  loss_ce_7: 0.5555  loss_mask_7: 0.313  loss_dice_7: 1.004  loss_ce_8: 0.549  loss_mask_8: 0.2935  loss_dice_8: 0.9918    time: 0.3206  last_time: 0.3115  data_time: 0.0036  last_data_time: 0.0026   lr: 7.8494e-05  max_mem: 14733M
[10/03 23:07:04] d2.utils.events INFO:  eta: 0:40:14  iter: 2379  total_loss: 21.64  loss_ce: 0.4925  loss_mask: 0.1917  loss_dice: 1.083  loss_contrastive: 0  loss_ce_0: 1.079  loss_mask_0: 0.2816  loss_dice_0: 1.317  loss_ce_1: 0.7986  loss_mask_1: 0.2027  loss_dice_1: 1.305  loss_ce_2: 0.6833  loss_mask_2: 0.2058  loss_dice_2: 1.311  loss_ce_3: 0.5716  loss_mask_3: 0.2182  loss_dice_3: 1.265  loss_ce_4: 0.4845  loss_mask_4: 0.1956  loss_dice_4: 1.29  loss_ce_5: 0.5029  loss_mask_5: 0.1851  loss_dice_5: 1.248  loss_ce_6: 0.472  loss_mask_6: 0.1796  loss_dice_6: 1.103  loss_ce_7: 0.4415  loss_mask_7: 0.1844  loss_dice_7: 1.178  loss_ce_8: 0.4149  loss_mask_8: 0.1949  loss_dice_8: 1.072    time: 0.3206  last_time: 0.3304  data_time: 0.0036  last_data_time: 0.0028   lr: 7.8309e-05  max_mem: 14733M
[10/03 23:07:10] d2.utils.events INFO:  eta: 0:40:07  iter: 2399  total_loss: 18.41  loss_ce: 0.5075  loss_mask: 0.3417  loss_dice: 0.7784  loss_contrastive: 0  loss_ce_0: 1.118  loss_mask_0: 0.395  loss_dice_0: 1.05  loss_ce_1: 0.8768  loss_mask_1: 0.3769  loss_dice_1: 0.8668  loss_ce_2: 0.8306  loss_mask_2: 0.3293  loss_dice_2: 0.881  loss_ce_3: 0.5956  loss_mask_3: 0.3715  loss_dice_3: 0.8066  loss_ce_4: 0.6693  loss_mask_4: 0.3524  loss_dice_4: 0.8024  loss_ce_5: 0.5081  loss_mask_5: 0.2889  loss_dice_5: 0.7995  loss_ce_6: 0.5964  loss_mask_6: 0.3447  loss_dice_6: 0.8173  loss_ce_7: 0.5429  loss_mask_7: 0.3157  loss_dice_7: 0.8034  loss_ce_8: 0.5013  loss_mask_8: 0.3209  loss_dice_8: 0.7854    time: 0.3206  last_time: 0.3356  data_time: 0.0034  last_data_time: 0.0031   lr: 7.8124e-05  max_mem: 14733M
[10/03 23:07:17] d2.utils.events INFO:  eta: 0:40:04  iter: 2419  total_loss: 24.35  loss_ce: 0.4898  loss_mask: 0.3978  loss_dice: 1.026  loss_contrastive: 0  loss_ce_0: 1.312  loss_mask_0: 0.4853  loss_dice_0: 1.454  loss_ce_1: 0.7529  loss_mask_1: 0.4833  loss_dice_1: 1.152  loss_ce_2: 0.7316  loss_mask_2: 0.4924  loss_dice_2: 1.136  loss_ce_3: 0.7254  loss_mask_3: 0.4231  loss_dice_3: 1.123  loss_ce_4: 0.6902  loss_mask_4: 0.4953  loss_dice_4: 1.178  loss_ce_5: 0.6282  loss_mask_5: 0.4596  loss_dice_5: 1.045  loss_ce_6: 0.5352  loss_mask_6: 0.4349  loss_dice_6: 1.001  loss_ce_7: 0.5335  loss_mask_7: 0.42  loss_dice_7: 1.029  loss_ce_8: 0.5243  loss_mask_8: 0.4093  loss_dice_8: 0.9762    time: 0.3206  last_time: 0.3037  data_time: 0.0033  last_data_time: 0.0031   lr: 7.7939e-05  max_mem: 14733M
[10/03 23:07:23] d2.utils.events INFO:  eta: 0:39:56  iter: 2439  total_loss: 25.62  loss_ce: 0.6579  loss_mask: 0.3954  loss_dice: 1.14  loss_contrastive: 0  loss_ce_0: 1.342  loss_mask_0: 0.4966  loss_dice_0: 1.535  loss_ce_1: 0.7654  loss_mask_1: 0.4004  loss_dice_1: 1.178  loss_ce_2: 0.8377  loss_mask_2: 0.4228  loss_dice_2: 1.197  loss_ce_3: 0.721  loss_mask_3: 0.4374  loss_dice_3: 1.246  loss_ce_4: 0.6371  loss_mask_4: 0.4192  loss_dice_4: 1.139  loss_ce_5: 0.5913  loss_mask_5: 0.379  loss_dice_5: 1.164  loss_ce_6: 0.5554  loss_mask_6: 0.3909  loss_dice_6: 1.142  loss_ce_7: 0.5297  loss_mask_7: 0.4011  loss_dice_7: 1.101  loss_ce_8: 0.5261  loss_mask_8: 0.3939  loss_dice_8: 1.134    time: 0.3206  last_time: 0.3270  data_time: 0.0032  last_data_time: 0.0035   lr: 7.7754e-05  max_mem: 14733M
[10/03 23:07:29] d2.utils.events INFO:  eta: 0:39:49  iter: 2459  total_loss: 26.49  loss_ce: 0.5062  loss_mask: 0.33  loss_dice: 1.142  loss_contrastive: 0  loss_ce_0: 1.273  loss_mask_0: 0.3921  loss_dice_0: 1.506  loss_ce_1: 0.822  loss_mask_1: 0.3385  loss_dice_1: 1.284  loss_ce_2: 0.6239  loss_mask_2: 0.368  loss_dice_2: 1.398  loss_ce_3: 0.5712  loss_mask_3: 0.3503  loss_dice_3: 1.152  loss_ce_4: 0.5226  loss_mask_4: 0.3387  loss_dice_4: 1.103  loss_ce_5: 0.6702  loss_mask_5: 0.3491  loss_dice_5: 1.251  loss_ce_6: 0.6795  loss_mask_6: 0.3499  loss_dice_6: 1.059  loss_ce_7: 0.6176  loss_mask_7: 0.3355  loss_dice_7: 1.122  loss_ce_8: 0.5508  loss_mask_8: 0.3195  loss_dice_8: 1.118    time: 0.3206  last_time: 0.3133  data_time: 0.0037  last_data_time: 0.0029   lr: 7.7569e-05  max_mem: 14733M
[10/03 23:07:36] d2.utils.events INFO:  eta: 0:39:42  iter: 2479  total_loss: 16.08  loss_ce: 0.3273  loss_mask: 0.1874  loss_dice: 0.9098  loss_contrastive: 0  loss_ce_0: 1.102  loss_mask_0: 0.2623  loss_dice_0: 1.161  loss_ce_1: 0.5198  loss_mask_1: 0.2194  loss_dice_1: 1.15  loss_ce_2: 0.5818  loss_mask_2: 0.2499  loss_dice_2: 0.9446  loss_ce_3: 0.4624  loss_mask_3: 0.2238  loss_dice_3: 1.1  loss_ce_4: 0.4  loss_mask_4: 0.2186  loss_dice_4: 0.8774  loss_ce_5: 0.4147  loss_mask_5: 0.2338  loss_dice_5: 1.006  loss_ce_6: 0.5515  loss_mask_6: 0.2128  loss_dice_6: 0.7803  loss_ce_7: 0.3675  loss_mask_7: 0.2243  loss_dice_7: 0.8357  loss_ce_8: 0.2838  loss_mask_8: 0.2026  loss_dice_8: 0.925    time: 0.3206  last_time: 0.3018  data_time: 0.0038  last_data_time: 0.0027   lr: 7.7383e-05  max_mem: 14733M
[10/03 23:07:42] d2.utils.events INFO:  eta: 0:39:36  iter: 2499  total_loss: 19.68  loss_ce: 0.3198  loss_mask: 0.4499  loss_dice: 1.018  loss_contrastive: 0  loss_ce_0: 1.043  loss_mask_0: 0.4963  loss_dice_0: 1.197  loss_ce_1: 0.6687  loss_mask_1: 0.4935  loss_dice_1: 1.041  loss_ce_2: 0.6391  loss_mask_2: 0.5272  loss_dice_2: 0.9401  loss_ce_3: 0.6167  loss_mask_3: 0.4555  loss_dice_3: 0.8604  loss_ce_4: 0.4836  loss_mask_4: 0.5088  loss_dice_4: 1.11  loss_ce_5: 0.4265  loss_mask_5: 0.5108  loss_dice_5: 0.9292  loss_ce_6: 0.3927  loss_mask_6: 0.4832  loss_dice_6: 0.9613  loss_ce_7: 0.3986  loss_mask_7: 0.5117  loss_dice_7: 0.9524  loss_ce_8: 0.4325  loss_mask_8: 0.5129  loss_dice_8: 0.9011    time: 0.3206  last_time: 0.3041  data_time: 0.0037  last_data_time: 0.0026   lr: 7.7198e-05  max_mem: 14733M
[10/03 23:07:49] d2.utils.events INFO:  eta: 0:39:30  iter: 2519  total_loss: 26.06  loss_ce: 0.5075  loss_mask: 0.4007  loss_dice: 1.14  loss_contrastive: 0  loss_ce_0: 1.451  loss_mask_0: 0.4689  loss_dice_0: 1.454  loss_ce_1: 1.013  loss_mask_1: 0.432  loss_dice_1: 1.209  loss_ce_2: 0.9627  loss_mask_2: 0.4988  loss_dice_2: 1.095  loss_ce_3: 0.8065  loss_mask_3: 0.3934  loss_dice_3: 1.201  loss_ce_4: 0.5716  loss_mask_4: 0.3926  loss_dice_4: 1.195  loss_ce_5: 0.6728  loss_mask_5: 0.377  loss_dice_5: 1.128  loss_ce_6: 0.5934  loss_mask_6: 0.409  loss_dice_6: 1.087  loss_ce_7: 0.6256  loss_mask_7: 0.3794  loss_dice_7: 1.158  loss_ce_8: 0.6648  loss_mask_8: 0.3963  loss_dice_8: 1.074    time: 0.3206  last_time: 0.3130  data_time: 0.0031  last_data_time: 0.0035   lr: 7.7013e-05  max_mem: 14733M
[10/03 23:07:55] d2.utils.events INFO:  eta: 0:39:26  iter: 2539  total_loss: 22.87  loss_ce: 0.3769  loss_mask: 0.2925  loss_dice: 1.134  loss_contrastive: 0  loss_ce_0: 1.243  loss_mask_0: 0.2844  loss_dice_0: 1.399  loss_ce_1: 0.996  loss_mask_1: 0.2925  loss_dice_1: 1.221  loss_ce_2: 0.7725  loss_mask_2: 0.3207  loss_dice_2: 1.144  loss_ce_3: 0.6227  loss_mask_3: 0.2952  loss_dice_3: 1.148  loss_ce_4: 0.5292  loss_mask_4: 0.3101  loss_dice_4: 1.186  loss_ce_5: 0.4493  loss_mask_5: 0.2814  loss_dice_5: 1.183  loss_ce_6: 0.4498  loss_mask_6: 0.3003  loss_dice_6: 1.001  loss_ce_7: 0.4466  loss_mask_7: 0.2725  loss_dice_7: 1.048  loss_ce_8: 0.4465  loss_mask_8: 0.265  loss_dice_8: 1.168    time: 0.3206  last_time: 0.3341  data_time: 0.0036  last_data_time: 0.0035   lr: 7.6828e-05  max_mem: 14733M
[10/03 23:08:02] d2.utils.events INFO:  eta: 0:39:18  iter: 2559  total_loss: 21.29  loss_ce: 0.4064  loss_mask: 0.3004  loss_dice: 1.117  loss_contrastive: 0  loss_ce_0: 1.227  loss_mask_0: 0.3557  loss_dice_0: 1.47  loss_ce_1: 0.7587  loss_mask_1: 0.3207  loss_dice_1: 1.253  loss_ce_2: 0.7484  loss_mask_2: 0.3067  loss_dice_2: 1.109  loss_ce_3: 0.5318  loss_mask_3: 0.3023  loss_dice_3: 1.076  loss_ce_4: 0.4754  loss_mask_4: 0.2909  loss_dice_4: 1.21  loss_ce_5: 0.3459  loss_mask_5: 0.2953  loss_dice_5: 1.052  loss_ce_6: 0.3276  loss_mask_6: 0.2835  loss_dice_6: 1.03  loss_ce_7: 0.3476  loss_mask_7: 0.2861  loss_dice_7: 1.053  loss_ce_8: 0.3862  loss_mask_8: 0.3014  loss_dice_8: 1.18    time: 0.3206  last_time: 0.3244  data_time: 0.0036  last_data_time: 0.0063   lr: 7.6642e-05  max_mem: 14733M
[10/03 23:08:08] d2.utils.events INFO:  eta: 0:39:14  iter: 2579  total_loss: 15.97  loss_ce: 0.3145  loss_mask: 0.1978  loss_dice: 0.6889  loss_contrastive: 0  loss_ce_0: 1.261  loss_mask_0: 0.2563  loss_dice_0: 0.9214  loss_ce_1: 0.6997  loss_mask_1: 0.2154  loss_dice_1: 0.688  loss_ce_2: 0.4887  loss_mask_2: 0.2479  loss_dice_2: 0.8089  loss_ce_3: 0.4696  loss_mask_3: 0.2478  loss_dice_3: 0.7326  loss_ce_4: 0.31  loss_mask_4: 0.2259  loss_dice_4: 0.8027  loss_ce_5: 0.3779  loss_mask_5: 0.2387  loss_dice_5: 0.8023  loss_ce_6: 0.2904  loss_mask_6: 0.2345  loss_dice_6: 0.7406  loss_ce_7: 0.3255  loss_mask_7: 0.2176  loss_dice_7: 0.7471  loss_ce_8: 0.3061  loss_mask_8: 0.2  loss_dice_8: 0.8315    time: 0.3206  last_time: 0.3170  data_time: 0.0036  last_data_time: 0.0078   lr: 7.6457e-05  max_mem: 14733M
[10/03 23:08:14] d2.utils.events INFO:  eta: 0:39:07  iter: 2599  total_loss: 21.8  loss_ce: 0.5942  loss_mask: 0.3245  loss_dice: 1.012  loss_contrastive: 0  loss_ce_0: 1.082  loss_mask_0: 0.3934  loss_dice_0: 1.36  loss_ce_1: 0.8341  loss_mask_1: 0.3629  loss_dice_1: 1.207  loss_ce_2: 0.7569  loss_mask_2: 0.3353  loss_dice_2: 1.139  loss_ce_3: 0.6577  loss_mask_3: 0.3435  loss_dice_3: 1.188  loss_ce_4: 0.5862  loss_mask_4: 0.3177  loss_dice_4: 1.092  loss_ce_5: 0.6006  loss_mask_5: 0.3251  loss_dice_5: 1.117  loss_ce_6: 0.5423  loss_mask_6: 0.3026  loss_dice_6: 1.092  loss_ce_7: 0.5241  loss_mask_7: 0.3004  loss_dice_7: 1.012  loss_ce_8: 0.4809  loss_mask_8: 0.325  loss_dice_8: 1.15    time: 0.3206  last_time: 0.3430  data_time: 0.0031  last_data_time: 0.0050   lr: 7.6271e-05  max_mem: 14733M
[10/03 23:08:21] d2.utils.events INFO:  eta: 0:39:00  iter: 2619  total_loss: 24.35  loss_ce: 0.5282  loss_mask: 0.2916  loss_dice: 1.113  loss_contrastive: 0  loss_ce_0: 1.423  loss_mask_0: 0.2691  loss_dice_0: 1.44  loss_ce_1: 1.028  loss_mask_1: 0.2589  loss_dice_1: 1.383  loss_ce_2: 0.8335  loss_mask_2: 0.3116  loss_dice_2: 1.2  loss_ce_3: 0.7418  loss_mask_3: 0.2636  loss_dice_3: 1.291  loss_ce_4: 0.5687  loss_mask_4: 0.2716  loss_dice_4: 1.148  loss_ce_5: 0.6432  loss_mask_5: 0.2614  loss_dice_5: 1.15  loss_ce_6: 0.5524  loss_mask_6: 0.2815  loss_dice_6: 1.116  loss_ce_7: 0.5439  loss_mask_7: 0.2783  loss_dice_7: 1.083  loss_ce_8: 0.6657  loss_mask_8: 0.3009  loss_dice_8: 1.188    time: 0.3205  last_time: 0.3172  data_time: 0.0034  last_data_time: 0.0048   lr: 7.6086e-05  max_mem: 14733M
[10/03 23:08:27] d2.utils.events INFO:  eta: 0:38:52  iter: 2639  total_loss: 21.2  loss_ce: 0.2051  loss_mask: 0.2019  loss_dice: 0.9057  loss_contrastive: 0  loss_ce_0: 1.102  loss_mask_0: 0.3135  loss_dice_0: 1.121  loss_ce_1: 0.6271  loss_mask_1: 0.2185  loss_dice_1: 1.059  loss_ce_2: 0.6296  loss_mask_2: 0.2862  loss_dice_2: 1.041  loss_ce_3: 0.3139  loss_mask_3: 0.2945  loss_dice_3: 1.085  loss_ce_4: 0.2515  loss_mask_4: 0.2323  loss_dice_4: 0.9187  loss_ce_5: 0.2925  loss_mask_5: 0.2804  loss_dice_5: 1.073  loss_ce_6: 0.2099  loss_mask_6: 0.2439  loss_dice_6: 1.075  loss_ce_7: 0.3307  loss_mask_7: 0.2399  loss_dice_7: 1.014  loss_ce_8: 0.179  loss_mask_8: 0.1978  loss_dice_8: 0.9769    time: 0.3205  last_time: 0.3419  data_time: 0.0035  last_data_time: 0.0040   lr: 7.59e-05  max_mem: 14733M
[10/03 23:08:34] d2.utils.events INFO:  eta: 0:38:47  iter: 2659  total_loss: 19.45  loss_ce: 0.4149  loss_mask: 0.4692  loss_dice: 1.012  loss_contrastive: 0  loss_ce_0: 0.9746  loss_mask_0: 0.4741  loss_dice_0: 1.265  loss_ce_1: 0.687  loss_mask_1: 0.4511  loss_dice_1: 1.05  loss_ce_2: 0.5385  loss_mask_2: 0.473  loss_dice_2: 1.032  loss_ce_3: 0.5427  loss_mask_3: 0.4281  loss_dice_3: 1.023  loss_ce_4: 0.4454  loss_mask_4: 0.455  loss_dice_4: 1.015  loss_ce_5: 0.4485  loss_mask_5: 0.4802  loss_dice_5: 1.009  loss_ce_6: 0.3229  loss_mask_6: 0.4481  loss_dice_6: 0.9876  loss_ce_7: 0.3428  loss_mask_7: 0.4489  loss_dice_7: 1.018  loss_ce_8: 0.387  loss_mask_8: 0.4659  loss_dice_8: 1.139    time: 0.3205  last_time: 0.3426  data_time: 0.0035  last_data_time: 0.0023   lr: 7.5715e-05  max_mem: 14733M
[10/03 23:08:40] d2.utils.events INFO:  eta: 0:38:43  iter: 2679  total_loss: 20.25  loss_ce: 0.4248  loss_mask: 0.3539  loss_dice: 0.7516  loss_contrastive: 0  loss_ce_0: 1.137  loss_mask_0: 0.4064  loss_dice_0: 1.056  loss_ce_1: 0.56  loss_mask_1: 0.3809  loss_dice_1: 0.9493  loss_ce_2: 0.5416  loss_mask_2: 0.3766  loss_dice_2: 0.8946  loss_ce_3: 0.4333  loss_mask_3: 0.386  loss_dice_3: 0.8708  loss_ce_4: 0.4407  loss_mask_4: 0.4033  loss_dice_4: 0.8861  loss_ce_5: 0.3371  loss_mask_5: 0.3901  loss_dice_5: 0.9413  loss_ce_6: 0.2739  loss_mask_6: 0.353  loss_dice_6: 0.8999  loss_ce_7: 0.3316  loss_mask_7: 0.3612  loss_dice_7: 0.9354  loss_ce_8: 0.3502  loss_mask_8: 0.3328  loss_dice_8: 0.8395    time: 0.3206  last_time: 0.3088  data_time: 0.0033  last_data_time: 0.0030   lr: 7.5529e-05  max_mem: 14733M
[10/03 23:08:46] d2.utils.events INFO:  eta: 0:38:37  iter: 2699  total_loss: 25.09  loss_ce: 0.6246  loss_mask: 0.326  loss_dice: 1.235  loss_contrastive: 0  loss_ce_0: 1.602  loss_mask_0: 0.4571  loss_dice_0: 1.467  loss_ce_1: 1.1  loss_mask_1: 0.2819  loss_dice_1: 1.354  loss_ce_2: 0.9636  loss_mask_2: 0.2841  loss_dice_2: 1.247  loss_ce_3: 0.8486  loss_mask_3: 0.3511  loss_dice_3: 1.262  loss_ce_4: 0.7081  loss_mask_4: 0.3354  loss_dice_4: 1.224  loss_ce_5: 0.5893  loss_mask_5: 0.3378  loss_dice_5: 1.228  loss_ce_6: 0.6883  loss_mask_6: 0.332  loss_dice_6: 1.27  loss_ce_7: 0.5682  loss_mask_7: 0.3326  loss_dice_7: 1.243  loss_ce_8: 0.5735  loss_mask_8: 0.3167  loss_dice_8: 1.281    time: 0.3205  last_time: 0.3188  data_time: 0.0032  last_data_time: 0.0034   lr: 7.5343e-05  max_mem: 14733M
[10/03 23:08:53] d2.utils.events INFO:  eta: 0:38:30  iter: 2719  total_loss: 20.83  loss_ce: 0.4102  loss_mask: 0.3323  loss_dice: 1.122  loss_contrastive: 0  loss_ce_0: 1.238  loss_mask_0: 0.3957  loss_dice_0: 1.435  loss_ce_1: 0.8274  loss_mask_1: 0.3326  loss_dice_1: 1.036  loss_ce_2: 0.706  loss_mask_2: 0.3505  loss_dice_2: 1.125  loss_ce_3: 0.6082  loss_mask_3: 0.3177  loss_dice_3: 1.073  loss_ce_4: 0.4947  loss_mask_4: 0.3009  loss_dice_4: 1.09  loss_ce_5: 0.4179  loss_mask_5: 0.3281  loss_dice_5: 1.135  loss_ce_6: 0.386  loss_mask_6: 0.3276  loss_dice_6: 0.9092  loss_ce_7: 0.4409  loss_mask_7: 0.3322  loss_dice_7: 1.057  loss_ce_8: 0.4381  loss_mask_8: 0.3379  loss_dice_8: 1.091    time: 0.3205  last_time: 0.2299  data_time: 0.0034  last_data_time: 0.0028   lr: 7.5157e-05  max_mem: 14733M
[10/03 23:08:59] d2.utils.events INFO:  eta: 0:38:24  iter: 2739  total_loss: 20.22  loss_ce: 0.4367  loss_mask: 0.4674  loss_dice: 0.9591  loss_contrastive: 0  loss_ce_0: 0.894  loss_mask_0: 0.3786  loss_dice_0: 1.139  loss_ce_1: 0.7436  loss_mask_1: 0.3789  loss_dice_1: 1.163  loss_ce_2: 0.5969  loss_mask_2: 0.4021  loss_dice_2: 1.1  loss_ce_3: 0.5832  loss_mask_3: 0.3967  loss_dice_3: 1.034  loss_ce_4: 0.5437  loss_mask_4: 0.4641  loss_dice_4: 1.032  loss_ce_5: 0.4696  loss_mask_5: 0.4381  loss_dice_5: 0.9869  loss_ce_6: 0.439  loss_mask_6: 0.4642  loss_dice_6: 1.032  loss_ce_7: 0.3878  loss_mask_7: 0.5003  loss_dice_7: 1.105  loss_ce_8: 0.4817  loss_mask_8: 0.4552  loss_dice_8: 1.088    time: 0.3205  last_time: 0.3052  data_time: 0.0031  last_data_time: 0.0027   lr: 7.4972e-05  max_mem: 14733M
[10/03 23:09:06] d2.utils.events INFO:  eta: 0:38:19  iter: 2759  total_loss: 20.06  loss_ce: 0.3618  loss_mask: 0.3893  loss_dice: 0.8636  loss_contrastive: 0  loss_ce_0: 0.9799  loss_mask_0: 0.4266  loss_dice_0: 0.935  loss_ce_1: 0.7781  loss_mask_1: 0.3812  loss_dice_1: 0.7533  loss_ce_2: 0.7499  loss_mask_2: 0.3802  loss_dice_2: 0.8097  loss_ce_3: 0.447  loss_mask_3: 0.378  loss_dice_3: 0.8442  loss_ce_4: 0.4717  loss_mask_4: 0.343  loss_dice_4: 0.8291  loss_ce_5: 0.3963  loss_mask_5: 0.3887  loss_dice_5: 0.9007  loss_ce_6: 0.3685  loss_mask_6: 0.3559  loss_dice_6: 0.8879  loss_ce_7: 0.3799  loss_mask_7: 0.3665  loss_dice_7: 0.8624  loss_ce_8: 0.335  loss_mask_8: 0.3758  loss_dice_8: 0.8414    time: 0.3205  last_time: 0.3252  data_time: 0.0031  last_data_time: 0.0027   lr: 7.4786e-05  max_mem: 14733M
[10/03 23:09:12] d2.utils.events INFO:  eta: 0:38:13  iter: 2779  total_loss: 22.15  loss_ce: 0.4608  loss_mask: 0.2063  loss_dice: 1.102  loss_contrastive: 0  loss_ce_0: 1.085  loss_mask_0: 0.2242  loss_dice_0: 1.492  loss_ce_1: 0.772  loss_mask_1: 0.2165  loss_dice_1: 1.251  loss_ce_2: 0.8772  loss_mask_2: 0.2089  loss_dice_2: 1.237  loss_ce_3: 0.7029  loss_mask_3: 0.1962  loss_dice_3: 1.189  loss_ce_4: 0.6446  loss_mask_4: 0.1961  loss_dice_4: 1.174  loss_ce_5: 0.5957  loss_mask_5: 0.1946  loss_dice_5: 1.242  loss_ce_6: 0.5709  loss_mask_6: 0.1911  loss_dice_6: 1.123  loss_ce_7: 0.3534  loss_mask_7: 0.2126  loss_dice_7: 1.183  loss_ce_8: 0.4061  loss_mask_8: 0.2008  loss_dice_8: 1.056    time: 0.3206  last_time: 0.3200  data_time: 0.0037  last_data_time: 0.0034   lr: 7.46e-05  max_mem: 14733M
[10/03 23:09:19] d2.utils.events INFO:  eta: 0:38:08  iter: 2799  total_loss: 25.85  loss_ce: 0.5759  loss_mask: 0.3242  loss_dice: 1.214  loss_contrastive: 0  loss_ce_0: 1.35  loss_mask_0: 0.3167  loss_dice_0: 1.571  loss_ce_1: 0.9453  loss_mask_1: 0.2972  loss_dice_1: 1.254  loss_ce_2: 0.7466  loss_mask_2: 0.3035  loss_dice_2: 1.392  loss_ce_3: 0.8028  loss_mask_3: 0.3252  loss_dice_3: 1.413  loss_ce_4: 0.7025  loss_mask_4: 0.2882  loss_dice_4: 1.336  loss_ce_5: 0.6874  loss_mask_5: 0.3021  loss_dice_5: 1.376  loss_ce_6: 0.6298  loss_mask_6: 0.2759  loss_dice_6: 1.119  loss_ce_7: 0.6184  loss_mask_7: 0.2818  loss_dice_7: 1.197  loss_ce_8: 0.5402  loss_mask_8: 0.2644  loss_dice_8: 1.233    time: 0.3206  last_time: 0.3273  data_time: 0.0033  last_data_time: 0.0032   lr: 7.4414e-05  max_mem: 14733M
[10/03 23:09:25] d2.utils.events INFO:  eta: 0:38:02  iter: 2819  total_loss: 18.54  loss_ce: 0.3876  loss_mask: 0.3602  loss_dice: 0.9913  loss_contrastive: 0  loss_ce_0: 1.167  loss_mask_0: 0.4148  loss_dice_0: 1.197  loss_ce_1: 0.832  loss_mask_1: 0.35  loss_dice_1: 1.122  loss_ce_2: 0.6394  loss_mask_2: 0.3209  loss_dice_2: 1.045  loss_ce_3: 0.546  loss_mask_3: 0.3212  loss_dice_3: 1.039  loss_ce_4: 0.5098  loss_mask_4: 0.3637  loss_dice_4: 1.066  loss_ce_5: 0.4873  loss_mask_5: 0.3366  loss_dice_5: 1.011  loss_ce_6: 0.5143  loss_mask_6: 0.3562  loss_dice_6: 0.9567  loss_ce_7: 0.353  loss_mask_7: 0.3493  loss_dice_7: 1.026  loss_ce_8: 0.3534  loss_mask_8: 0.3636  loss_dice_8: 0.9389    time: 0.3205  last_time: 0.3272  data_time: 0.0036  last_data_time: 0.0032   lr: 7.4228e-05  max_mem: 14733M
[10/03 23:09:31] d2.utils.events INFO:  eta: 0:37:55  iter: 2839  total_loss: 20.12  loss_ce: 0.3122  loss_mask: 0.3165  loss_dice: 0.8699  loss_contrastive: 0  loss_ce_0: 0.8623  loss_mask_0: 0.3281  loss_dice_0: 1.05  loss_ce_1: 0.5016  loss_mask_1: 0.3095  loss_dice_1: 1.06  loss_ce_2: 0.5865  loss_mask_2: 0.3471  loss_dice_2: 0.9039  loss_ce_3: 0.5711  loss_mask_3: 0.2941  loss_dice_3: 0.9508  loss_ce_4: 0.3681  loss_mask_4: 0.3487  loss_dice_4: 0.9306  loss_ce_5: 0.4041  loss_mask_5: 0.3288  loss_dice_5: 0.9424  loss_ce_6: 0.488  loss_mask_6: 0.3483  loss_dice_6: 0.8808  loss_ce_7: 0.3545  loss_mask_7: 0.3564  loss_dice_7: 0.8843  loss_ce_8: 0.3117  loss_mask_8: 0.2869  loss_dice_8: 0.8751    time: 0.3206  last_time: 0.3256  data_time: 0.0030  last_data_time: 0.0027   lr: 7.4042e-05  max_mem: 14733M
[10/03 23:09:38] d2.utils.events INFO:  eta: 0:37:49  iter: 2859  total_loss: 18.19  loss_ce: 0.3644  loss_mask: 0.3679  loss_dice: 0.98  loss_contrastive: 0  loss_ce_0: 1.123  loss_mask_0: 0.446  loss_dice_0: 1.12  loss_ce_1: 0.7853  loss_mask_1: 0.5189  loss_dice_1: 1.038  loss_ce_2: 0.5754  loss_mask_2: 0.3998  loss_dice_2: 0.9568  loss_ce_3: 0.4985  loss_mask_3: 0.4034  loss_dice_3: 0.885  loss_ce_4: 0.4239  loss_mask_4: 0.4181  loss_dice_4: 0.9711  loss_ce_5: 0.4039  loss_mask_5: 0.4136  loss_dice_5: 0.9077  loss_ce_6: 0.4637  loss_mask_6: 0.3505  loss_dice_6: 0.9179  loss_ce_7: 0.4495  loss_mask_7: 0.3581  loss_dice_7: 0.8919  loss_ce_8: 0.3627  loss_mask_8: 0.3521  loss_dice_8: 0.9382    time: 0.3205  last_time: 0.3326  data_time: 0.0033  last_data_time: 0.0029   lr: 7.3856e-05  max_mem: 14733M
[10/03 23:09:44] d2.utils.events INFO:  eta: 0:37:43  iter: 2879  total_loss: 18.03  loss_ce: 0.437  loss_mask: 0.3975  loss_dice: 0.9354  loss_contrastive: 0  loss_ce_0: 1.236  loss_mask_0: 0.3406  loss_dice_0: 1.183  loss_ce_1: 0.7372  loss_mask_1: 0.3844  loss_dice_1: 1.007  loss_ce_2: 0.5962  loss_mask_2: 0.3736  loss_dice_2: 0.9872  loss_ce_3: 0.5425  loss_mask_3: 0.3456  loss_dice_3: 0.9817  loss_ce_4: 0.4984  loss_mask_4: 0.3566  loss_dice_4: 0.9529  loss_ce_5: 0.498  loss_mask_5: 0.3127  loss_dice_5: 0.9217  loss_ce_6: 0.4558  loss_mask_6: 0.317  loss_dice_6: 0.9204  loss_ce_7: 0.4545  loss_mask_7: 0.3585  loss_dice_7: 0.9409  loss_ce_8: 0.4229  loss_mask_8: 0.3534  loss_dice_8: 0.9299    time: 0.3205  last_time: 0.3075  data_time: 0.0037  last_data_time: 0.0026   lr: 7.3669e-05  max_mem: 14733M
[10/03 23:09:51] d2.utils.events INFO:  eta: 0:37:36  iter: 2899  total_loss: 23.29  loss_ce: 0.3938  loss_mask: 0.4067  loss_dice: 1.151  loss_contrastive: 0  loss_ce_0: 1.237  loss_mask_0: 0.5154  loss_dice_0: 1.363  loss_ce_1: 0.773  loss_mask_1: 0.4054  loss_dice_1: 1.388  loss_ce_2: 0.7769  loss_mask_2: 0.3485  loss_dice_2: 1.13  loss_ce_3: 0.6064  loss_mask_3: 0.3484  loss_dice_3: 1.105  loss_ce_4: 0.4703  loss_mask_4: 0.3651  loss_dice_4: 1.179  loss_ce_5: 0.5596  loss_mask_5: 0.3717  loss_dice_5: 1.139  loss_ce_6: 0.3886  loss_mask_6: 0.3533  loss_dice_6: 1.18  loss_ce_7: 0.4453  loss_mask_7: 0.3231  loss_dice_7: 1.214  loss_ce_8: 0.4459  loss_mask_8: 0.3471  loss_dice_8: 1.184    time: 0.3205  last_time: 0.3039  data_time: 0.0035  last_data_time: 0.0034   lr: 7.3483e-05  max_mem: 14733M
[10/03 23:09:57] d2.utils.events INFO:  eta: 0:37:30  iter: 2919  total_loss: 24.8  loss_ce: 0.4759  loss_mask: 0.3429  loss_dice: 1.033  loss_contrastive: 0  loss_ce_0: 1.322  loss_mask_0: 0.5696  loss_dice_0: 1.356  loss_ce_1: 0.8781  loss_mask_1: 0.3789  loss_dice_1: 1.194  loss_ce_2: 0.8206  loss_mask_2: 0.3791  loss_dice_2: 1.094  loss_ce_3: 0.6618  loss_mask_3: 0.3225  loss_dice_3: 1.165  loss_ce_4: 0.7089  loss_mask_4: 0.3631  loss_dice_4: 1.079  loss_ce_5: 0.6122  loss_mask_5: 0.3451  loss_dice_5: 1.106  loss_ce_6: 0.6205  loss_mask_6: 0.3142  loss_dice_6: 1.103  loss_ce_7: 0.5272  loss_mask_7: 0.3286  loss_dice_7: 1.143  loss_ce_8: 0.4052  loss_mask_8: 0.3192  loss_dice_8: 1.111    time: 0.3205  last_time: 0.3103  data_time: 0.0031  last_data_time: 0.0035   lr: 7.3297e-05  max_mem: 14733M
[10/03 23:10:04] d2.utils.events INFO:  eta: 0:37:23  iter: 2939  total_loss: 26.89  loss_ce: 0.4429  loss_mask: 0.4173  loss_dice: 1.199  loss_contrastive: 0  loss_ce_0: 1.157  loss_mask_0: 0.477  loss_dice_0: 1.402  loss_ce_1: 0.9362  loss_mask_1: 0.4209  loss_dice_1: 1.218  loss_ce_2: 0.9365  loss_mask_2: 0.3934  loss_dice_2: 1.3  loss_ce_3: 0.688  loss_mask_3: 0.4171  loss_dice_3: 1.15  loss_ce_4: 0.735  loss_mask_4: 0.4183  loss_dice_4: 1.107  loss_ce_5: 0.5588  loss_mask_5: 0.4102  loss_dice_5: 1.224  loss_ce_6: 0.5305  loss_mask_6: 0.4145  loss_dice_6: 1.07  loss_ce_7: 0.5038  loss_mask_7: 0.4155  loss_dice_7: 1.149  loss_ce_8: 0.474  loss_mask_8: 0.4104  loss_dice_8: 1.196    time: 0.3205  last_time: 0.3111  data_time: 0.0035  last_data_time: 0.0037   lr: 7.311e-05  max_mem: 14733M
[10/03 23:10:10] d2.utils.events INFO:  eta: 0:37:17  iter: 2959  total_loss: 20.7  loss_ce: 0.3344  loss_mask: 0.2916  loss_dice: 1.092  loss_contrastive: 0  loss_ce_0: 1.061  loss_mask_0: 0.3377  loss_dice_0: 1.347  loss_ce_1: 0.6355  loss_mask_1: 0.2673  loss_dice_1: 1.271  loss_ce_2: 0.5122  loss_mask_2: 0.3221  loss_dice_2: 1.173  loss_ce_3: 0.4333  loss_mask_3: 0.2408  loss_dice_3: 1.214  loss_ce_4: 0.2961  loss_mask_4: 0.2578  loss_dice_4: 1.049  loss_ce_5: 0.2979  loss_mask_5: 0.3031  loss_dice_5: 1.149  loss_ce_6: 0.3118  loss_mask_6: 0.3129  loss_dice_6: 1.131  loss_ce_7: 0.3224  loss_mask_7: 0.2825  loss_dice_7: 1.088  loss_ce_8: 0.3535  loss_mask_8: 0.3122  loss_dice_8: 1.139    time: 0.3205  last_time: 0.3080  data_time: 0.0029  last_data_time: 0.0032   lr: 7.2924e-05  max_mem: 14733M
[10/03 23:10:16] d2.utils.events INFO:  eta: 0:37:11  iter: 2979  total_loss: 22.94  loss_ce: 0.5719  loss_mask: 0.3659  loss_dice: 1.017  loss_contrastive: 0  loss_ce_0: 0.9957  loss_mask_0: 0.5341  loss_dice_0: 1.421  loss_ce_1: 0.65  loss_mask_1: 0.3775  loss_dice_1: 1.18  loss_ce_2: 0.6195  loss_mask_2: 0.3313  loss_dice_2: 1.105  loss_ce_3: 0.5657  loss_mask_3: 0.3845  loss_dice_3: 1.084  loss_ce_4: 0.5858  loss_mask_4: 0.3296  loss_dice_4: 1.098  loss_ce_5: 0.4233  loss_mask_5: 0.3456  loss_dice_5: 1.026  loss_ce_6: 0.4286  loss_mask_6: 0.3924  loss_dice_6: 0.9864  loss_ce_7: 0.4568  loss_mask_7: 0.3779  loss_dice_7: 0.9937  loss_ce_8: 0.4047  loss_mask_8: 0.4205  loss_dice_8: 1.002    time: 0.3205  last_time: 0.3047  data_time: 0.0036  last_data_time: 0.0025   lr: 7.2738e-05  max_mem: 14733M
[10/03 23:10:23] d2.utils.events INFO:  eta: 0:37:04  iter: 2999  total_loss: 27.12  loss_ce: 0.6303  loss_mask: 0.4018  loss_dice: 1.283  loss_contrastive: 0  loss_ce_0: 1.396  loss_mask_0: 0.4555  loss_dice_0: 1.686  loss_ce_1: 0.9607  loss_mask_1: 0.3792  loss_dice_1: 1.428  loss_ce_2: 0.93  loss_mask_2: 0.3887  loss_dice_2: 1.384  loss_ce_3: 0.7346  loss_mask_3: 0.3957  loss_dice_3: 1.404  loss_ce_4: 0.662  loss_mask_4: 0.4035  loss_dice_4: 1.334  loss_ce_5: 0.6798  loss_mask_5: 0.3916  loss_dice_5: 1.325  loss_ce_6: 0.687  loss_mask_6: 0.3975  loss_dice_6: 1.353  loss_ce_7: 0.5572  loss_mask_7: 0.3904  loss_dice_7: 1.353  loss_ce_8: 0.5901  loss_mask_8: 0.3891  loss_dice_8: 1.219    time: 0.3206  last_time: 0.3050  data_time: 0.0036  last_data_time: 0.0046   lr: 7.2551e-05  max_mem: 14733M
[10/03 23:10:29] d2.utils.events INFO:  eta: 0:36:58  iter: 3019  total_loss: 20.37  loss_ce: 0.5978  loss_mask: 0.3923  loss_dice: 0.8621  loss_contrastive: 0  loss_ce_0: 1.226  loss_mask_0: 0.4557  loss_dice_0: 1.304  loss_ce_1: 0.9062  loss_mask_1: 0.3887  loss_dice_1: 1.03  loss_ce_2: 0.8495  loss_mask_2: 0.3723  loss_dice_2: 0.9734  loss_ce_3: 0.788  loss_mask_3: 0.3945  loss_dice_3: 0.8952  loss_ce_4: 0.6252  loss_mask_4: 0.3915  loss_dice_4: 0.9566  loss_ce_5: 0.5781  loss_mask_5: 0.4344  loss_dice_5: 0.9042  loss_ce_6: 0.5706  loss_mask_6: 0.4103  loss_dice_6: 0.9919  loss_ce_7: 0.4653  loss_mask_7: 0.4362  loss_dice_7: 1.034  loss_ce_8: 0.5081  loss_mask_8: 0.4015  loss_dice_8: 0.9158    time: 0.3205  last_time: 0.3178  data_time: 0.0030  last_data_time: 0.0025   lr: 7.2365e-05  max_mem: 14733M
[10/03 23:10:36] d2.utils.events INFO:  eta: 0:36:51  iter: 3039  total_loss: 18.04  loss_ce: 0.3981  loss_mask: 0.2267  loss_dice: 0.9282  loss_contrastive: 0  loss_ce_0: 1.185  loss_mask_0: 0.2642  loss_dice_0: 1.011  loss_ce_1: 0.7333  loss_mask_1: 0.2633  loss_dice_1: 0.9363  loss_ce_2: 0.6751  loss_mask_2: 0.2387  loss_dice_2: 0.989  loss_ce_3: 0.5439  loss_mask_3: 0.2206  loss_dice_3: 1.024  loss_ce_4: 0.3763  loss_mask_4: 0.2097  loss_dice_4: 0.944  loss_ce_5: 0.3728  loss_mask_5: 0.2174  loss_dice_5: 0.9048  loss_ce_6: 0.3158  loss_mask_6: 0.2183  loss_dice_6: 0.9206  loss_ce_7: 0.3017  loss_mask_7: 0.2135  loss_dice_7: 0.9004  loss_ce_8: 0.3933  loss_mask_8: 0.2248  loss_dice_8: 0.8475    time: 0.3205  last_time: 0.3310  data_time: 0.0034  last_data_time: 0.0028   lr: 7.2178e-05  max_mem: 14733M
[10/03 23:10:42] d2.utils.events INFO:  eta: 0:36:44  iter: 3059  total_loss: 22.75  loss_ce: 0.5174  loss_mask: 0.4081  loss_dice: 1.11  loss_contrastive: 0  loss_ce_0: 1.377  loss_mask_0: 0.4488  loss_dice_0: 1.347  loss_ce_1: 0.9135  loss_mask_1: 0.441  loss_dice_1: 1.26  loss_ce_2: 0.7627  loss_mask_2: 0.3999  loss_dice_2: 1.144  loss_ce_3: 0.6819  loss_mask_3: 0.3976  loss_dice_3: 1.132  loss_ce_4: 0.6071  loss_mask_4: 0.3986  loss_dice_4: 1.125  loss_ce_5: 0.5497  loss_mask_5: 0.4091  loss_dice_5: 1.158  loss_ce_6: 0.497  loss_mask_6: 0.4035  loss_dice_6: 1.047  loss_ce_7: 0.5046  loss_mask_7: 0.4103  loss_dice_7: 1.014  loss_ce_8: 0.4669  loss_mask_8: 0.4006  loss_dice_8: 1.053    time: 0.3205  last_time: 0.3071  data_time: 0.0034  last_data_time: 0.0027   lr: 7.1991e-05  max_mem: 14733M
[10/03 23:10:48] d2.utils.events INFO:  eta: 0:36:36  iter: 3079  total_loss: 25.72  loss_ce: 0.4157  loss_mask: 0.3759  loss_dice: 1.125  loss_contrastive: 0  loss_ce_0: 1.303  loss_mask_0: 0.4914  loss_dice_0: 1.64  loss_ce_1: 0.9234  loss_mask_1: 0.3676  loss_dice_1: 1.277  loss_ce_2: 0.7168  loss_mask_2: 0.3888  loss_dice_2: 1.328  loss_ce_3: 0.704  loss_mask_3: 0.4151  loss_dice_3: 1.218  loss_ce_4: 0.5816  loss_mask_4: 0.343  loss_dice_4: 1.211  loss_ce_5: 0.5118  loss_mask_5: 0.347  loss_dice_5: 1.085  loss_ce_6: 0.5365  loss_mask_6: 0.3705  loss_dice_6: 1.274  loss_ce_7: 0.6421  loss_mask_7: 0.3622  loss_dice_7: 1.281  loss_ce_8: 0.4958  loss_mask_8: 0.3435  loss_dice_8: 1.152    time: 0.3205  last_time: 0.3082  data_time: 0.0034  last_data_time: 0.0038   lr: 7.1805e-05  max_mem: 14733M
[10/03 23:10:55] d2.utils.events INFO:  eta: 0:36:28  iter: 3099  total_loss: 25.42  loss_ce: 0.5331  loss_mask: 0.372  loss_dice: 1.099  loss_contrastive: 0  loss_ce_0: 1.62  loss_mask_0: 0.475  loss_dice_0: 1.745  loss_ce_1: 1.03  loss_mask_1: 0.3306  loss_dice_1: 1.521  loss_ce_2: 0.7998  loss_mask_2: 0.3471  loss_dice_2: 1.369  loss_ce_3: 0.7052  loss_mask_3: 0.3256  loss_dice_3: 1.302  loss_ce_4: 0.5897  loss_mask_4: 0.3223  loss_dice_4: 1.3  loss_ce_5: 0.5375  loss_mask_5: 0.3836  loss_dice_5: 1.208  loss_ce_6: 0.5446  loss_mask_6: 0.3811  loss_dice_6: 1.141  loss_ce_7: 0.5439  loss_mask_7: 0.3711  loss_dice_7: 1.18  loss_ce_8: 0.6649  loss_mask_8: 0.3902  loss_dice_8: 1.257    time: 0.3205  last_time: 0.3028  data_time: 0.0035  last_data_time: 0.0025   lr: 7.1618e-05  max_mem: 14733M
[10/03 23:11:01] d2.utils.events INFO:  eta: 0:36:22  iter: 3119  total_loss: 19.95  loss_ce: 0.4231  loss_mask: 0.494  loss_dice: 0.9027  loss_contrastive: 0  loss_ce_0: 1.153  loss_mask_0: 0.4674  loss_dice_0: 0.9306  loss_ce_1: 0.6414  loss_mask_1: 0.482  loss_dice_1: 0.8907  loss_ce_2: 0.5273  loss_mask_2: 0.4436  loss_dice_2: 0.8397  loss_ce_3: 0.4408  loss_mask_3: 0.4353  loss_dice_3: 0.9496  loss_ce_4: 0.3975  loss_mask_4: 0.4415  loss_dice_4: 0.8258  loss_ce_5: 0.3978  loss_mask_5: 0.4272  loss_dice_5: 0.9023  loss_ce_6: 0.3359  loss_mask_6: 0.4587  loss_dice_6: 0.9367  loss_ce_7: 0.3677  loss_mask_7: 0.4171  loss_dice_7: 0.9202  loss_ce_8: 0.4418  loss_mask_8: 0.4285  loss_dice_8: 0.9109    time: 0.3204  last_time: 0.3049  data_time: 0.0035  last_data_time: 0.0025   lr: 7.1431e-05  max_mem: 14733M
[10/03 23:11:07] d2.utils.events INFO:  eta: 0:36:16  iter: 3139  total_loss: 27.09  loss_ce: 0.6016  loss_mask: 0.3655  loss_dice: 1.204  loss_contrastive: 0  loss_ce_0: 1.082  loss_mask_0: 0.4907  loss_dice_0: 1.496  loss_ce_1: 0.8411  loss_mask_1: 0.3803  loss_dice_1: 1.323  loss_ce_2: 0.8118  loss_mask_2: 0.3861  loss_dice_2: 1.228  loss_ce_3: 0.7517  loss_mask_3: 0.3704  loss_dice_3: 1.263  loss_ce_4: 0.6927  loss_mask_4: 0.4125  loss_dice_4: 1.345  loss_ce_5: 0.6459  loss_mask_5: 0.408  loss_dice_5: 1.212  loss_ce_6: 0.6308  loss_mask_6: 0.3146  loss_dice_6: 1.202  loss_ce_7: 0.5845  loss_mask_7: 0.3597  loss_dice_7: 1.207  loss_ce_8: 0.6848  loss_mask_8: 0.3014  loss_dice_8: 1.191    time: 0.3204  last_time: 0.3049  data_time: 0.0034  last_data_time: 0.0026   lr: 7.1244e-05  max_mem: 14733M
[10/03 23:11:14] d2.utils.events INFO:  eta: 0:36:10  iter: 3159  total_loss: 22.28  loss_ce: 0.4195  loss_mask: 0.4513  loss_dice: 1.146  loss_contrastive: 0  loss_ce_0: 1.136  loss_mask_0: 0.4834  loss_dice_0: 1.44  loss_ce_1: 0.6893  loss_mask_1: 0.4309  loss_dice_1: 1.161  loss_ce_2: 0.5589  loss_mask_2: 0.4076  loss_dice_2: 1.209  loss_ce_3: 0.5344  loss_mask_3: 0.4453  loss_dice_3: 1.144  loss_ce_4: 0.5394  loss_mask_4: 0.444  loss_dice_4: 1.164  loss_ce_5: 0.5169  loss_mask_5: 0.4219  loss_dice_5: 1.171  loss_ce_6: 0.4879  loss_mask_6: 0.4514  loss_dice_6: 1.196  loss_ce_7: 0.4495  loss_mask_7: 0.4421  loss_dice_7: 1.138  loss_ce_8: 0.4368  loss_mask_8: 0.4418  loss_dice_8: 1.19    time: 0.3205  last_time: 0.3685  data_time: 0.0032  last_data_time: 0.0026   lr: 7.1057e-05  max_mem: 14733M
[10/03 23:11:20] d2.utils.events INFO:  eta: 0:36:05  iter: 3179  total_loss: 25.04  loss_ce: 0.5036  loss_mask: 0.3213  loss_dice: 1.231  loss_contrastive: 0  loss_ce_0: 1.169  loss_mask_0: 0.4077  loss_dice_0: 1.452  loss_ce_1: 0.9625  loss_mask_1: 0.3683  loss_dice_1: 1.206  loss_ce_2: 0.8719  loss_mask_2: 0.3612  loss_dice_2: 1.174  loss_ce_3: 0.6815  loss_mask_3: 0.3205  loss_dice_3: 1.187  loss_ce_4: 0.6371  loss_mask_4: 0.327  loss_dice_4: 1.248  loss_ce_5: 0.612  loss_mask_5: 0.3073  loss_dice_5: 1.225  loss_ce_6: 0.6023  loss_mask_6: 0.2791  loss_dice_6: 1.187  loss_ce_7: 0.5666  loss_mask_7: 0.3152  loss_dice_7: 1.18  loss_ce_8: 0.5518  loss_mask_8: 0.3075  loss_dice_8: 1.157    time: 0.3205  last_time: 0.3036  data_time: 0.0030  last_data_time: 0.0035   lr: 7.087e-05  max_mem: 14733M
[10/03 23:11:27] d2.utils.events INFO:  eta: 0:35:58  iter: 3199  total_loss: 23.97  loss_ce: 0.5251  loss_mask: 0.3505  loss_dice: 1.168  loss_contrastive: 0  loss_ce_0: 1.545  loss_mask_0: 0.4204  loss_dice_0: 1.555  loss_ce_1: 0.7518  loss_mask_1: 0.3598  loss_dice_1: 1.203  loss_ce_2: 0.7342  loss_mask_2: 0.3628  loss_dice_2: 1.294  loss_ce_3: 0.6035  loss_mask_3: 0.3594  loss_dice_3: 1.172  loss_ce_4: 0.5456  loss_mask_4: 0.3382  loss_dice_4: 1.205  loss_ce_5: 0.462  loss_mask_5: 0.3387  loss_dice_5: 1.229  loss_ce_6: 0.4247  loss_mask_6: 0.3531  loss_dice_6: 1.148  loss_ce_7: 0.5185  loss_mask_7: 0.3415  loss_dice_7: 1.069  loss_ce_8: 0.4869  loss_mask_8: 0.3395  loss_dice_8: 1.225    time: 0.3205  last_time: 0.3220  data_time: 0.0037  last_data_time: 0.0025   lr: 7.0683e-05  max_mem: 14733M
[10/03 23:11:34] d2.utils.events INFO:  eta: 0:35:54  iter: 3219  total_loss: 23.7  loss_ce: 0.5034  loss_mask: 0.2823  loss_dice: 1.16  loss_contrastive: 0  loss_ce_0: 1.325  loss_mask_0: 0.3215  loss_dice_0: 1.299  loss_ce_1: 0.8571  loss_mask_1: 0.3075  loss_dice_1: 1.42  loss_ce_2: 0.6909  loss_mask_2: 0.3224  loss_dice_2: 1.137  loss_ce_3: 0.6426  loss_mask_3: 0.3064  loss_dice_3: 1.149  loss_ce_4: 0.5927  loss_mask_4: 0.3019  loss_dice_4: 1.135  loss_ce_5: 0.5328  loss_mask_5: 0.2852  loss_dice_5: 1.093  loss_ce_6: 0.4963  loss_mask_6: 0.2912  loss_dice_6: 1.055  loss_ce_7: 0.5176  loss_mask_7: 0.2876  loss_dice_7: 1.123  loss_ce_8: 0.5277  loss_mask_8: 0.2824  loss_dice_8: 1.077    time: 0.3206  last_time: 0.3104  data_time: 0.0033  last_data_time: 0.0033   lr: 7.0496e-05  max_mem: 14733M
[10/03 23:11:40] d2.utils.events INFO:  eta: 0:35:46  iter: 3239  total_loss: 19.93  loss_ce: 0.4763  loss_mask: 0.3568  loss_dice: 1.039  loss_contrastive: 0  loss_ce_0: 0.9789  loss_mask_0: 0.3272  loss_dice_0: 1.356  loss_ce_1: 0.8306  loss_mask_1: 0.4317  loss_dice_1: 1.314  loss_ce_2: 0.737  loss_mask_2: 0.3257  loss_dice_2: 1.199  loss_ce_3: 0.5404  loss_mask_3: 0.3159  loss_dice_3: 1.063  loss_ce_4: 0.4796  loss_mask_4: 0.3449  loss_dice_4: 1.163  loss_ce_5: 0.5118  loss_mask_5: 0.3333  loss_dice_5: 1.107  loss_ce_6: 0.4735  loss_mask_6: 0.3799  loss_dice_6: 1.153  loss_ce_7: 0.4701  loss_mask_7: 0.3233  loss_dice_7: 1.204  loss_ce_8: 0.4567  loss_mask_8: 0.3506  loss_dice_8: 1.259    time: 0.3206  last_time: 0.3159  data_time: 0.0035  last_data_time: 0.0035   lr: 7.0309e-05  max_mem: 14733M
[10/03 23:11:46] d2.utils.events INFO:  eta: 0:35:39  iter: 3259  total_loss: 23.91  loss_ce: 0.6246  loss_mask: 0.379  loss_dice: 1.066  loss_contrastive: 0  loss_ce_0: 1.517  loss_mask_0: 0.4289  loss_dice_0: 1.265  loss_ce_1: 1.046  loss_mask_1: 0.3648  loss_dice_1: 1.242  loss_ce_2: 0.8786  loss_mask_2: 0.3881  loss_dice_2: 1.118  loss_ce_3: 0.8042  loss_mask_3: 0.3384  loss_dice_3: 1.008  loss_ce_4: 0.7659  loss_mask_4: 0.3369  loss_dice_4: 1.15  loss_ce_5: 0.6438  loss_mask_5: 0.338  loss_dice_5: 1.113  loss_ce_6: 0.699  loss_mask_6: 0.3415  loss_dice_6: 1.08  loss_ce_7: 0.6597  loss_mask_7: 0.3636  loss_dice_7: 1.139  loss_ce_8: 0.6357  loss_mask_8: 0.3703  loss_dice_8: 1.143    time: 0.3205  last_time: 0.3033  data_time: 0.0034  last_data_time: 0.0030   lr: 7.0122e-05  max_mem: 14733M
[10/03 23:11:53] d2.utils.events INFO:  eta: 0:35:32  iter: 3279  total_loss: 22.67  loss_ce: 0.3738  loss_mask: 0.4169  loss_dice: 1.107  loss_contrastive: 0  loss_ce_0: 1.113  loss_mask_0: 0.4794  loss_dice_0: 1.273  loss_ce_1: 0.7391  loss_mask_1: 0.4465  loss_dice_1: 1.145  loss_ce_2: 0.6116  loss_mask_2: 0.4385  loss_dice_2: 1.074  loss_ce_3: 0.633  loss_mask_3: 0.4294  loss_dice_3: 1.14  loss_ce_4: 0.4844  loss_mask_4: 0.4336  loss_dice_4: 1.128  loss_ce_5: 0.4606  loss_mask_5: 0.4319  loss_dice_5: 1.045  loss_ce_6: 0.4353  loss_mask_6: 0.425  loss_dice_6: 1.092  loss_ce_7: 0.42  loss_mask_7: 0.4172  loss_dice_7: 1.092  loss_ce_8: 0.4453  loss_mask_8: 0.4141  loss_dice_8: 1.066    time: 0.3205  last_time: 0.3269  data_time: 0.0033  last_data_time: 0.0032   lr: 6.9934e-05  max_mem: 14733M
[10/03 23:11:59] d2.utils.events INFO:  eta: 0:35:26  iter: 3299  total_loss: 21.2  loss_ce: 0.4808  loss_mask: 0.255  loss_dice: 1.002  loss_contrastive: 0  loss_ce_0: 1.395  loss_mask_0: 0.3469  loss_dice_0: 1.161  loss_ce_1: 0.9342  loss_mask_1: 0.2401  loss_dice_1: 1.052  loss_ce_2: 0.7845  loss_mask_2: 0.2741  loss_dice_2: 1.04  loss_ce_3: 0.6343  loss_mask_3: 0.282  loss_dice_3: 1.002  loss_ce_4: 0.4028  loss_mask_4: 0.2702  loss_dice_4: 0.9984  loss_ce_5: 0.537  loss_mask_5: 0.2736  loss_dice_5: 0.9607  loss_ce_6: 0.535  loss_mask_6: 0.2626  loss_dice_6: 1.03  loss_ce_7: 0.4372  loss_mask_7: 0.2615  loss_dice_7: 0.9803  loss_ce_8: 0.4274  loss_mask_8: 0.2649  loss_dice_8: 1.033    time: 0.3205  last_time: 0.3278  data_time: 0.0037  last_data_time: 0.0038   lr: 6.9747e-05  max_mem: 14733M
[10/03 23:12:06] d2.utils.events INFO:  eta: 0:35:22  iter: 3319  total_loss: 22.93  loss_ce: 0.4614  loss_mask: 0.4355  loss_dice: 1.21  loss_contrastive: 0  loss_ce_0: 1.274  loss_mask_0: 0.5142  loss_dice_0: 1.34  loss_ce_1: 0.7053  loss_mask_1: 0.5256  loss_dice_1: 1.187  loss_ce_2: 0.6558  loss_mask_2: 0.4635  loss_dice_2: 1.16  loss_ce_3: 0.5141  loss_mask_3: 0.5152  loss_dice_3: 1.181  loss_ce_4: 0.4623  loss_mask_4: 0.484  loss_dice_4: 1.218  loss_ce_5: 0.491  loss_mask_5: 0.4637  loss_dice_5: 1.149  loss_ce_6: 0.4182  loss_mask_6: 0.4566  loss_dice_6: 1.174  loss_ce_7: 0.4579  loss_mask_7: 0.4543  loss_dice_7: 1.11  loss_ce_8: 0.4848  loss_mask_8: 0.4367  loss_dice_8: 1.158    time: 0.3205  last_time: 0.3230  data_time: 0.0032  last_data_time: 0.0031   lr: 6.956e-05  max_mem: 14733M
[10/03 23:12:12] d2.utils.events INFO:  eta: 0:35:16  iter: 3339  total_loss: 17.81  loss_ce: 0.2535  loss_mask: 0.263  loss_dice: 0.9402  loss_contrastive: 0  loss_ce_0: 0.9071  loss_mask_0: 0.3229  loss_dice_0: 1.281  loss_ce_1: 0.53  loss_mask_1: 0.2733  loss_dice_1: 1.007  loss_ce_2: 0.4792  loss_mask_2: 0.2534  loss_dice_2: 0.9917  loss_ce_3: 0.4558  loss_mask_3: 0.2526  loss_dice_3: 0.991  loss_ce_4: 0.4459  loss_mask_4: 0.2322  loss_dice_4: 0.9956  loss_ce_5: 0.3014  loss_mask_5: 0.2473  loss_dice_5: 1.016  loss_ce_6: 0.2446  loss_mask_6: 0.2705  loss_dice_6: 0.957  loss_ce_7: 0.3122  loss_mask_7: 0.2872  loss_dice_7: 0.9958  loss_ce_8: 0.2586  loss_mask_8: 0.2615  loss_dice_8: 0.9839    time: 0.3205  last_time: 0.3109  data_time: 0.0036  last_data_time: 0.0047   lr: 6.9372e-05  max_mem: 14733M
[10/03 23:12:18] d2.utils.events INFO:  eta: 0:35:07  iter: 3359  total_loss: 24.55  loss_ce: 0.6763  loss_mask: 0.2067  loss_dice: 1.089  loss_contrastive: 0  loss_ce_0: 1.224  loss_mask_0: 0.3231  loss_dice_0: 1.302  loss_ce_1: 0.7921  loss_mask_1: 0.3118  loss_dice_1: 1.152  loss_ce_2: 0.6419  loss_mask_2: 0.3143  loss_dice_2: 1.086  loss_ce_3: 0.6293  loss_mask_3: 0.2026  loss_dice_3: 1.104  loss_ce_4: 0.5905  loss_mask_4: 0.1993  loss_dice_4: 1.043  loss_ce_5: 0.6169  loss_mask_5: 0.2896  loss_dice_5: 1.086  loss_ce_6: 0.6225  loss_mask_6: 0.2184  loss_dice_6: 0.9958  loss_ce_7: 0.6855  loss_mask_7: 0.2293  loss_dice_7: 0.9864  loss_ce_8: 0.614  loss_mask_8: 0.2022  loss_dice_8: 1.124    time: 0.3205  last_time: 0.3157  data_time: 0.0032  last_data_time: 0.0023   lr: 6.9185e-05  max_mem: 14733M
[10/03 23:12:25] d2.utils.events INFO:  eta: 0:35:00  iter: 3379  total_loss: 23.55  loss_ce: 0.5472  loss_mask: 0.3329  loss_dice: 0.9362  loss_contrastive: 0  loss_ce_0: 1.422  loss_mask_0: 0.4355  loss_dice_0: 1.27  loss_ce_1: 0.943  loss_mask_1: 0.3594  loss_dice_1: 1.118  loss_ce_2: 0.8743  loss_mask_2: 0.31  loss_dice_2: 1.068  loss_ce_3: 0.6344  loss_mask_3: 0.3537  loss_dice_3: 1.024  loss_ce_4: 0.6428  loss_mask_4: 0.3563  loss_dice_4: 0.954  loss_ce_5: 0.5679  loss_mask_5: 0.3413  loss_dice_5: 0.9413  loss_ce_6: 0.5608  loss_mask_6: 0.3303  loss_dice_6: 0.9344  loss_ce_7: 0.5922  loss_mask_7: 0.339  loss_dice_7: 0.9803  loss_ce_8: 0.5227  loss_mask_8: 0.3591  loss_dice_8: 1.008    time: 0.3205  last_time: 0.3068  data_time: 0.0032  last_data_time: 0.0033   lr: 6.8997e-05  max_mem: 14733M
[10/03 23:12:31] d2.utils.events INFO:  eta: 0:34:54  iter: 3399  total_loss: 21.48  loss_ce: 0.4588  loss_mask: 0.3264  loss_dice: 1.009  loss_contrastive: 0  loss_ce_0: 1.154  loss_mask_0: 0.4314  loss_dice_0: 1.339  loss_ce_1: 0.857  loss_mask_1: 0.3379  loss_dice_1: 1.236  loss_ce_2: 0.5706  loss_mask_2: 0.3483  loss_dice_2: 1.229  loss_ce_3: 0.5079  loss_mask_3: 0.3191  loss_dice_3: 1.007  loss_ce_4: 0.4393  loss_mask_4: 0.3285  loss_dice_4: 1.088  loss_ce_5: 0.3855  loss_mask_5: 0.353  loss_dice_5: 1.026  loss_ce_6: 0.3901  loss_mask_6: 0.3386  loss_dice_6: 1.104  loss_ce_7: 0.4557  loss_mask_7: 0.3351  loss_dice_7: 1.024  loss_ce_8: 0.4395  loss_mask_8: 0.3327  loss_dice_8: 1.062    time: 0.3205  last_time: 0.3355  data_time: 0.0032  last_data_time: 0.0049   lr: 6.881e-05  max_mem: 14733M
[10/03 23:12:38] d2.utils.events INFO:  eta: 0:34:47  iter: 3419  total_loss: 21.04  loss_ce: 0.3354  loss_mask: 0.4707  loss_dice: 1.184  loss_contrastive: 0  loss_ce_0: 0.7745  loss_mask_0: 0.6505  loss_dice_0: 1.193  loss_ce_1: 0.5023  loss_mask_1: 0.4919  loss_dice_1: 1.155  loss_ce_2: 0.4576  loss_mask_2: 0.5121  loss_dice_2: 1.071  loss_ce_3: 0.3281  loss_mask_3: 0.4787  loss_dice_3: 1.084  loss_ce_4: 0.2958  loss_mask_4: 0.5383  loss_dice_4: 1.125  loss_ce_5: 0.2794  loss_mask_5: 0.5428  loss_dice_5: 1.186  loss_ce_6: 0.2695  loss_mask_6: 0.4948  loss_dice_6: 1.121  loss_ce_7: 0.3455  loss_mask_7: 0.5287  loss_dice_7: 1.088  loss_ce_8: 0.2604  loss_mask_8: 0.5162  loss_dice_8: 1.109    time: 0.3205  last_time: 0.3094  data_time: 0.0034  last_data_time: 0.0030   lr: 6.8622e-05  max_mem: 14733M
[10/03 23:12:44] d2.utils.events INFO:  eta: 0:34:41  iter: 3439  total_loss: 21.72  loss_ce: 0.3731  loss_mask: 0.313  loss_dice: 1.003  loss_contrastive: 0  loss_ce_0: 1.216  loss_mask_0: 0.4176  loss_dice_0: 1.334  loss_ce_1: 0.6641  loss_mask_1: 0.2873  loss_dice_1: 1.158  loss_ce_2: 0.6265  loss_mask_2: 0.263  loss_dice_2: 1.024  loss_ce_3: 0.5842  loss_mask_3: 0.2944  loss_dice_3: 0.9258  loss_ce_4: 0.5554  loss_mask_4: 0.3223  loss_dice_4: 1.006  loss_ce_5: 0.3926  loss_mask_5: 0.2955  loss_dice_5: 0.9868  loss_ce_6: 0.3924  loss_mask_6: 0.2564  loss_dice_6: 0.936  loss_ce_7: 0.3807  loss_mask_7: 0.261  loss_dice_7: 1.002  loss_ce_8: 0.4939  loss_mask_8: 0.265  loss_dice_8: 0.9677    time: 0.3205  last_time: 0.3149  data_time: 0.0033  last_data_time: 0.0030   lr: 6.8434e-05  max_mem: 14733M
[10/03 23:12:50] d2.utils.events INFO:  eta: 0:34:35  iter: 3459  total_loss: 19.33  loss_ce: 0.4586  loss_mask: 0.3071  loss_dice: 1.113  loss_contrastive: 0  loss_ce_0: 0.8926  loss_mask_0: 0.3655  loss_dice_0: 1.37  loss_ce_1: 0.6599  loss_mask_1: 0.3347  loss_dice_1: 1.177  loss_ce_2: 0.626  loss_mask_2: 0.297  loss_dice_2: 1.174  loss_ce_3: 0.5029  loss_mask_3: 0.3417  loss_dice_3: 1.189  loss_ce_4: 0.5268  loss_mask_4: 0.3161  loss_dice_4: 1.132  loss_ce_5: 0.3367  loss_mask_5: 0.3088  loss_dice_5: 1.138  loss_ce_6: 0.3753  loss_mask_6: 0.3125  loss_dice_6: 1.198  loss_ce_7: 0.4241  loss_mask_7: 0.3401  loss_dice_7: 1.109  loss_ce_8: 0.3984  loss_mask_8: 0.3251  loss_dice_8: 1.137    time: 0.3205  last_time: 0.3207  data_time: 0.0038  last_data_time: 0.0120   lr: 6.8246e-05  max_mem: 14733M
[10/03 23:12:57] d2.utils.events INFO:  eta: 0:34:28  iter: 3479  total_loss: 18.15  loss_ce: 0.5053  loss_mask: 0.4113  loss_dice: 0.7321  loss_contrastive: 0  loss_ce_0: 1.005  loss_mask_0: 0.4414  loss_dice_0: 0.8005  loss_ce_1: 0.7416  loss_mask_1: 0.3967  loss_dice_1: 0.738  loss_ce_2: 0.5962  loss_mask_2: 0.436  loss_dice_2: 0.718  loss_ce_3: 0.5111  loss_mask_3: 0.4104  loss_dice_3: 0.6551  loss_ce_4: 0.4948  loss_mask_4: 0.4199  loss_dice_4: 0.7511  loss_ce_5: 0.4499  loss_mask_5: 0.4177  loss_dice_5: 0.7648  loss_ce_6: 0.4277  loss_mask_6: 0.4217  loss_dice_6: 0.7464  loss_ce_7: 0.4985  loss_mask_7: 0.4106  loss_dice_7: 0.6789  loss_ce_8: 0.4  loss_mask_8: 0.4071  loss_dice_8: 0.7521    time: 0.3204  last_time: 0.3212  data_time: 0.0032  last_data_time: 0.0023   lr: 6.8059e-05  max_mem: 14733M
[10/03 23:13:03] d2.utils.events INFO:  eta: 0:34:22  iter: 3499  total_loss: 17.54  loss_ce: 0.1224  loss_mask: 0.2696  loss_dice: 0.8444  loss_contrastive: 0  loss_ce_0: 0.6755  loss_mask_0: 0.3162  loss_dice_0: 1.006  loss_ce_1: 0.3006  loss_mask_1: 0.2816  loss_dice_1: 0.918  loss_ce_2: 0.18  loss_mask_2: 0.3156  loss_dice_2: 0.8725  loss_ce_3: 0.1845  loss_mask_3: 0.2811  loss_dice_3: 0.8301  loss_ce_4: 0.1303  loss_mask_4: 0.2715  loss_dice_4: 0.9116  loss_ce_5: 0.1392  loss_mask_5: 0.2745  loss_dice_5: 0.9037  loss_ce_6: 0.1221  loss_mask_6: 0.2603  loss_dice_6: 0.9248  loss_ce_7: 0.107  loss_mask_7: 0.2743  loss_dice_7: 0.947  loss_ce_8: 0.1112  loss_mask_8: 0.2606  loss_dice_8: 0.8696    time: 0.3204  last_time: 0.3182  data_time: 0.0030  last_data_time: 0.0026   lr: 6.7871e-05  max_mem: 14733M
[10/03 23:13:10] d2.utils.events INFO:  eta: 0:34:16  iter: 3519  total_loss: 21.3  loss_ce: 0.4381  loss_mask: 0.3031  loss_dice: 1.086  loss_contrastive: 0  loss_ce_0: 1.633  loss_mask_0: 0.3202  loss_dice_0: 1.406  loss_ce_1: 0.7787  loss_mask_1: 0.2796  loss_dice_1: 1.184  loss_ce_2: 0.6508  loss_mask_2: 0.3318  loss_dice_2: 1.133  loss_ce_3: 0.5568  loss_mask_3: 0.2382  loss_dice_3: 1.14  loss_ce_4: 0.5106  loss_mask_4: 0.2392  loss_dice_4: 0.9626  loss_ce_5: 0.4622  loss_mask_5: 0.2475  loss_dice_5: 1.084  loss_ce_6: 0.5439  loss_mask_6: 0.2656  loss_dice_6: 1.066  loss_ce_7: 0.455  loss_mask_7: 0.2635  loss_dice_7: 0.9469  loss_ce_8: 0.4138  loss_mask_8: 0.2711  loss_dice_8: 1.197    time: 0.3204  last_time: 0.3101  data_time: 0.0036  last_data_time: 0.0030   lr: 6.7683e-05  max_mem: 14733M
[10/03 23:13:16] d2.utils.events INFO:  eta: 0:34:10  iter: 3539  total_loss: 20.9  loss_ce: 0.3135  loss_mask: 0.3436  loss_dice: 1.013  loss_contrastive: 0  loss_ce_0: 1.308  loss_mask_0: 0.4472  loss_dice_0: 1.428  loss_ce_1: 0.7398  loss_mask_1: 0.402  loss_dice_1: 1.196  loss_ce_2: 0.5956  loss_mask_2: 0.3842  loss_dice_2: 1.24  loss_ce_3: 0.4628  loss_mask_3: 0.3583  loss_dice_3: 1.089  loss_ce_4: 0.3849  loss_mask_4: 0.3549  loss_dice_4: 1.077  loss_ce_5: 0.3673  loss_mask_5: 0.3876  loss_dice_5: 1.078  loss_ce_6: 0.3359  loss_mask_6: 0.373  loss_dice_6: 1.044  loss_ce_7: 0.3358  loss_mask_7: 0.3402  loss_dice_7: 1.108  loss_ce_8: 0.3212  loss_mask_8: 0.3384  loss_dice_8: 1.113    time: 0.3204  last_time: 0.3108  data_time: 0.0036  last_data_time: 0.0028   lr: 6.7495e-05  max_mem: 14733M
[10/03 23:13:22] d2.utils.events INFO:  eta: 0:34:04  iter: 3559  total_loss: 18.41  loss_ce: 0.4499  loss_mask: 0.3366  loss_dice: 0.9299  loss_contrastive: 0  loss_ce_0: 1.16  loss_mask_0: 0.4177  loss_dice_0: 1.213  loss_ce_1: 0.7546  loss_mask_1: 0.3725  loss_dice_1: 1  loss_ce_2: 0.6395  loss_mask_2: 0.3697  loss_dice_2: 1.065  loss_ce_3: 0.5091  loss_mask_3: 0.3434  loss_dice_3: 0.9445  loss_ce_4: 0.5455  loss_mask_4: 0.3254  loss_dice_4: 0.942  loss_ce_5: 0.5459  loss_mask_5: 0.3369  loss_dice_5: 1.047  loss_ce_6: 0.5024  loss_mask_6: 0.3238  loss_dice_6: 0.9704  loss_ce_7: 0.4223  loss_mask_7: 0.3209  loss_dice_7: 1.108  loss_ce_8: 0.4555  loss_mask_8: 0.3438  loss_dice_8: 0.9478    time: 0.3204  last_time: 0.3269  data_time: 0.0034  last_data_time: 0.0033   lr: 6.7307e-05  max_mem: 14733M
[10/03 23:13:29] d2.utils.events INFO:  eta: 0:33:57  iter: 3579  total_loss: 20.61  loss_ce: 0.4239  loss_mask: 0.2255  loss_dice: 1.052  loss_contrastive: 0  loss_ce_0: 1.169  loss_mask_0: 0.3149  loss_dice_0: 1.414  loss_ce_1: 0.7897  loss_mask_1: 0.2486  loss_dice_1: 1.305  loss_ce_2: 0.6349  loss_mask_2: 0.2542  loss_dice_2: 1.159  loss_ce_3: 0.5297  loss_mask_3: 0.2857  loss_dice_3: 1.209  loss_ce_4: 0.5299  loss_mask_4: 0.222  loss_dice_4: 1.135  loss_ce_5: 0.4827  loss_mask_5: 0.2379  loss_dice_5: 1.187  loss_ce_6: 0.4763  loss_mask_6: 0.2232  loss_dice_6: 1.121  loss_ce_7: 0.468  loss_mask_7: 0.2376  loss_dice_7: 1.107  loss_ce_8: 0.4236  loss_mask_8: 0.2314  loss_dice_8: 1.063    time: 0.3204  last_time: 0.3356  data_time: 0.0034  last_data_time: 0.0025   lr: 6.7119e-05  max_mem: 14733M
[10/03 23:13:35] d2.utils.events INFO:  eta: 0:33:50  iter: 3599  total_loss: 21.63  loss_ce: 0.5398  loss_mask: 0.2672  loss_dice: 1.026  loss_contrastive: 0  loss_ce_0: 1.244  loss_mask_0: 0.3888  loss_dice_0: 1.319  loss_ce_1: 0.873  loss_mask_1: 0.3199  loss_dice_1: 1.165  loss_ce_2: 0.7931  loss_mask_2: 0.3201  loss_dice_2: 1.119  loss_ce_3: 0.6405  loss_mask_3: 0.3065  loss_dice_3: 1.053  loss_ce_4: 0.5737  loss_mask_4: 0.3044  loss_dice_4: 1.035  loss_ce_5: 0.6058  loss_mask_5: 0.3016  loss_dice_5: 1.045  loss_ce_6: 0.6087  loss_mask_6: 0.2941  loss_dice_6: 1.036  loss_ce_7: 0.6467  loss_mask_7: 0.3124  loss_dice_7: 0.9886  loss_ce_8: 0.5979  loss_mask_8: 0.3028  loss_dice_8: 0.9854    time: 0.3204  last_time: 0.3176  data_time: 0.0037  last_data_time: 0.0037   lr: 6.693e-05  max_mem: 14733M
[10/03 23:13:42] d2.utils.events INFO:  eta: 0:33:44  iter: 3619  total_loss: 15.16  loss_ce: 0.2472  loss_mask: 0.2791  loss_dice: 0.726  loss_contrastive: 0  loss_ce_0: 1.067  loss_mask_0: 0.3403  loss_dice_0: 0.9478  loss_ce_1: 0.7955  loss_mask_1: 0.2917  loss_dice_1: 0.6929  loss_ce_2: 0.5795  loss_mask_2: 0.2856  loss_dice_2: 0.6806  loss_ce_3: 0.4057  loss_mask_3: 0.2641  loss_dice_3: 0.7051  loss_ce_4: 0.333  loss_mask_4: 0.2792  loss_dice_4: 0.69  loss_ce_5: 0.3623  loss_mask_5: 0.2815  loss_dice_5: 0.7515  loss_ce_6: 0.2597  loss_mask_6: 0.2838  loss_dice_6: 0.7454  loss_ce_7: 0.2686  loss_mask_7: 0.2874  loss_dice_7: 0.7091  loss_ce_8: 0.2719  loss_mask_8: 0.2923  loss_dice_8: 0.7063    time: 0.3204  last_time: 0.2929  data_time: 0.0031  last_data_time: 0.0018   lr: 6.6742e-05  max_mem: 14733M
[10/03 23:13:48] d2.utils.events INFO:  eta: 0:33:37  iter: 3639  total_loss: 19.31  loss_ce: 0.4049  loss_mask: 0.2788  loss_dice: 0.8104  loss_contrastive: 0  loss_ce_0: 1.039  loss_mask_0: 0.3386  loss_dice_0: 1.183  loss_ce_1: 0.6492  loss_mask_1: 0.2826  loss_dice_1: 0.9582  loss_ce_2: 0.5737  loss_mask_2: 0.279  loss_dice_2: 0.9532  loss_ce_3: 0.5611  loss_mask_3: 0.2897  loss_dice_3: 0.8379  loss_ce_4: 0.4446  loss_mask_4: 0.2692  loss_dice_4: 0.9165  loss_ce_5: 0.3644  loss_mask_5: 0.2896  loss_dice_5: 0.9072  loss_ce_6: 0.3879  loss_mask_6: 0.3409  loss_dice_6: 0.8321  loss_ce_7: 0.3831  loss_mask_7: 0.2901  loss_dice_7: 0.8591  loss_ce_8: 0.4503  loss_mask_8: 0.2595  loss_dice_8: 0.7918    time: 0.3204  last_time: 0.3301  data_time: 0.0038  last_data_time: 0.0100   lr: 6.6554e-05  max_mem: 14733M
[10/03 23:13:54] d2.utils.events INFO:  eta: 0:33:31  iter: 3659  total_loss: 20.69  loss_ce: 0.4563  loss_mask: 0.3095  loss_dice: 0.97  loss_contrastive: 0  loss_ce_0: 1.22  loss_mask_0: 0.3733  loss_dice_0: 0.9406  loss_ce_1: 0.7826  loss_mask_1: 0.35  loss_dice_1: 0.929  loss_ce_2: 0.5756  loss_mask_2: 0.3177  loss_dice_2: 0.957  loss_ce_3: 0.5136  loss_mask_3: 0.3143  loss_dice_3: 0.9691  loss_ce_4: 0.4131  loss_mask_4: 0.3426  loss_dice_4: 0.9606  loss_ce_5: 0.4142  loss_mask_5: 0.3249  loss_dice_5: 0.9814  loss_ce_6: 0.3358  loss_mask_6: 0.3155  loss_dice_6: 0.977  loss_ce_7: 0.3876  loss_mask_7: 0.3178  loss_dice_7: 0.9629  loss_ce_8: 0.4397  loss_mask_8: 0.2896  loss_dice_8: 0.9711    time: 0.3204  last_time: 0.3291  data_time: 0.0031  last_data_time: 0.0032   lr: 6.6365e-05  max_mem: 14733M
[10/03 23:14:01] d2.utils.events INFO:  eta: 0:33:24  iter: 3679  total_loss: 20.61  loss_ce: 0.3953  loss_mask: 0.3609  loss_dice: 1.155  loss_contrastive: 0  loss_ce_0: 1.024  loss_mask_0: 0.3517  loss_dice_0: 1.33  loss_ce_1: 0.7363  loss_mask_1: 0.2977  loss_dice_1: 1.233  loss_ce_2: 0.6007  loss_mask_2: 0.2677  loss_dice_2: 1.206  loss_ce_3: 0.4395  loss_mask_3: 0.2675  loss_dice_3: 1.2  loss_ce_4: 0.4082  loss_mask_4: 0.2686  loss_dice_4: 1.161  loss_ce_5: 0.4453  loss_mask_5: 0.2492  loss_dice_5: 1.132  loss_ce_6: 0.4076  loss_mask_6: 0.2683  loss_dice_6: 1.15  loss_ce_7: 0.3714  loss_mask_7: 0.2667  loss_dice_7: 1.097  loss_ce_8: 0.3116  loss_mask_8: 0.2962  loss_dice_8: 1.135    time: 0.3204  last_time: 0.3027  data_time: 0.0034  last_data_time: 0.0033   lr: 6.6177e-05  max_mem: 14733M
[10/03 23:14:07] d2.utils.events INFO:  eta: 0:33:18  iter: 3699  total_loss: 22.3  loss_ce: 0.3911  loss_mask: 0.4272  loss_dice: 1.09  loss_contrastive: 0  loss_ce_0: 1.04  loss_mask_0: 0.4992  loss_dice_0: 1.257  loss_ce_1: 0.5205  loss_mask_1: 0.4448  loss_dice_1: 1.11  loss_ce_2: 0.506  loss_mask_2: 0.3979  loss_dice_2: 1.21  loss_ce_3: 0.5118  loss_mask_3: 0.3917  loss_dice_3: 1.024  loss_ce_4: 0.3568  loss_mask_4: 0.4211  loss_dice_4: 1.028  loss_ce_5: 0.3544  loss_mask_5: 0.4476  loss_dice_5: 1.131  loss_ce_6: 0.3281  loss_mask_6: 0.4124  loss_dice_6: 1.038  loss_ce_7: 0.4088  loss_mask_7: 0.4108  loss_dice_7: 1.055  loss_ce_8: 0.2827  loss_mask_8: 0.4057  loss_dice_8: 1.132    time: 0.3204  last_time: 0.3364  data_time: 0.0031  last_data_time: 0.0036   lr: 6.5989e-05  max_mem: 14733M
[10/03 23:14:14] d2.utils.events INFO:  eta: 0:33:13  iter: 3719  total_loss: 24.13  loss_ce: 0.4485  loss_mask: 0.2264  loss_dice: 1.127  loss_contrastive: 0  loss_ce_0: 1.243  loss_mask_0: 0.2618  loss_dice_0: 1.295  loss_ce_1: 0.8691  loss_mask_1: 0.2271  loss_dice_1: 1.28  loss_ce_2: 0.6171  loss_mask_2: 0.2331  loss_dice_2: 1.257  loss_ce_3: 0.5516  loss_mask_3: 0.1997  loss_dice_3: 1.123  loss_ce_4: 0.4696  loss_mask_4: 0.2007  loss_dice_4: 1.166  loss_ce_5: 0.4922  loss_mask_5: 0.2257  loss_dice_5: 1.209  loss_ce_6: 0.4381  loss_mask_6: 0.206  loss_dice_6: 1.103  loss_ce_7: 0.5741  loss_mask_7: 0.2021  loss_dice_7: 1.15  loss_ce_8: 0.4689  loss_mask_8: 0.2056  loss_dice_8: 1.178    time: 0.3204  last_time: 0.3186  data_time: 0.0036  last_data_time: 0.0046   lr: 6.58e-05  max_mem: 14733M
[10/03 23:14:20] d2.utils.events INFO:  eta: 0:33:06  iter: 3739  total_loss: 21.87  loss_ce: 0.4259  loss_mask: 0.2549  loss_dice: 1.187  loss_contrastive: 0  loss_ce_0: 1.079  loss_mask_0: 0.3084  loss_dice_0: 1.327  loss_ce_1: 0.8283  loss_mask_1: 0.2851  loss_dice_1: 1.234  loss_ce_2: 0.6635  loss_mask_2: 0.2634  loss_dice_2: 1.187  loss_ce_3: 0.5357  loss_mask_3: 0.2622  loss_dice_3: 1.194  loss_ce_4: 0.4956  loss_mask_4: 0.2787  loss_dice_4: 1.204  loss_ce_5: 0.5265  loss_mask_5: 0.267  loss_dice_5: 1.227  loss_ce_6: 0.4944  loss_mask_6: 0.2668  loss_dice_6: 1.173  loss_ce_7: 0.4868  loss_mask_7: 0.2704  loss_dice_7: 1.17  loss_ce_8: 0.5104  loss_mask_8: 0.2624  loss_dice_8: 1.263    time: 0.3204  last_time: 0.3309  data_time: 0.0037  last_data_time: 0.0030   lr: 6.5611e-05  max_mem: 14733M
[10/03 23:14:26] d2.utils.events INFO:  eta: 0:32:59  iter: 3759  total_loss: 20.29  loss_ce: 0.3244  loss_mask: 0.3583  loss_dice: 0.9794  loss_contrastive: 0  loss_ce_0: 0.8819  loss_mask_0: 0.445  loss_dice_0: 1.207  loss_ce_1: 0.6402  loss_mask_1: 0.3269  loss_dice_1: 1.046  loss_ce_2: 0.5171  loss_mask_2: 0.3341  loss_dice_2: 1.078  loss_ce_3: 0.4581  loss_mask_3: 0.3096  loss_dice_3: 1.033  loss_ce_4: 0.3582  loss_mask_4: 0.3514  loss_dice_4: 1.022  loss_ce_5: 0.4645  loss_mask_5: 0.3378  loss_dice_5: 1.024  loss_ce_6: 0.4087  loss_mask_6: 0.3807  loss_dice_6: 1.045  loss_ce_7: 0.4669  loss_mask_7: 0.3922  loss_dice_7: 0.977  loss_ce_8: 0.415  loss_mask_8: 0.3485  loss_dice_8: 0.9986    time: 0.3203  last_time: 0.3400  data_time: 0.0033  last_data_time: 0.0028   lr: 6.5423e-05  max_mem: 14733M
[10/03 23:14:33] d2.utils.events INFO:  eta: 0:32:52  iter: 3779  total_loss: 19.92  loss_ce: 0.357  loss_mask: 0.2965  loss_dice: 0.9149  loss_contrastive: 0  loss_ce_0: 1.01  loss_mask_0: 0.3917  loss_dice_0: 1.107  loss_ce_1: 0.7407  loss_mask_1: 0.3449  loss_dice_1: 0.9961  loss_ce_2: 0.465  loss_mask_2: 0.2869  loss_dice_2: 0.9728  loss_ce_3: 0.4018  loss_mask_3: 0.2784  loss_dice_3: 0.912  loss_ce_4: 0.3497  loss_mask_4: 0.2714  loss_dice_4: 0.9677  loss_ce_5: 0.3391  loss_mask_5: 0.3051  loss_dice_5: 1.024  loss_ce_6: 0.3943  loss_mask_6: 0.2681  loss_dice_6: 0.9993  loss_ce_7: 0.358  loss_mask_7: 0.3004  loss_dice_7: 0.9683  loss_ce_8: 0.3421  loss_mask_8: 0.3058  loss_dice_8: 0.9388    time: 0.3204  last_time: 0.3193  data_time: 0.0033  last_data_time: 0.0028   lr: 6.5234e-05  max_mem: 14733M
[10/03 23:14:39] d2.utils.events INFO:  eta: 0:32:45  iter: 3799  total_loss: 25.3  loss_ce: 0.4391  loss_mask: 0.4597  loss_dice: 1.213  loss_contrastive: 0  loss_ce_0: 1.283  loss_mask_0: 0.4165  loss_dice_0: 1.505  loss_ce_1: 0.7573  loss_mask_1: 0.3595  loss_dice_1: 1.338  loss_ce_2: 0.6823  loss_mask_2: 0.3712  loss_dice_2: 1.233  loss_ce_3: 0.6373  loss_mask_3: 0.4445  loss_dice_3: 1.331  loss_ce_4: 0.6316  loss_mask_4: 0.4003  loss_dice_4: 1.248  loss_ce_5: 0.6049  loss_mask_5: 0.3925  loss_dice_5: 1.202  loss_ce_6: 0.5542  loss_mask_6: 0.4012  loss_dice_6: 1.19  loss_ce_7: 0.4416  loss_mask_7: 0.4377  loss_dice_7: 1.167  loss_ce_8: 0.4796  loss_mask_8: 0.4412  loss_dice_8: 1.273    time: 0.3204  last_time: 0.3023  data_time: 0.0036  last_data_time: 0.0035   lr: 6.5045e-05  max_mem: 14733M
[10/03 23:14:46] d2.utils.events INFO:  eta: 0:32:39  iter: 3819  total_loss: 21.94  loss_ce: 0.4405  loss_mask: 0.4294  loss_dice: 1.102  loss_contrastive: 0  loss_ce_0: 1.122  loss_mask_0: 0.4441  loss_dice_0: 1.348  loss_ce_1: 0.7736  loss_mask_1: 0.4278  loss_dice_1: 1.14  loss_ce_2: 0.6347  loss_mask_2: 0.4283  loss_dice_2: 1.148  loss_ce_3: 0.5899  loss_mask_3: 0.4225  loss_dice_3: 1.041  loss_ce_4: 0.3906  loss_mask_4: 0.3933  loss_dice_4: 1.08  loss_ce_5: 0.4893  loss_mask_5: 0.4095  loss_dice_5: 1.058  loss_ce_6: 0.5325  loss_mask_6: 0.3681  loss_dice_6: 1.063  loss_ce_7: 0.4897  loss_mask_7: 0.3983  loss_dice_7: 1.097  loss_ce_8: 0.5251  loss_mask_8: 0.4164  loss_dice_8: 1.084    time: 0.3204  last_time: 0.3075  data_time: 0.0034  last_data_time: 0.0033   lr: 6.4856e-05  max_mem: 14733M
[10/03 23:14:52] d2.utils.events INFO:  eta: 0:32:32  iter: 3839  total_loss: 22.37  loss_ce: 0.4409  loss_mask: 0.3799  loss_dice: 0.9847  loss_contrastive: 0  loss_ce_0: 1.142  loss_mask_0: 0.3913  loss_dice_0: 1.225  loss_ce_1: 0.6934  loss_mask_1: 0.2967  loss_dice_1: 1.153  loss_ce_2: 0.6006  loss_mask_2: 0.314  loss_dice_2: 0.9768  loss_ce_3: 0.5053  loss_mask_3: 0.3252  loss_dice_3: 0.9849  loss_ce_4: 0.5253  loss_mask_4: 0.3028  loss_dice_4: 1.145  loss_ce_5: 0.4496  loss_mask_5: 0.3822  loss_dice_5: 1.142  loss_ce_6: 0.4714  loss_mask_6: 0.3816  loss_dice_6: 1.062  loss_ce_7: 0.4421  loss_mask_7: 0.4455  loss_dice_7: 0.9975  loss_ce_8: 0.4556  loss_mask_8: 0.3845  loss_dice_8: 1.036    time: 0.3203  last_time: 0.3172  data_time: 0.0031  last_data_time: 0.0032   lr: 6.4668e-05  max_mem: 14733M
[10/03 23:14:58] d2.utils.events INFO:  eta: 0:32:26  iter: 3859  total_loss: 21.94  loss_ce: 0.2936  loss_mask: 0.3026  loss_dice: 0.8951  loss_contrastive: 0  loss_ce_0: 0.9668  loss_mask_0: 0.3496  loss_dice_0: 1.006  loss_ce_1: 0.5779  loss_mask_1: 0.2927  loss_dice_1: 1.043  loss_ce_2: 0.5143  loss_mask_2: 0.2844  loss_dice_2: 0.9837  loss_ce_3: 0.5164  loss_mask_3: 0.2976  loss_dice_3: 0.8921  loss_ce_4: 0.375  loss_mask_4: 0.3472  loss_dice_4: 0.7363  loss_ce_5: 0.4064  loss_mask_5: 0.3106  loss_dice_5: 0.985  loss_ce_6: 0.4103  loss_mask_6: 0.3135  loss_dice_6: 0.8499  loss_ce_7: 0.4023  loss_mask_7: 0.377  loss_dice_7: 0.7305  loss_ce_8: 0.3042  loss_mask_8: 0.4024  loss_dice_8: 0.668    time: 0.3203  last_time: 0.3027  data_time: 0.0032  last_data_time: 0.0027   lr: 6.4479e-05  max_mem: 14733M
[10/03 23:15:05] d2.utils.events INFO:  eta: 0:32:20  iter: 3879  total_loss: 27.5  loss_ce: 0.568  loss_mask: 0.3035  loss_dice: 1.251  loss_contrastive: 0  loss_ce_0: 1.364  loss_mask_0: 0.3404  loss_dice_0: 1.641  loss_ce_1: 0.9902  loss_mask_1: 0.2754  loss_dice_1: 1.456  loss_ce_2: 0.8977  loss_mask_2: 0.309  loss_dice_2: 1.291  loss_ce_3: 0.8525  loss_mask_3: 0.297  loss_dice_3: 1.319  loss_ce_4: 0.6987  loss_mask_4: 0.2919  loss_dice_4: 1.297  loss_ce_5: 0.7236  loss_mask_5: 0.2826  loss_dice_5: 1.292  loss_ce_6: 0.6987  loss_mask_6: 0.3031  loss_dice_6: 1.261  loss_ce_7: 0.6312  loss_mask_7: 0.3106  loss_dice_7: 1.147  loss_ce_8: 0.6382  loss_mask_8: 0.3167  loss_dice_8: 1.182    time: 0.3204  last_time: 0.3104  data_time: 0.0035  last_data_time: 0.0034   lr: 6.429e-05  max_mem: 14733M
[10/03 23:15:12] d2.utils.events INFO:  eta: 0:32:13  iter: 3899  total_loss: 21.21  loss_ce: 0.4431  loss_mask: 0.2432  loss_dice: 1.127  loss_contrastive: 0  loss_ce_0: 1.081  loss_mask_0: 0.2764  loss_dice_0: 1.259  loss_ce_1: 0.8262  loss_mask_1: 0.2198  loss_dice_1: 1.184  loss_ce_2: 0.5474  loss_mask_2: 0.2537  loss_dice_2: 1.087  loss_ce_3: 0.562  loss_mask_3: 0.2305  loss_dice_3: 1.137  loss_ce_4: 0.6229  loss_mask_4: 0.2484  loss_dice_4: 1.152  loss_ce_5: 0.4825  loss_mask_5: 0.228  loss_dice_5: 1.108  loss_ce_6: 0.4921  loss_mask_6: 0.2297  loss_dice_6: 1.104  loss_ce_7: 0.4194  loss_mask_7: 0.2632  loss_dice_7: 1.12  loss_ce_8: 0.5926  loss_mask_8: 0.3253  loss_dice_8: 1.218    time: 0.3204  last_time: 0.3122  data_time: 0.0035  last_data_time: 0.0025   lr: 6.41e-05  max_mem: 14765M
[10/03 23:15:18] d2.utils.events INFO:  eta: 0:32:07  iter: 3919  total_loss: 23.6  loss_ce: 0.646  loss_mask: 0.3441  loss_dice: 1.097  loss_contrastive: 0  loss_ce_0: 1.117  loss_mask_0: 0.4017  loss_dice_0: 1.199  loss_ce_1: 0.8258  loss_mask_1: 0.3408  loss_dice_1: 1.189  loss_ce_2: 0.6689  loss_mask_2: 0.3654  loss_dice_2: 1.149  loss_ce_3: 0.642  loss_mask_3: 0.3465  loss_dice_3: 1.094  loss_ce_4: 0.6602  loss_mask_4: 0.3212  loss_dice_4: 1.1  loss_ce_5: 0.5248  loss_mask_5: 0.3328  loss_dice_5: 1.133  loss_ce_6: 0.5989  loss_mask_6: 0.3548  loss_dice_6: 0.9775  loss_ce_7: 0.4978  loss_mask_7: 0.353  loss_dice_7: 1.126  loss_ce_8: 0.4988  loss_mask_8: 0.3425  loss_dice_8: 1.135    time: 0.3204  last_time: 0.3040  data_time: 0.0035  last_data_time: 0.0041   lr: 6.3911e-05  max_mem: 14765M
[10/03 23:15:24] d2.utils.events INFO:  eta: 0:32:01  iter: 3939  total_loss: 18.42  loss_ce: 0.3958  loss_mask: 0.2683  loss_dice: 0.8767  loss_contrastive: 0  loss_ce_0: 1.109  loss_mask_0: 0.268  loss_dice_0: 1.151  loss_ce_1: 0.7791  loss_mask_1: 0.2211  loss_dice_1: 1.064  loss_ce_2: 0.6485  loss_mask_2: 0.2643  loss_dice_2: 1.045  loss_ce_3: 0.4894  loss_mask_3: 0.2663  loss_dice_3: 0.8518  loss_ce_4: 0.4703  loss_mask_4: 0.2372  loss_dice_4: 0.9907  loss_ce_5: 0.4034  loss_mask_5: 0.248  loss_dice_5: 0.8972  loss_ce_6: 0.348  loss_mask_6: 0.2747  loss_dice_6: 0.937  loss_ce_7: 0.4193  loss_mask_7: 0.2688  loss_dice_7: 0.8558  loss_ce_8: 0.453  loss_mask_8: 0.2649  loss_dice_8: 0.8224    time: 0.3204  last_time: 0.3398  data_time: 0.0032  last_data_time: 0.0035   lr: 6.3722e-05  max_mem: 14765M
[10/03 23:15:31] d2.utils.events INFO:  eta: 0:31:54  iter: 3959  total_loss: 20.67  loss_ce: 0.496  loss_mask: 0.2848  loss_dice: 0.9114  loss_contrastive: 0  loss_ce_0: 1.258  loss_mask_0: 0.2758  loss_dice_0: 1.19  loss_ce_1: 0.8286  loss_mask_1: 0.2756  loss_dice_1: 1.028  loss_ce_2: 0.6408  loss_mask_2: 0.2765  loss_dice_2: 1.024  loss_ce_3: 0.59  loss_mask_3: 0.2907  loss_dice_3: 0.9179  loss_ce_4: 0.4599  loss_mask_4: 0.2597  loss_dice_4: 0.9503  loss_ce_5: 0.479  loss_mask_5: 0.2724  loss_dice_5: 0.9925  loss_ce_6: 0.4422  loss_mask_6: 0.3107  loss_dice_6: 0.9796  loss_ce_7: 0.3975  loss_mask_7: 0.2895  loss_dice_7: 0.9518  loss_ce_8: 0.4729  loss_mask_8: 0.333  loss_dice_8: 0.939    time: 0.3204  last_time: 0.3252  data_time: 0.0038  last_data_time: 0.0029   lr: 6.3533e-05  max_mem: 14765M
[10/03 23:15:37] d2.utils.events INFO:  eta: 0:31:49  iter: 3979  total_loss: 22.54  loss_ce: 0.3572  loss_mask: 0.3333  loss_dice: 0.8759  loss_contrastive: 0  loss_ce_0: 0.8808  loss_mask_0: 0.3477  loss_dice_0: 1.026  loss_ce_1: 0.4641  loss_mask_1: 0.3621  loss_dice_1: 1.137  loss_ce_2: 0.3587  loss_mask_2: 0.328  loss_dice_2: 0.875  loss_ce_3: 0.4965  loss_mask_3: 0.3481  loss_dice_3: 0.7957  loss_ce_4: 0.3104  loss_mask_4: 0.3305  loss_dice_4: 0.8764  loss_ce_5: 0.355  loss_mask_5: 0.3389  loss_dice_5: 0.8915  loss_ce_6: 0.4149  loss_mask_6: 0.3452  loss_dice_6: 0.9535  loss_ce_7: 0.503  loss_mask_7: 0.3399  loss_dice_7: 0.9225  loss_ce_8: 0.3488  loss_mask_8: 0.3277  loss_dice_8: 0.867    time: 0.3204  last_time: 0.3145  data_time: 0.0031  last_data_time: 0.0028   lr: 6.3343e-05  max_mem: 14765M
[10/03 23:15:44] d2.utils.events INFO:  eta: 0:31:43  iter: 3999  total_loss: 18.26  loss_ce: 0.4087  loss_mask: 0.2548  loss_dice: 0.9884  loss_contrastive: 0  loss_ce_0: 1.289  loss_mask_0: 0.3534  loss_dice_0: 1.235  loss_ce_1: 0.8333  loss_mask_1: 0.2594  loss_dice_1: 1.035  loss_ce_2: 0.6071  loss_mask_2: 0.2356  loss_dice_2: 1.005  loss_ce_3: 0.4338  loss_mask_3: 0.2563  loss_dice_3: 1.014  loss_ce_4: 0.5059  loss_mask_4: 0.2339  loss_dice_4: 0.9569  loss_ce_5: 0.4662  loss_mask_5: 0.2562  loss_dice_5: 0.9934  loss_ce_6: 0.3544  loss_mask_6: 0.2771  loss_dice_6: 1.029  loss_ce_7: 0.403  loss_mask_7: 0.2838  loss_dice_7: 1.031  loss_ce_8: 0.3706  loss_mask_8: 0.2482  loss_dice_8: 1.009    time: 0.3204  last_time: 0.3215  data_time: 0.0030  last_data_time: 0.0030   lr: 6.3154e-05  max_mem: 14765M
[10/03 23:15:50] d2.utils.events INFO:  eta: 0:31:36  iter: 4019  total_loss: 25.29  loss_ce: 0.542  loss_mask: 0.2672  loss_dice: 1.173  loss_contrastive: 0  loss_ce_0: 1.171  loss_mask_0: 0.2947  loss_dice_0: 1.394  loss_ce_1: 0.8004  loss_mask_1: 0.2423  loss_dice_1: 1.241  loss_ce_2: 0.7562  loss_mask_2: 0.2212  loss_dice_2: 1.206  loss_ce_3: 0.6323  loss_mask_3: 0.2576  loss_dice_3: 1.178  loss_ce_4: 0.5468  loss_mask_4: 0.2338  loss_dice_4: 1.121  loss_ce_5: 0.5184  loss_mask_5: 0.252  loss_dice_5: 1.112  loss_ce_6: 0.5295  loss_mask_6: 0.2493  loss_dice_6: 1.125  loss_ce_7: 0.5418  loss_mask_7: 0.2567  loss_dice_7: 1.117  loss_ce_8: 0.5062  loss_mask_8: 0.2364  loss_dice_8: 1.182    time: 0.3204  last_time: 0.3036  data_time: 0.0035  last_data_time: 0.0021   lr: 6.2965e-05  max_mem: 14765M
[10/03 23:15:57] d2.utils.events INFO:  eta: 0:31:29  iter: 4039  total_loss: 20.99  loss_ce: 0.3393  loss_mask: 0.3842  loss_dice: 1.023  loss_contrastive: 0  loss_ce_0: 1.228  loss_mask_0: 0.3913  loss_dice_0: 1.357  loss_ce_1: 0.855  loss_mask_1: 0.3559  loss_dice_1: 1.112  loss_ce_2: 0.6418  loss_mask_2: 0.3853  loss_dice_2: 1.028  loss_ce_3: 0.5765  loss_mask_3: 0.3808  loss_dice_3: 1.071  loss_ce_4: 0.4842  loss_mask_4: 0.3549  loss_dice_4: 1.108  loss_ce_5: 0.4665  loss_mask_5: 0.3572  loss_dice_5: 1.093  loss_ce_6: 0.4542  loss_mask_6: 0.4026  loss_dice_6: 1.077  loss_ce_7: 0.379  loss_mask_7: 0.384  loss_dice_7: 1.077  loss_ce_8: 0.4165  loss_mask_8: 0.3864  loss_dice_8: 1.05    time: 0.3204  last_time: 0.3225  data_time: 0.0035  last_data_time: 0.0026   lr: 6.2775e-05  max_mem: 14765M
[10/03 23:16:03] d2.utils.events INFO:  eta: 0:31:23  iter: 4059  total_loss: 20.01  loss_ce: 0.2353  loss_mask: 0.2125  loss_dice: 1.247  loss_contrastive: 0  loss_ce_0: 1.065  loss_mask_0: 0.2814  loss_dice_0: 1.304  loss_ce_1: 0.701  loss_mask_1: 0.2108  loss_dice_1: 1.297  loss_ce_2: 0.5499  loss_mask_2: 0.2248  loss_dice_2: 1.088  loss_ce_3: 0.3004  loss_mask_3: 0.2156  loss_dice_3: 1.094  loss_ce_4: 0.3028  loss_mask_4: 0.2249  loss_dice_4: 1.054  loss_ce_5: 0.4334  loss_mask_5: 0.2219  loss_dice_5: 1.054  loss_ce_6: 0.2635  loss_mask_6: 0.2168  loss_dice_6: 1.097  loss_ce_7: 0.2936  loss_mask_7: 0.2151  loss_dice_7: 1.104  loss_ce_8: 0.2178  loss_mask_8: 0.2144  loss_dice_8: 0.9668    time: 0.3204  last_time: 0.3057  data_time: 0.0036  last_data_time: 0.0027   lr: 6.2585e-05  max_mem: 14765M
[10/03 23:16:09] d2.utils.events INFO:  eta: 0:31:16  iter: 4079  total_loss: 22.13  loss_ce: 0.3607  loss_mask: 0.4199  loss_dice: 1.28  loss_contrastive: 0  loss_ce_0: 1.035  loss_mask_0: 0.4103  loss_dice_0: 1.324  loss_ce_1: 0.7239  loss_mask_1: 0.3372  loss_dice_1: 1.291  loss_ce_2: 0.4143  loss_mask_2: 0.3572  loss_dice_2: 1.336  loss_ce_3: 0.4613  loss_mask_3: 0.3506  loss_dice_3: 1.029  loss_ce_4: 0.326  loss_mask_4: 0.3687  loss_dice_4: 1.276  loss_ce_5: 0.3221  loss_mask_5: 0.3316  loss_dice_5: 1.243  loss_ce_6: 0.3571  loss_mask_6: 0.3581  loss_dice_6: 1.222  loss_ce_7: 0.3041  loss_mask_7: 0.4052  loss_dice_7: 1.205  loss_ce_8: 0.3581  loss_mask_8: 0.3404  loss_dice_8: 1.072    time: 0.3204  last_time: 0.3085  data_time: 0.0035  last_data_time: 0.0028   lr: 6.2396e-05  max_mem: 14765M
[10/03 23:16:16] d2.utils.events INFO:  eta: 0:31:11  iter: 4099  total_loss: 14.99  loss_ce: 0.2922  loss_mask: 0.2672  loss_dice: 0.5852  loss_contrastive: 0  loss_ce_0: 0.7576  loss_mask_0: 0.3086  loss_dice_0: 0.8523  loss_ce_1: 0.4955  loss_mask_1: 0.2625  loss_dice_1: 0.7251  loss_ce_2: 0.4666  loss_mask_2: 0.2641  loss_dice_2: 0.7406  loss_ce_3: 0.4546  loss_mask_3: 0.2685  loss_dice_3: 0.601  loss_ce_4: 0.329  loss_mask_4: 0.2422  loss_dice_4: 0.6156  loss_ce_5: 0.3181  loss_mask_5: 0.2524  loss_dice_5: 0.6249  loss_ce_6: 0.3198  loss_mask_6: 0.2541  loss_dice_6: 0.5669  loss_ce_7: 0.2961  loss_mask_7: 0.2752  loss_dice_7: 0.747  loss_ce_8: 0.3288  loss_mask_8: 0.2754  loss_dice_8: 0.6421    time: 0.3204  last_time: 0.3312  data_time: 0.0033  last_data_time: 0.0067   lr: 6.2206e-05  max_mem: 14765M
[10/03 23:16:22] d2.utils.events INFO:  eta: 0:31:05  iter: 4119  total_loss: 14.97  loss_ce: 0.382  loss_mask: 0.2733  loss_dice: 0.8159  loss_contrastive: 0  loss_ce_0: 0.9609  loss_mask_0: 0.3104  loss_dice_0: 0.8008  loss_ce_1: 0.6322  loss_mask_1: 0.3072  loss_dice_1: 0.7642  loss_ce_2: 0.538  loss_mask_2: 0.2785  loss_dice_2: 0.7873  loss_ce_3: 0.4717  loss_mask_3: 0.2887  loss_dice_3: 0.8396  loss_ce_4: 0.4111  loss_mask_4: 0.2723  loss_dice_4: 0.8839  loss_ce_5: 0.434  loss_mask_5: 0.2612  loss_dice_5: 0.6977  loss_ce_6: 0.3827  loss_mask_6: 0.2724  loss_dice_6: 0.7504  loss_ce_7: 0.3628  loss_mask_7: 0.2785  loss_dice_7: 0.895  loss_ce_8: 0.4116  loss_mask_8: 0.2712  loss_dice_8: 0.7864    time: 0.3204  last_time: 0.3096  data_time: 0.0035  last_data_time: 0.0036   lr: 6.2016e-05  max_mem: 14765M
[10/03 23:16:29] d2.utils.events INFO:  eta: 0:30:59  iter: 4139  total_loss: 23.19  loss_ce: 0.3347  loss_mask: 0.3149  loss_dice: 1.185  loss_contrastive: 0  loss_ce_0: 1.299  loss_mask_0: 0.3947  loss_dice_0: 1.385  loss_ce_1: 0.8362  loss_mask_1: 0.3501  loss_dice_1: 1.294  loss_ce_2: 0.6779  loss_mask_2: 0.3213  loss_dice_2: 1.351  loss_ce_3: 0.4699  loss_mask_3: 0.3295  loss_dice_3: 1.133  loss_ce_4: 0.359  loss_mask_4: 0.341  loss_dice_4: 1.129  loss_ce_5: 0.3476  loss_mask_5: 0.3092  loss_dice_5: 1.27  loss_ce_6: 0.3495  loss_mask_6: 0.3296  loss_dice_6: 1.201  loss_ce_7: 0.3724  loss_mask_7: 0.3115  loss_dice_7: 1.254  loss_ce_8: 0.3535  loss_mask_8: 0.3336  loss_dice_8: 1.241    time: 0.3204  last_time: 0.3213  data_time: 0.0039  last_data_time: 0.0026   lr: 6.1826e-05  max_mem: 14765M
[10/03 23:16:35] d2.utils.events INFO:  eta: 0:30:53  iter: 4159  total_loss: 16.49  loss_ce: 0.365  loss_mask: 0.4121  loss_dice: 0.915  loss_contrastive: 0  loss_ce_0: 0.8709  loss_mask_0: 0.4175  loss_dice_0: 0.9311  loss_ce_1: 0.5823  loss_mask_1: 0.3731  loss_dice_1: 0.8501  loss_ce_2: 0.4674  loss_mask_2: 0.4139  loss_dice_2: 0.8229  loss_ce_3: 0.3719  loss_mask_3: 0.3872  loss_dice_3: 0.8413  loss_ce_4: 0.3456  loss_mask_4: 0.3946  loss_dice_4: 0.8373  loss_ce_5: 0.326  loss_mask_5: 0.4037  loss_dice_5: 0.8127  loss_ce_6: 0.3233  loss_mask_6: 0.404  loss_dice_6: 0.973  loss_ce_7: 0.3475  loss_mask_7: 0.3999  loss_dice_7: 0.7929  loss_ce_8: 0.2776  loss_mask_8: 0.4207  loss_dice_8: 0.8788    time: 0.3204  last_time: 0.3304  data_time: 0.0033  last_data_time: 0.0024   lr: 6.1637e-05  max_mem: 14765M
[10/03 23:16:42] d2.utils.events INFO:  eta: 0:30:46  iter: 4179  total_loss: 23.82  loss_ce: 0.5682  loss_mask: 0.2781  loss_dice: 1.091  loss_contrastive: 0  loss_ce_0: 1.45  loss_mask_0: 0.2609  loss_dice_0: 1.37  loss_ce_1: 0.9314  loss_mask_1: 0.2386  loss_dice_1: 1.209  loss_ce_2: 0.8333  loss_mask_2: 0.2756  loss_dice_2: 1.218  loss_ce_3: 0.6945  loss_mask_3: 0.2797  loss_dice_3: 1.016  loss_ce_4: 0.6435  loss_mask_4: 0.2811  loss_dice_4: 1.184  loss_ce_5: 0.6649  loss_mask_5: 0.2699  loss_dice_5: 1.229  loss_ce_6: 0.6694  loss_mask_6: 0.2831  loss_dice_6: 1.055  loss_ce_7: 0.6021  loss_mask_7: 0.2613  loss_dice_7: 1.096  loss_ce_8: 0.6041  loss_mask_8: 0.2834  loss_dice_8: 1.189    time: 0.3204  last_time: 0.3291  data_time: 0.0039  last_data_time: 0.0035   lr: 6.1447e-05  max_mem: 14765M
[10/03 23:16:48] d2.utils.events INFO:  eta: 0:30:42  iter: 4199  total_loss: 22.89  loss_ce: 0.2888  loss_mask: 0.3302  loss_dice: 1.153  loss_contrastive: 0  loss_ce_0: 0.9077  loss_mask_0: 0.5459  loss_dice_0: 1.28  loss_ce_1: 0.6391  loss_mask_1: 0.3999  loss_dice_1: 1.242  loss_ce_2: 0.7082  loss_mask_2: 0.3088  loss_dice_2: 1.147  loss_ce_3: 0.5779  loss_mask_3: 0.3014  loss_dice_3: 1.008  loss_ce_4: 0.3914  loss_mask_4: 0.2667  loss_dice_4: 1.129  loss_ce_5: 0.3031  loss_mask_5: 0.2965  loss_dice_5: 1.072  loss_ce_6: 0.3906  loss_mask_6: 0.294  loss_dice_6: 1.057  loss_ce_7: 0.2972  loss_mask_7: 0.304  loss_dice_7: 1.115  loss_ce_8: 0.3072  loss_mask_8: 0.3269  loss_dice_8: 1.026    time: 0.3204  last_time: 0.3180  data_time: 0.0034  last_data_time: 0.0051   lr: 6.1257e-05  max_mem: 14765M
[10/03 23:16:55] d2.utils.events INFO:  eta: 0:30:35  iter: 4219  total_loss: 22.67  loss_ce: 0.5195  loss_mask: 0.3015  loss_dice: 1.036  loss_contrastive: 0  loss_ce_0: 1.327  loss_mask_0: 0.3683  loss_dice_0: 1.334  loss_ce_1: 0.8643  loss_mask_1: 0.3308  loss_dice_1: 1.298  loss_ce_2: 0.6284  loss_mask_2: 0.3247  loss_dice_2: 1.143  loss_ce_3: 0.5591  loss_mask_3: 0.3256  loss_dice_3: 1.169  loss_ce_4: 0.5017  loss_mask_4: 0.3172  loss_dice_4: 1.088  loss_ce_5: 0.5296  loss_mask_5: 0.3062  loss_dice_5: 1.169  loss_ce_6: 0.457  loss_mask_6: 0.309  loss_dice_6: 1.052  loss_ce_7: 0.4953  loss_mask_7: 0.2995  loss_dice_7: 1.187  loss_ce_8: 0.4506  loss_mask_8: 0.3168  loss_dice_8: 1.14    time: 0.3204  last_time: 0.3479  data_time: 0.0036  last_data_time: 0.0034   lr: 6.1066e-05  max_mem: 14765M
[10/03 23:17:01] d2.utils.events INFO:  eta: 0:30:30  iter: 4239  total_loss: 20.41  loss_ce: 0.3901  loss_mask: 0.444  loss_dice: 1.056  loss_contrastive: 0  loss_ce_0: 0.9352  loss_mask_0: 0.5013  loss_dice_0: 1.209  loss_ce_1: 0.8127  loss_mask_1: 0.4121  loss_dice_1: 1.048  loss_ce_2: 0.6783  loss_mask_2: 0.4216  loss_dice_2: 1.023  loss_ce_3: 0.579  loss_mask_3: 0.3974  loss_dice_3: 0.9369  loss_ce_4: 0.5316  loss_mask_4: 0.4209  loss_dice_4: 0.9564  loss_ce_5: 0.4834  loss_mask_5: 0.4035  loss_dice_5: 0.956  loss_ce_6: 0.437  loss_mask_6: 0.4379  loss_dice_6: 0.8812  loss_ce_7: 0.4391  loss_mask_7: 0.4433  loss_dice_7: 0.8936  loss_ce_8: 0.4165  loss_mask_8: 0.435  loss_dice_8: 0.9555    time: 0.3205  last_time: 0.3222  data_time: 0.0038  last_data_time: 0.0033   lr: 6.0876e-05  max_mem: 14765M
[10/03 23:17:07] d2.utils.events INFO:  eta: 0:30:23  iter: 4259  total_loss: 19.42  loss_ce: 0.1631  loss_mask: 0.329  loss_dice: 1.116  loss_contrastive: 0  loss_ce_0: 1.024  loss_mask_0: 0.3431  loss_dice_0: 1.193  loss_ce_1: 0.6042  loss_mask_1: 0.3473  loss_dice_1: 1.132  loss_ce_2: 0.5114  loss_mask_2: 0.3489  loss_dice_2: 1.03  loss_ce_3: 0.3546  loss_mask_3: 0.34  loss_dice_3: 1.089  loss_ce_4: 0.2566  loss_mask_4: 0.3259  loss_dice_4: 1.071  loss_ce_5: 0.297  loss_mask_5: 0.3657  loss_dice_5: 1.113  loss_ce_6: 0.1804  loss_mask_6: 0.3461  loss_dice_6: 1.115  loss_ce_7: 0.2003  loss_mask_7: 0.3428  loss_dice_7: 1.142  loss_ce_8: 0.1633  loss_mask_8: 0.3251  loss_dice_8: 1.087    time: 0.3204  last_time: 0.3102  data_time: 0.0032  last_data_time: 0.0032   lr: 6.0686e-05  max_mem: 14765M
[10/03 23:17:14] d2.utils.events INFO:  eta: 0:30:17  iter: 4279  total_loss: 18.34  loss_ce: 0.2874  loss_mask: 0.3246  loss_dice: 0.905  loss_contrastive: 0  loss_ce_0: 0.788  loss_mask_0: 0.3843  loss_dice_0: 1.15  loss_ce_1: 0.4398  loss_mask_1: 0.3654  loss_dice_1: 1.032  loss_ce_2: 0.4317  loss_mask_2: 0.3652  loss_dice_2: 0.9944  loss_ce_3: 0.2681  loss_mask_3: 0.3366  loss_dice_3: 0.9435  loss_ce_4: 0.3001  loss_mask_4: 0.3467  loss_dice_4: 1.012  loss_ce_5: 0.3135  loss_mask_5: 0.3434  loss_dice_5: 0.902  loss_ce_6: 0.2787  loss_mask_6: 0.3565  loss_dice_6: 0.947  loss_ce_7: 0.2637  loss_mask_7: 0.3352  loss_dice_7: 0.9601  loss_ce_8: 0.4123  loss_mask_8: 0.3219  loss_dice_8: 0.8749    time: 0.3204  last_time: 0.3030  data_time: 0.0033  last_data_time: 0.0031   lr: 6.0496e-05  max_mem: 14765M
[10/03 23:17:20] d2.utils.events INFO:  eta: 0:30:12  iter: 4299  total_loss: 18.99  loss_ce: 0.3198  loss_mask: 0.3741  loss_dice: 1.036  loss_contrastive: 0  loss_ce_0: 1.272  loss_mask_0: 0.4664  loss_dice_0: 1.183  loss_ce_1: 0.7371  loss_mask_1: 0.392  loss_dice_1: 1.154  loss_ce_2: 0.5891  loss_mask_2: 0.3756  loss_dice_2: 1.107  loss_ce_3: 0.5811  loss_mask_3: 0.3753  loss_dice_3: 1.074  loss_ce_4: 0.4925  loss_mask_4: 0.3563  loss_dice_4: 1.041  loss_ce_5: 0.4184  loss_mask_5: 0.3689  loss_dice_5: 1.133  loss_ce_6: 0.3799  loss_mask_6: 0.3642  loss_dice_6: 0.9674  loss_ce_7: 0.3549  loss_mask_7: 0.3677  loss_dice_7: 1.018  loss_ce_8: 0.3179  loss_mask_8: 0.367  loss_dice_8: 0.9968    time: 0.3205  last_time: 0.3117  data_time: 0.0033  last_data_time: 0.0036   lr: 6.0305e-05  max_mem: 14765M
[10/03 23:17:27] d2.utils.events INFO:  eta: 0:30:04  iter: 4319  total_loss: 21.04  loss_ce: 0.4213  loss_mask: 0.2792  loss_dice: 1.096  loss_contrastive: 0  loss_ce_0: 0.9707  loss_mask_0: 0.2885  loss_dice_0: 1.321  loss_ce_1: 0.6485  loss_mask_1: 0.2765  loss_dice_1: 1.192  loss_ce_2: 0.6286  loss_mask_2: 0.2928  loss_dice_2: 1.21  loss_ce_3: 0.4415  loss_mask_3: 0.2789  loss_dice_3: 1.233  loss_ce_4: 0.5513  loss_mask_4: 0.2873  loss_dice_4: 1.14  loss_ce_5: 0.4639  loss_mask_5: 0.2764  loss_dice_5: 1.105  loss_ce_6: 0.5386  loss_mask_6: 0.2713  loss_dice_6: 1.11  loss_ce_7: 0.4906  loss_mask_7: 0.2737  loss_dice_7: 1.081  loss_ce_8: 0.4353  loss_mask_8: 0.2626  loss_dice_8: 1.156    time: 0.3204  last_time: 0.3028  data_time: 0.0035  last_data_time: 0.0019   lr: 6.0115e-05  max_mem: 14765M
[10/03 23:17:33] d2.utils.events INFO:  eta: 0:29:57  iter: 4339  total_loss: 20.49  loss_ce: 0.3687  loss_mask: 0.3795  loss_dice: 0.8982  loss_contrastive: 0  loss_ce_0: 1.318  loss_mask_0: 0.5876  loss_dice_0: 1.364  loss_ce_1: 0.7079  loss_mask_1: 0.4515  loss_dice_1: 1.025  loss_ce_2: 0.5755  loss_mask_2: 0.4592  loss_dice_2: 0.9702  loss_ce_3: 0.5735  loss_mask_3: 0.4395  loss_dice_3: 0.9847  loss_ce_4: 0.414  loss_mask_4: 0.431  loss_dice_4: 0.9646  loss_ce_5: 0.4302  loss_mask_5: 0.424  loss_dice_5: 0.8984  loss_ce_6: 0.4507  loss_mask_6: 0.4176  loss_dice_6: 0.951  loss_ce_7: 0.4172  loss_mask_7: 0.4082  loss_dice_7: 0.8413  loss_ce_8: 0.3571  loss_mask_8: 0.4056  loss_dice_8: 0.9829    time: 0.3204  last_time: 0.3233  data_time: 0.0035  last_data_time: 0.0052   lr: 5.9924e-05  max_mem: 14765M
[10/03 23:17:39] d2.utils.events INFO:  eta: 0:29:51  iter: 4359  total_loss: 21.71  loss_ce: 0.4657  loss_mask: 0.174  loss_dice: 1.201  loss_contrastive: 0  loss_ce_0: 1.182  loss_mask_0: 0.2651  loss_dice_0: 1.427  loss_ce_1: 0.7966  loss_mask_1: 0.2846  loss_dice_1: 1.271  loss_ce_2: 0.6229  loss_mask_2: 0.2236  loss_dice_2: 1.252  loss_ce_3: 0.4868  loss_mask_3: 0.2167  loss_dice_3: 1.174  loss_ce_4: 0.5077  loss_mask_4: 0.194  loss_dice_4: 1.225  loss_ce_5: 0.479  loss_mask_5: 0.174  loss_dice_5: 1.159  loss_ce_6: 0.4412  loss_mask_6: 0.1721  loss_dice_6: 1.166  loss_ce_7: 0.5332  loss_mask_7: 0.1682  loss_dice_7: 1.197  loss_ce_8: 0.3888  loss_mask_8: 0.1728  loss_dice_8: 1.127    time: 0.3204  last_time: 0.3122  data_time: 0.0035  last_data_time: 0.0025   lr: 5.9734e-05  max_mem: 14765M
[10/03 23:17:46] d2.utils.events INFO:  eta: 0:29:43  iter: 4379  total_loss: 14.95  loss_ce: 0.3629  loss_mask: 0.311  loss_dice: 0.7721  loss_contrastive: 0  loss_ce_0: 0.9042  loss_mask_0: 0.4724  loss_dice_0: 1.051  loss_ce_1: 0.6919  loss_mask_1: 0.3497  loss_dice_1: 0.8981  loss_ce_2: 0.6093  loss_mask_2: 0.3295  loss_dice_2: 0.885  loss_ce_3: 0.3788  loss_mask_3: 0.3184  loss_dice_3: 0.7437  loss_ce_4: 0.2852  loss_mask_4: 0.298  loss_dice_4: 0.8209  loss_ce_5: 0.2983  loss_mask_5: 0.3134  loss_dice_5: 0.779  loss_ce_6: 0.2464  loss_mask_6: 0.3231  loss_dice_6: 0.7853  loss_ce_7: 0.288  loss_mask_7: 0.3111  loss_dice_7: 0.7809  loss_ce_8: 0.2939  loss_mask_8: 0.2955  loss_dice_8: 0.8703    time: 0.3204  last_time: 0.3122  data_time: 0.0033  last_data_time: 0.0034   lr: 5.9543e-05  max_mem: 14765M
[10/03 23:17:52] d2.utils.events INFO:  eta: 0:29:37  iter: 4399  total_loss: 19.34  loss_ce: 0.4194  loss_mask: 0.2641  loss_dice: 1.045  loss_contrastive: 0  loss_ce_0: 0.9319  loss_mask_0: 0.3434  loss_dice_0: 1.327  loss_ce_1: 0.646  loss_mask_1: 0.301  loss_dice_1: 0.9716  loss_ce_2: 0.592  loss_mask_2: 0.2804  loss_dice_2: 1.025  loss_ce_3: 0.4834  loss_mask_3: 0.3178  loss_dice_3: 1.034  loss_ce_4: 0.4169  loss_mask_4: 0.2829  loss_dice_4: 1.059  loss_ce_5: 0.3341  loss_mask_5: 0.2721  loss_dice_5: 1.026  loss_ce_6: 0.3949  loss_mask_6: 0.257  loss_dice_6: 1.028  loss_ce_7: 0.3963  loss_mask_7: 0.2709  loss_dice_7: 1.039  loss_ce_8: 0.3831  loss_mask_8: 0.2542  loss_dice_8: 1.032    time: 0.3204  last_time: 0.3197  data_time: 0.0031  last_data_time: 0.0027   lr: 5.9352e-05  max_mem: 14765M
[10/03 23:17:59] d2.utils.events INFO:  eta: 0:29:30  iter: 4419  total_loss: 18.06  loss_ce: 0.1428  loss_mask: 0.2648  loss_dice: 0.8371  loss_contrastive: 0  loss_ce_0: 0.7768  loss_mask_0: 0.3565  loss_dice_0: 1.039  loss_ce_1: 0.4884  loss_mask_1: 0.2607  loss_dice_1: 0.8101  loss_ce_2: 0.2849  loss_mask_2: 0.2607  loss_dice_2: 0.8854  loss_ce_3: 0.3259  loss_mask_3: 0.2209  loss_dice_3: 0.7992  loss_ce_4: 0.3369  loss_mask_4: 0.2628  loss_dice_4: 0.8881  loss_ce_5: 0.2336  loss_mask_5: 0.2518  loss_dice_5: 0.8653  loss_ce_6: 0.1374  loss_mask_6: 0.2133  loss_dice_6: 0.7711  loss_ce_7: 0.2411  loss_mask_7: 0.2887  loss_dice_7: 0.7484  loss_ce_8: 0.136  loss_mask_8: 0.2476  loss_dice_8: 0.8778    time: 0.3204  last_time: 0.3120  data_time: 0.0031  last_data_time: 0.0030   lr: 5.9162e-05  max_mem: 14765M
[10/03 23:18:05] d2.utils.events INFO:  eta: 0:29:24  iter: 4439  total_loss: 21.64  loss_ce: 0.3335  loss_mask: 0.3249  loss_dice: 1.098  loss_contrastive: 0  loss_ce_0: 0.8993  loss_mask_0: 0.4114  loss_dice_0: 1.225  loss_ce_1: 0.5963  loss_mask_1: 0.3173  loss_dice_1: 1.151  loss_ce_2: 0.5018  loss_mask_2: 0.3381  loss_dice_2: 1.201  loss_ce_3: 0.4312  loss_mask_3: 0.2749  loss_dice_3: 0.996  loss_ce_4: 0.4073  loss_mask_4: 0.3343  loss_dice_4: 1.095  loss_ce_5: 0.4073  loss_mask_5: 0.3505  loss_dice_5: 1.134  loss_ce_6: 0.3861  loss_mask_6: 0.3858  loss_dice_6: 1.086  loss_ce_7: 0.2534  loss_mask_7: 0.3637  loss_dice_7: 1.051  loss_ce_8: 0.3554  loss_mask_8: 0.3124  loss_dice_8: 1.132    time: 0.3204  last_time: 0.3235  data_time: 0.0031  last_data_time: 0.0046   lr: 5.8971e-05  max_mem: 14765M
[10/03 23:18:11] d2.utils.events INFO:  eta: 0:29:17  iter: 4459  total_loss: 19.01  loss_ce: 0.2516  loss_mask: 0.2861  loss_dice: 0.8944  loss_contrastive: 0  loss_ce_0: 1.089  loss_mask_0: 0.4173  loss_dice_0: 1.086  loss_ce_1: 0.7091  loss_mask_1: 0.3484  loss_dice_1: 1.03  loss_ce_2: 0.5315  loss_mask_2: 0.3408  loss_dice_2: 0.9056  loss_ce_3: 0.5205  loss_mask_3: 0.3094  loss_dice_3: 0.8544  loss_ce_4: 0.4798  loss_mask_4: 0.3304  loss_dice_4: 0.9346  loss_ce_5: 0.3544  loss_mask_5: 0.3017  loss_dice_5: 0.9246  loss_ce_6: 0.304  loss_mask_6: 0.324  loss_dice_6: 0.8742  loss_ce_7: 0.3057  loss_mask_7: 0.3132  loss_dice_7: 0.8726  loss_ce_8: 0.2939  loss_mask_8: 0.3311  loss_dice_8: 0.894    time: 0.3204  last_time: 0.3014  data_time: 0.0035  last_data_time: 0.0033   lr: 5.878e-05  max_mem: 14765M
[10/03 23:18:18] d2.utils.events INFO:  eta: 0:29:13  iter: 4479  total_loss: 24.81  loss_ce: 0.4954  loss_mask: 0.6516  loss_dice: 1.125  loss_contrastive: 0  loss_ce_0: 1.144  loss_mask_0: 0.5823  loss_dice_0: 1.354  loss_ce_1: 0.8648  loss_mask_1: 0.7047  loss_dice_1: 1.249  loss_ce_2: 0.6854  loss_mask_2: 0.6251  loss_dice_2: 1.372  loss_ce_3: 0.6859  loss_mask_3: 0.6874  loss_dice_3: 1.264  loss_ce_4: 0.6532  loss_mask_4: 0.669  loss_dice_4: 1.288  loss_ce_5: 0.5775  loss_mask_5: 0.5558  loss_dice_5: 1.191  loss_ce_6: 0.5376  loss_mask_6: 0.4979  loss_dice_6: 1.081  loss_ce_7: 0.525  loss_mask_7: 0.6034  loss_dice_7: 1.072  loss_ce_8: 0.5086  loss_mask_8: 0.6235  loss_dice_8: 1.227    time: 0.3204  last_time: 0.3056  data_time: 0.0035  last_data_time: 0.0032   lr: 5.8589e-05  max_mem: 14765M
[10/03 23:18:24] d2.utils.events INFO:  eta: 0:29:05  iter: 4499  total_loss: 20.6  loss_ce: 0.3068  loss_mask: 0.3847  loss_dice: 1.148  loss_contrastive: 0  loss_ce_0: 0.8645  loss_mask_0: 0.467  loss_dice_0: 1.242  loss_ce_1: 0.6416  loss_mask_1: 0.3938  loss_dice_1: 1.197  loss_ce_2: 0.4465  loss_mask_2: 0.4307  loss_dice_2: 1.145  loss_ce_3: 0.3642  loss_mask_3: 0.3905  loss_dice_3: 1.135  loss_ce_4: 0.3255  loss_mask_4: 0.3952  loss_dice_4: 1.134  loss_ce_5: 0.2177  loss_mask_5: 0.4444  loss_dice_5: 1.124  loss_ce_6: 0.4185  loss_mask_6: 0.3599  loss_dice_6: 1.105  loss_ce_7: 0.258  loss_mask_7: 0.3868  loss_dice_7: 1.181  loss_ce_8: 0.2678  loss_mask_8: 0.3822  loss_dice_8: 1.129    time: 0.3204  last_time: 0.3419  data_time: 0.0031  last_data_time: 0.0029   lr: 5.8398e-05  max_mem: 14765M
[10/03 23:18:31] d2.utils.events INFO:  eta: 0:28:59  iter: 4519  total_loss: 19.79  loss_ce: 0.2244  loss_mask: 0.4032  loss_dice: 1.001  loss_contrastive: 0  loss_ce_0: 0.9341  loss_mask_0: 0.4388  loss_dice_0: 1.123  loss_ce_1: 0.559  loss_mask_1: 0.4091  loss_dice_1: 0.9889  loss_ce_2: 0.4376  loss_mask_2: 0.4042  loss_dice_2: 1.072  loss_ce_3: 0.2293  loss_mask_3: 0.3849  loss_dice_3: 1.041  loss_ce_4: 0.3673  loss_mask_4: 0.4125  loss_dice_4: 1.01  loss_ce_5: 0.34  loss_mask_5: 0.4096  loss_dice_5: 1.017  loss_ce_6: 0.2666  loss_mask_6: 0.4091  loss_dice_6: 1.051  loss_ce_7: 0.2278  loss_mask_7: 0.4119  loss_dice_7: 1.022  loss_ce_8: 0.236  loss_mask_8: 0.4123  loss_dice_8: 1.021    time: 0.3204  last_time: 0.3223  data_time: 0.0035  last_data_time: 0.0028   lr: 5.8207e-05  max_mem: 14765M
[10/03 23:18:37] d2.utils.events INFO:  eta: 0:28:53  iter: 4539  total_loss: 24.89  loss_ce: 0.5616  loss_mask: 0.2985  loss_dice: 1.171  loss_contrastive: 0  loss_ce_0: 1.115  loss_mask_0: 0.3637  loss_dice_0: 1.34  loss_ce_1: 0.942  loss_mask_1: 0.2901  loss_dice_1: 1.239  loss_ce_2: 0.8265  loss_mask_2: 0.2984  loss_dice_2: 1.233  loss_ce_3: 0.6626  loss_mask_3: 0.286  loss_dice_3: 1.176  loss_ce_4: 0.4763  loss_mask_4: 0.348  loss_dice_4: 1.228  loss_ce_5: 0.5568  loss_mask_5: 0.339  loss_dice_5: 1.297  loss_ce_6: 0.5902  loss_mask_6: 0.3848  loss_dice_6: 1.219  loss_ce_7: 0.5556  loss_mask_7: 0.333  loss_dice_7: 1.129  loss_ce_8: 0.5014  loss_mask_8: 0.313  loss_dice_8: 1.148    time: 0.3204  last_time: 0.3201  data_time: 0.0032  last_data_time: 0.0031   lr: 5.8016e-05  max_mem: 14765M
[10/03 23:18:43] d2.utils.events INFO:  eta: 0:28:46  iter: 4559  total_loss: 21.07  loss_ce: 0.4181  loss_mask: 0.2094  loss_dice: 1.069  loss_contrastive: 0  loss_ce_0: 1.206  loss_mask_0: 0.2869  loss_dice_0: 1.276  loss_ce_1: 0.7746  loss_mask_1: 0.195  loss_dice_1: 1.041  loss_ce_2: 0.8439  loss_mask_2: 0.2076  loss_dice_2: 1.065  loss_ce_3: 0.6581  loss_mask_3: 0.2183  loss_dice_3: 1.102  loss_ce_4: 0.5743  loss_mask_4: 0.2268  loss_dice_4: 1.073  loss_ce_5: 0.548  loss_mask_5: 0.2184  loss_dice_5: 1.143  loss_ce_6: 0.4491  loss_mask_6: 0.2192  loss_dice_6: 0.9477  loss_ce_7: 0.5101  loss_mask_7: 0.2203  loss_dice_7: 1.097  loss_ce_8: 0.4576  loss_mask_8: 0.2036  loss_dice_8: 0.9972    time: 0.3204  last_time: 0.3036  data_time: 0.0033  last_data_time: 0.0028   lr: 5.7824e-05  max_mem: 14765M
[10/03 23:18:50] d2.utils.events INFO:  eta: 0:28:41  iter: 4579  total_loss: 20.4  loss_ce: 0.358  loss_mask: 0.2393  loss_dice: 1.084  loss_contrastive: 0  loss_ce_0: 0.9768  loss_mask_0: 0.3073  loss_dice_0: 1.34  loss_ce_1: 0.7549  loss_mask_1: 0.2658  loss_dice_1: 1.083  loss_ce_2: 0.63  loss_mask_2: 0.2476  loss_dice_2: 0.987  loss_ce_3: 0.5735  loss_mask_3: 0.2644  loss_dice_3: 0.9657  loss_ce_4: 0.4923  loss_mask_4: 0.2241  loss_dice_4: 0.9762  loss_ce_5: 0.5128  loss_mask_5: 0.236  loss_dice_5: 1.005  loss_ce_6: 0.3789  loss_mask_6: 0.2245  loss_dice_6: 1.037  loss_ce_7: 0.4295  loss_mask_7: 0.2385  loss_dice_7: 0.9905  loss_ce_8: 0.5634  loss_mask_8: 0.2279  loss_dice_8: 0.9204    time: 0.3204  last_time: 0.3215  data_time: 0.0034  last_data_time: 0.0024   lr: 5.7633e-05  max_mem: 14765M
[10/03 23:18:56] d2.utils.events INFO:  eta: 0:28:35  iter: 4599  total_loss: 20.64  loss_ce: 0.3511  loss_mask: 0.3148  loss_dice: 1.051  loss_contrastive: 0  loss_ce_0: 1.301  loss_mask_0: 0.3254  loss_dice_0: 1.248  loss_ce_1: 0.7049  loss_mask_1: 0.3258  loss_dice_1: 1.133  loss_ce_2: 0.5435  loss_mask_2: 0.3078  loss_dice_2: 1.131  loss_ce_3: 0.4613  loss_mask_3: 0.2601  loss_dice_3: 1.018  loss_ce_4: 0.4014  loss_mask_4: 0.2872  loss_dice_4: 1.091  loss_ce_5: 0.4054  loss_mask_5: 0.3287  loss_dice_5: 1.07  loss_ce_6: 0.3611  loss_mask_6: 0.3257  loss_dice_6: 1.063  loss_ce_7: 0.3492  loss_mask_7: 0.3153  loss_dice_7: 1.04  loss_ce_8: 0.3649  loss_mask_8: 0.3343  loss_dice_8: 1.07    time: 0.3204  last_time: 0.3353  data_time: 0.0042  last_data_time: 0.0057   lr: 5.7442e-05  max_mem: 14765M
[10/03 23:19:03] d2.utils.events INFO:  eta: 0:28:29  iter: 4619  total_loss: 18.56  loss_ce: 0.2763  loss_mask: 0.2102  loss_dice: 1.014  loss_contrastive: 0  loss_ce_0: 1.099  loss_mask_0: 0.2643  loss_dice_0: 1.401  loss_ce_1: 0.7191  loss_mask_1: 0.2076  loss_dice_1: 1.202  loss_ce_2: 0.5779  loss_mask_2: 0.2185  loss_dice_2: 1.063  loss_ce_3: 0.3968  loss_mask_3: 0.2175  loss_dice_3: 1.064  loss_ce_4: 0.2984  loss_mask_4: 0.2142  loss_dice_4: 1.051  loss_ce_5: 0.3091  loss_mask_5: 0.1927  loss_dice_5: 1.004  loss_ce_6: 0.2651  loss_mask_6: 0.1899  loss_dice_6: 1.007  loss_ce_7: 0.2485  loss_mask_7: 0.1899  loss_dice_7: 0.9882  loss_ce_8: 0.3048  loss_mask_8: 0.1911  loss_dice_8: 1.001    time: 0.3204  last_time: 0.3120  data_time: 0.0032  last_data_time: 0.0024   lr: 5.725e-05  max_mem: 14765M
[10/03 23:19:09] d2.utils.events INFO:  eta: 0:28:23  iter: 4639  total_loss: 20.03  loss_ce: 0.4053  loss_mask: 0.4029  loss_dice: 1.037  loss_contrastive: 0  loss_ce_0: 1.097  loss_mask_0: 0.4469  loss_dice_0: 1.214  loss_ce_1: 0.6483  loss_mask_1: 0.3849  loss_dice_1: 1.12  loss_ce_2: 0.4341  loss_mask_2: 0.4212  loss_dice_2: 1.142  loss_ce_3: 0.4254  loss_mask_3: 0.4233  loss_dice_3: 1.075  loss_ce_4: 0.4062  loss_mask_4: 0.337  loss_dice_4: 1.014  loss_ce_5: 0.4569  loss_mask_5: 0.3789  loss_dice_5: 1.089  loss_ce_6: 0.4242  loss_mask_6: 0.4178  loss_dice_6: 1.095  loss_ce_7: 0.3759  loss_mask_7: 0.417  loss_dice_7: 1.042  loss_ce_8: 0.408  loss_mask_8: 0.5265  loss_dice_8: 1.011    time: 0.3203  last_time: 0.3233  data_time: 0.0035  last_data_time: 0.0038   lr: 5.7059e-05  max_mem: 14765M
[10/03 23:19:15] d2.utils.events INFO:  eta: 0:28:16  iter: 4659  total_loss: 19.29  loss_ce: 0.4354  loss_mask: 0.1839  loss_dice: 0.9814  loss_contrastive: 0  loss_ce_0: 1.151  loss_mask_0: 0.2546  loss_dice_0: 1.159  loss_ce_1: 0.7773  loss_mask_1: 0.1858  loss_dice_1: 1.05  loss_ce_2: 0.5226  loss_mask_2: 0.1971  loss_dice_2: 1.01  loss_ce_3: 0.5305  loss_mask_3: 0.1989  loss_dice_3: 0.9628  loss_ce_4: 0.5016  loss_mask_4: 0.1962  loss_dice_4: 0.9396  loss_ce_5: 0.4219  loss_mask_5: 0.1895  loss_dice_5: 0.9356  loss_ce_6: 0.4721  loss_mask_6: 0.1821  loss_dice_6: 0.9849  loss_ce_7: 0.33  loss_mask_7: 0.1789  loss_dice_7: 0.9932  loss_ce_8: 0.4652  loss_mask_8: 0.1783  loss_dice_8: 1.038    time: 0.3203  last_time: 0.3078  data_time: 0.0036  last_data_time: 0.0057   lr: 5.6867e-05  max_mem: 14765M
[10/03 23:19:22] d2.utils.events INFO:  eta: 0:28:10  iter: 4679  total_loss: 19.44  loss_ce: 0.2777  loss_mask: 0.4111  loss_dice: 0.9984  loss_contrastive: 0  loss_ce_0: 0.8434  loss_mask_0: 0.4623  loss_dice_0: 1.242  loss_ce_1: 0.6545  loss_mask_1: 0.4154  loss_dice_1: 1.028  loss_ce_2: 0.4959  loss_mask_2: 0.3797  loss_dice_2: 1.01  loss_ce_3: 0.3993  loss_mask_3: 0.374  loss_dice_3: 1.06  loss_ce_4: 0.3071  loss_mask_4: 0.3811  loss_dice_4: 0.9273  loss_ce_5: 0.267  loss_mask_5: 0.3881  loss_dice_5: 0.9842  loss_ce_6: 0.2463  loss_mask_6: 0.4039  loss_dice_6: 0.8982  loss_ce_7: 0.3148  loss_mask_7: 0.4072  loss_dice_7: 0.9352  loss_ce_8: 0.311  loss_mask_8: 0.4199  loss_dice_8: 0.9267    time: 0.3203  last_time: 0.3393  data_time: 0.0032  last_data_time: 0.0029   lr: 5.6675e-05  max_mem: 14765M
[10/03 23:19:28] d2.utils.events INFO:  eta: 0:28:03  iter: 4699  total_loss: 20.22  loss_ce: 0.4697  loss_mask: 0.2973  loss_dice: 1.046  loss_contrastive: 0  loss_ce_0: 1.133  loss_mask_0: 0.33  loss_dice_0: 1.218  loss_ce_1: 0.7093  loss_mask_1: 0.3285  loss_dice_1: 1.057  loss_ce_2: 0.5925  loss_mask_2: 0.3112  loss_dice_2: 1.064  loss_ce_3: 0.5005  loss_mask_3: 0.2957  loss_dice_3: 1.012  loss_ce_4: 0.409  loss_mask_4: 0.288  loss_dice_4: 1.019  loss_ce_5: 0.4159  loss_mask_5: 0.2798  loss_dice_5: 1.083  loss_ce_6: 0.4961  loss_mask_6: 0.2926  loss_dice_6: 1.016  loss_ce_7: 0.4919  loss_mask_7: 0.2809  loss_dice_7: 0.9893  loss_ce_8: 0.519  loss_mask_8: 0.2802  loss_dice_8: 1.026    time: 0.3203  last_time: 0.3100  data_time: 0.0036  last_data_time: 0.0031   lr: 5.6484e-05  max_mem: 14765M
[10/03 23:19:35] d2.utils.events INFO:  eta: 0:27:57  iter: 4719  total_loss: 20.02  loss_ce: 0.4217  loss_mask: 0.2997  loss_dice: 1.156  loss_contrastive: 0  loss_ce_0: 1.156  loss_mask_0: 0.3147  loss_dice_0: 1.51  loss_ce_1: 0.8409  loss_mask_1: 0.312  loss_dice_1: 1.155  loss_ce_2: 0.6735  loss_mask_2: 0.2905  loss_dice_2: 1.185  loss_ce_3: 0.4654  loss_mask_3: 0.249  loss_dice_3: 1.224  loss_ce_4: 0.512  loss_mask_4: 0.2872  loss_dice_4: 1.03  loss_ce_5: 0.3299  loss_mask_5: 0.3062  loss_dice_5: 1.198  loss_ce_6: 0.3991  loss_mask_6: 0.3029  loss_dice_6: 0.9772  loss_ce_7: 0.2938  loss_mask_7: 0.3368  loss_dice_7: 1.093  loss_ce_8: 0.4229  loss_mask_8: 0.2965  loss_dice_8: 1.07    time: 0.3203  last_time: 0.3077  data_time: 0.0033  last_data_time: 0.0030   lr: 5.6292e-05  max_mem: 14765M
[10/03 23:19:41] d2.utils.events INFO:  eta: 0:27:51  iter: 4739  total_loss: 22.8  loss_ce: 0.3815  loss_mask: 0.4107  loss_dice: 1.069  loss_contrastive: 0  loss_ce_0: 1.233  loss_mask_0: 0.4661  loss_dice_0: 1.382  loss_ce_1: 0.842  loss_mask_1: 0.4706  loss_dice_1: 1.157  loss_ce_2: 0.7294  loss_mask_2: 0.4608  loss_dice_2: 1.203  loss_ce_3: 0.4925  loss_mask_3: 0.4509  loss_dice_3: 0.9749  loss_ce_4: 0.4138  loss_mask_4: 0.4054  loss_dice_4: 1.052  loss_ce_5: 0.4482  loss_mask_5: 0.4073  loss_dice_5: 1.034  loss_ce_6: 0.4286  loss_mask_6: 0.4089  loss_dice_6: 0.9515  loss_ce_7: 0.4648  loss_mask_7: 0.4041  loss_dice_7: 0.9135  loss_ce_8: 0.4315  loss_mask_8: 0.3919  loss_dice_8: 0.9765    time: 0.3203  last_time: 0.3104  data_time: 0.0031  last_data_time: 0.0029   lr: 5.61e-05  max_mem: 14765M
[10/03 23:19:47] d2.utils.events INFO:  eta: 0:27:46  iter: 4759  total_loss: 26.63  loss_ce: 0.516  loss_mask: 0.3316  loss_dice: 1.092  loss_contrastive: 0  loss_ce_0: 1.443  loss_mask_0: 0.6188  loss_dice_0: 1.533  loss_ce_1: 0.8602  loss_mask_1: 0.5439  loss_dice_1: 1.4  loss_ce_2: 0.8706  loss_mask_2: 0.4372  loss_dice_2: 1.301  loss_ce_3: 0.7111  loss_mask_3: 0.4224  loss_dice_3: 1.162  loss_ce_4: 0.6527  loss_mask_4: 0.4258  loss_dice_4: 1.266  loss_ce_5: 0.5846  loss_mask_5: 0.3457  loss_dice_5: 1.153  loss_ce_6: 0.5471  loss_mask_6: 0.3445  loss_dice_6: 1.131  loss_ce_7: 0.5302  loss_mask_7: 0.3375  loss_dice_7: 1.229  loss_ce_8: 0.6125  loss_mask_8: 0.3734  loss_dice_8: 1.193    time: 0.3203  last_time: 0.3183  data_time: 0.0034  last_data_time: 0.0029   lr: 5.5908e-05  max_mem: 14765M
[10/03 23:19:54] d2.utils.events INFO:  eta: 0:27:39  iter: 4779  total_loss: 21.59  loss_ce: 0.4282  loss_mask: 0.3507  loss_dice: 0.8701  loss_contrastive: 0  loss_ce_0: 1.06  loss_mask_0: 0.3737  loss_dice_0: 1.287  loss_ce_1: 0.6553  loss_mask_1: 0.3566  loss_dice_1: 1.024  loss_ce_2: 0.7553  loss_mask_2: 0.3467  loss_dice_2: 1.057  loss_ce_3: 0.6228  loss_mask_3: 0.3314  loss_dice_3: 0.9521  loss_ce_4: 0.5532  loss_mask_4: 0.3539  loss_dice_4: 0.8275  loss_ce_5: 0.5659  loss_mask_5: 0.3535  loss_dice_5: 0.8542  loss_ce_6: 0.4878  loss_mask_6: 0.3318  loss_dice_6: 0.8295  loss_ce_7: 0.473  loss_mask_7: 0.3444  loss_dice_7: 0.9211  loss_ce_8: 0.4606  loss_mask_8: 0.358  loss_dice_8: 0.8163    time: 0.3203  last_time: 0.3084  data_time: 0.0033  last_data_time: 0.0025   lr: 5.5716e-05  max_mem: 14765M
[10/03 23:20:00] d2.utils.events INFO:  eta: 0:27:31  iter: 4799  total_loss: 18.2  loss_ce: 0.2076  loss_mask: 0.3324  loss_dice: 0.9806  loss_contrastive: 0  loss_ce_0: 0.91  loss_mask_0: 0.4038  loss_dice_0: 1.301  loss_ce_1: 0.5277  loss_mask_1: 0.3795  loss_dice_1: 0.9518  loss_ce_2: 0.4787  loss_mask_2: 0.3641  loss_dice_2: 1.061  loss_ce_3: 0.2662  loss_mask_3: 0.3379  loss_dice_3: 1.018  loss_ce_4: 0.2659  loss_mask_4: 0.3666  loss_dice_4: 1.063  loss_ce_5: 0.2978  loss_mask_5: 0.3081  loss_dice_5: 1.002  loss_ce_6: 0.2552  loss_mask_6: 0.3238  loss_dice_6: 1.009  loss_ce_7: 0.2588  loss_mask_7: 0.297  loss_dice_7: 1.067  loss_ce_8: 0.2153  loss_mask_8: 0.3158  loss_dice_8: 1.006    time: 0.3203  last_time: 0.3415  data_time: 0.0033  last_data_time: 0.0026   lr: 5.5524e-05  max_mem: 14765M
[10/03 23:20:07] d2.utils.events INFO:  eta: 0:27:25  iter: 4819  total_loss: 22.09  loss_ce: 0.343  loss_mask: 0.3229  loss_dice: 1.099  loss_contrastive: 0  loss_ce_0: 1.184  loss_mask_0: 0.4687  loss_dice_0: 1.493  loss_ce_1: 0.7745  loss_mask_1: 0.3799  loss_dice_1: 1.296  loss_ce_2: 0.5953  loss_mask_2: 0.3282  loss_dice_2: 1.244  loss_ce_3: 0.6067  loss_mask_3: 0.3485  loss_dice_3: 1.111  loss_ce_4: 0.485  loss_mask_4: 0.3683  loss_dice_4: 1.117  loss_ce_5: 0.4364  loss_mask_5: 0.3567  loss_dice_5: 1.171  loss_ce_6: 0.3605  loss_mask_6: 0.3953  loss_dice_6: 1.089  loss_ce_7: 0.3526  loss_mask_7: 0.4051  loss_dice_7: 1.138  loss_ce_8: 0.4015  loss_mask_8: 0.3822  loss_dice_8: 1.04    time: 0.3203  last_time: 0.3142  data_time: 0.0032  last_data_time: 0.0039   lr: 5.5331e-05  max_mem: 14765M
[10/03 23:20:13] d2.utils.events INFO:  eta: 0:27:19  iter: 4839  total_loss: 20.87  loss_ce: 0.3911  loss_mask: 0.455  loss_dice: 1.146  loss_contrastive: 0  loss_ce_0: 1.048  loss_mask_0: 0.4077  loss_dice_0: 1.239  loss_ce_1: 0.5869  loss_mask_1: 0.3889  loss_dice_1: 1.2  loss_ce_2: 0.5205  loss_mask_2: 0.3503  loss_dice_2: 1.205  loss_ce_3: 0.5455  loss_mask_3: 0.3556  loss_dice_3: 1.102  loss_ce_4: 0.495  loss_mask_4: 0.4074  loss_dice_4: 1.127  loss_ce_5: 0.396  loss_mask_5: 0.3856  loss_dice_5: 1.085  loss_ce_6: 0.5167  loss_mask_6: 0.4554  loss_dice_6: 1.128  loss_ce_7: 0.3931  loss_mask_7: 0.4239  loss_dice_7: 1.125  loss_ce_8: 0.5289  loss_mask_8: 0.4038  loss_dice_8: 1.085    time: 0.3203  last_time: 0.3109  data_time: 0.0035  last_data_time: 0.0058   lr: 5.5139e-05  max_mem: 14765M
[10/03 23:20:19] d2.utils.events INFO:  eta: 0:27:12  iter: 4859  total_loss: 22.28  loss_ce: 0.3803  loss_mask: 0.3607  loss_dice: 1.266  loss_contrastive: 0  loss_ce_0: 1.052  loss_mask_0: 0.3878  loss_dice_0: 1.451  loss_ce_1: 0.6664  loss_mask_1: 0.3723  loss_dice_1: 1.182  loss_ce_2: 0.6024  loss_mask_2: 0.3555  loss_dice_2: 1.223  loss_ce_3: 0.4769  loss_mask_3: 0.3647  loss_dice_3: 1.173  loss_ce_4: 0.4474  loss_mask_4: 0.3872  loss_dice_4: 1.265  loss_ce_5: 0.4521  loss_mask_5: 0.363  loss_dice_5: 1.237  loss_ce_6: 0.4453  loss_mask_6: 0.3327  loss_dice_6: 1.321  loss_ce_7: 0.3523  loss_mask_7: 0.3755  loss_dice_7: 1.276  loss_ce_8: 0.3526  loss_mask_8: 0.3644  loss_dice_8: 1.243    time: 0.3203  last_time: 0.3183  data_time: 0.0037  last_data_time: 0.0038   lr: 5.4947e-05  max_mem: 14765M
[10/03 23:20:26] d2.utils.events INFO:  eta: 0:27:05  iter: 4879  total_loss: 17.11  loss_ce: 0.3521  loss_mask: 0.2184  loss_dice: 0.7021  loss_contrastive: 0  loss_ce_0: 0.9785  loss_mask_0: 0.2573  loss_dice_0: 0.9487  loss_ce_1: 0.6674  loss_mask_1: 0.2167  loss_dice_1: 0.7417  loss_ce_2: 0.6039  loss_mask_2: 0.1999  loss_dice_2: 0.7356  loss_ce_3: 0.5244  loss_mask_3: 0.201  loss_dice_3: 0.88  loss_ce_4: 0.4406  loss_mask_4: 0.2413  loss_dice_4: 0.7516  loss_ce_5: 0.4017  loss_mask_5: 0.2393  loss_dice_5: 0.8005  loss_ce_6: 0.386  loss_mask_6: 0.232  loss_dice_6: 0.7165  loss_ce_7: 0.3588  loss_mask_7: 0.2323  loss_dice_7: 0.7659  loss_ce_8: 0.361  loss_mask_8: 0.2418  loss_dice_8: 0.6874    time: 0.3203  last_time: 0.3011  data_time: 0.0029  last_data_time: 0.0033   lr: 5.4754e-05  max_mem: 14765M
[10/03 23:20:32] d2.utils.events INFO:  eta: 0:26:59  iter: 4899  total_loss: 16.71  loss_ce: 0.2186  loss_mask: 0.3534  loss_dice: 0.9385  loss_contrastive: 0  loss_ce_0: 0.9941  loss_mask_0: 0.4641  loss_dice_0: 1.084  loss_ce_1: 0.5371  loss_mask_1: 0.3824  loss_dice_1: 0.8975  loss_ce_2: 0.4603  loss_mask_2: 0.403  loss_dice_2: 0.9126  loss_ce_3: 0.3064  loss_mask_3: 0.4186  loss_dice_3: 0.9161  loss_ce_4: 0.3086  loss_mask_4: 0.3796  loss_dice_4: 0.9594  loss_ce_5: 0.2426  loss_mask_5: 0.3738  loss_dice_5: 0.8887  loss_ce_6: 0.3577  loss_mask_6: 0.3636  loss_dice_6: 1.004  loss_ce_7: 0.2283  loss_mask_7: 0.3922  loss_dice_7: 0.8833  loss_ce_8: 0.219  loss_mask_8: 0.3702  loss_dice_8: 0.9282    time: 0.3203  last_time: 0.3270  data_time: 0.0045  last_data_time: 0.0023   lr: 5.4562e-05  max_mem: 14765M
[10/03 23:20:38] d2.utils.events INFO:  eta: 0:26:53  iter: 4919  total_loss: 15.39  loss_ce: 0.3177  loss_mask: 0.157  loss_dice: 0.9584  loss_contrastive: 0  loss_ce_0: 0.9218  loss_mask_0: 0.225  loss_dice_0: 0.9393  loss_ce_1: 0.633  loss_mask_1: 0.1742  loss_dice_1: 0.9534  loss_ce_2: 0.4633  loss_mask_2: 0.1776  loss_dice_2: 0.815  loss_ce_3: 0.3545  loss_mask_3: 0.1836  loss_dice_3: 0.8865  loss_ce_4: 0.2582  loss_mask_4: 0.1907  loss_dice_4: 0.8484  loss_ce_5: 0.2675  loss_mask_5: 0.1748  loss_dice_5: 0.9214  loss_ce_6: 0.2985  loss_mask_6: 0.1735  loss_dice_6: 0.851  loss_ce_7: 0.3071  loss_mask_7: 0.1804  loss_dice_7: 0.8848  loss_ce_8: 0.3018  loss_mask_8: 0.1759  loss_dice_8: 0.8922    time: 0.3202  last_time: 0.3351  data_time: 0.0031  last_data_time: 0.0030   lr: 5.4369e-05  max_mem: 14765M
[10/03 23:20:45] d2.utils.events INFO:  eta: 0:26:46  iter: 4939  total_loss: 24.42  loss_ce: 0.4246  loss_mask: 0.4112  loss_dice: 1.109  loss_contrastive: 0  loss_ce_0: 1.488  loss_mask_0: 0.5496  loss_dice_0: 1.754  loss_ce_1: 0.8305  loss_mask_1: 0.4095  loss_dice_1: 1.396  loss_ce_2: 0.6992  loss_mask_2: 0.4145  loss_dice_2: 1.4  loss_ce_3: 0.6149  loss_mask_3: 0.4302  loss_dice_3: 1.224  loss_ce_4: 0.7041  loss_mask_4: 0.4208  loss_dice_4: 1.33  loss_ce_5: 0.562  loss_mask_5: 0.4155  loss_dice_5: 1.196  loss_ce_6: 0.4371  loss_mask_6: 0.4054  loss_dice_6: 1.116  loss_ce_7: 0.4497  loss_mask_7: 0.3904  loss_dice_7: 1.169  loss_ce_8: 0.4299  loss_mask_8: 0.3974  loss_dice_8: 1.074    time: 0.3202  last_time: 0.3332  data_time: 0.0034  last_data_time: 0.0031   lr: 5.4177e-05  max_mem: 14765M
[10/03 23:20:51] d2.utils.events INFO:  eta: 0:26:40  iter: 4959  total_loss: 26.48  loss_ce: 0.505  loss_mask: 0.2379  loss_dice: 1.072  loss_contrastive: 0  loss_ce_0: 1.514  loss_mask_0: 0.4842  loss_dice_0: 1.603  loss_ce_1: 1.132  loss_mask_1: 0.2556  loss_dice_1: 1.092  loss_ce_2: 0.9383  loss_mask_2: 0.3376  loss_dice_2: 1.181  loss_ce_3: 0.8475  loss_mask_3: 0.2508  loss_dice_3: 1.126  loss_ce_4: 0.6806  loss_mask_4: 0.2961  loss_dice_4: 1.401  loss_ce_5: 0.6107  loss_mask_5: 0.2225  loss_dice_5: 1.121  loss_ce_6: 0.6105  loss_mask_6: 0.2565  loss_dice_6: 0.9594  loss_ce_7: 0.6638  loss_mask_7: 0.2436  loss_dice_7: 0.8918  loss_ce_8: 0.6146  loss_mask_8: 0.2273  loss_dice_8: 0.9866    time: 0.3203  last_time: 0.4510  data_time: 0.0034  last_data_time: 0.0031   lr: 5.3984e-05  max_mem: 14765M
[10/03 23:20:58] d2.utils.events INFO:  eta: 0:26:32  iter: 4979  total_loss: 21.76  loss_ce: 0.4191  loss_mask: 0.3872  loss_dice: 0.9667  loss_contrastive: 0  loss_ce_0: 1.278  loss_mask_0: 0.4627  loss_dice_0: 1.563  loss_ce_1: 0.8748  loss_mask_1: 0.3703  loss_dice_1: 1.115  loss_ce_2: 0.7019  loss_mask_2: 0.421  loss_dice_2: 1.109  loss_ce_3: 0.6422  loss_mask_3: 0.3839  loss_dice_3: 0.9984  loss_ce_4: 0.4625  loss_mask_4: 0.3543  loss_dice_4: 1.089  loss_ce_5: 0.4693  loss_mask_5: 0.3826  loss_dice_5: 1.1  loss_ce_6: 0.3452  loss_mask_6: 0.3812  loss_dice_6: 0.9597  loss_ce_7: 0.394  loss_mask_7: 0.3239  loss_dice_7: 0.9451  loss_ce_8: 0.4626  loss_mask_8: 0.36  loss_dice_8: 0.9674    time: 0.3203  last_time: 0.3093  data_time: 0.0032  last_data_time: 0.0051   lr: 5.3791e-05  max_mem: 14765M
[10/03 23:21:04] fvcore.common.checkpoint INFO: Saving checkpoint to outputs/ceymo2/model_0004999.pth
[10/03 23:21:08] oneformer.data.dataset_mappers.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1024, 1024), max_size=2048, sample_style='choice')]
[10/03 23:21:08] oneformer.data.datasets.register_cityscapes_panoptic INFO: 1 cities found in '/mnt/source/datasets/cityscapes/leftImg8bit/val'.
[10/03 23:21:08] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/03 23:21:08] d2.data.common INFO: Serializing 267 elements to byte tensors and concatenating them all ...
[10/03 23:21:08] d2.data.common INFO: Serialized dataset takes 0.44 MiB
[10/03 23:21:08] d2.evaluation.evaluator INFO: Start inference on 267 batches
[10/03 23:21:08] d2.evaluation.cityscapes_evaluation INFO: Writing cityscapes results to temporary directory /tmp/cityscapes_eval_xwgbfp_l ...
[10/03 23:21:08] oneformer.evaluation.cityscapes_evaluation INFO: Writing cityscapes results to temporary directory /tmp/cityscapes_eval_srbqw33h ...
[10/03 23:21:53] d2.evaluation.evaluator INFO: Inference done 11/267. Dataloading: 0.0021 s/iter. Inference: 0.4024 s/iter. Eval: 3.6694 s/iter. Total: 4.0740 s/iter. ETA=0:17:22
[10/03 23:22:02] d2.evaluation.evaluator INFO: Inference done 13/267. Dataloading: 0.0024 s/iter. Inference: 0.4099 s/iter. Eval: 3.7614 s/iter. Total: 4.1738 s/iter. ETA=0:17:40
[10/03 23:22:09] d2.evaluation.evaluator INFO: Inference done 15/267. Dataloading: 0.0026 s/iter. Inference: 0.4071 s/iter. Eval: 3.7171 s/iter. Total: 4.1268 s/iter. ETA=0:17:19
[10/03 23:22:18] d2.evaluation.evaluator INFO: Inference done 17/267. Dataloading: 0.0026 s/iter. Inference: 0.4111 s/iter. Eval: 3.7119 s/iter. Total: 4.1257 s/iter. ETA=0:17:11
[10/03 23:22:25] d2.evaluation.evaluator INFO: Inference done 19/267. Dataloading: 0.0027 s/iter. Inference: 0.4083 s/iter. Eval: 3.6687 s/iter. Total: 4.0797 s/iter. ETA=0:16:51
[10/03 23:22:33] d2.evaluation.evaluator INFO: Inference done 21/267. Dataloading: 0.0027 s/iter. Inference: 0.4061 s/iter. Eval: 3.6300 s/iter. Total: 4.0389 s/iter. ETA=0:16:33
[10/03 23:22:40] d2.evaluation.evaluator INFO: Inference done 23/267. Dataloading: 0.0029 s/iter. Inference: 0.4040 s/iter. Eval: 3.6094 s/iter. Total: 4.0164 s/iter. ETA=0:16:19
[10/03 23:22:48] d2.evaluation.evaluator INFO: Inference done 25/267. Dataloading: 0.0029 s/iter. Inference: 0.4031 s/iter. Eval: 3.6037 s/iter. Total: 4.0098 s/iter. ETA=0:16:10
[10/03 23:22:56] d2.evaluation.evaluator INFO: Inference done 27/267. Dataloading: 0.0029 s/iter. Inference: 0.4019 s/iter. Eval: 3.6089 s/iter. Total: 4.0138 s/iter. ETA=0:16:03
[10/03 23:23:05] d2.evaluation.evaluator INFO: Inference done 29/267. Dataloading: 0.0029 s/iter. Inference: 0.4013 s/iter. Eval: 3.6368 s/iter. Total: 4.0411 s/iter. ETA=0:16:01
[10/03 23:23:14] d2.evaluation.evaluator INFO: Inference done 31/267. Dataloading: 0.0029 s/iter. Inference: 0.4024 s/iter. Eval: 3.6812 s/iter. Total: 4.0867 s/iter. ETA=0:16:04
[10/03 23:23:23] d2.evaluation.evaluator INFO: Inference done 33/267. Dataloading: 0.0029 s/iter. Inference: 0.4018 s/iter. Eval: 3.7048 s/iter. Total: 4.1097 s/iter. ETA=0:16:01
[10/03 23:23:28] d2.evaluation.evaluator INFO: Inference done 34/267. Dataloading: 0.0030 s/iter. Inference: 0.4057 s/iter. Eval: 3.7324 s/iter. Total: 4.1411 s/iter. ETA=0:16:04
[10/03 23:23:35] d2.evaluation.evaluator INFO: Inference done 36/267. Dataloading: 0.0030 s/iter. Inference: 0.4084 s/iter. Eval: 3.6838 s/iter. Total: 4.0953 s/iter. ETA=0:15:46
[10/03 23:23:40] d2.utils.memory INFO: Attempting to copy inputs of <bound method OneFormer.instance_inference of OneFormer(
  (backbone): D2ConvNeXt(
    (downsample_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(3, 256, kernel_size=(4, 4), stride=(4, 4))
        (1): LayerNorm()
      )
      (1): Sequential(
        (0): LayerNorm()
        (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))
      )
      (2): Sequential(
        (0): LayerNorm()
        (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))
      )
      (3): Sequential(
        (0): LayerNorm()
        (1): Conv2d(1024, 2048, kernel_size=(2, 2), stride=(2, 2))
      )
    )
    (stages): ModuleList(
      (0): Sequential(
        (0): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): Identity()
        )
        (1): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): DropPath()
        )
      )
      (1): Sequential(
        (0): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
      )
      (2): Sequential(
        (0): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (3): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (4): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (5): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (6): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (7): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (8): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (9): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (10): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (11): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (12): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (13): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (14): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (15): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (16): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (17): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (18): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (19): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (20): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (21): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (22): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (23): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (24): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (25): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (26): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
      )
      (3): Sequential(
        (0): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
      )
    )
    (norm0): LayerNorm()
    (norm1): LayerNorm()
    (norm2): LayerNorm()
    (norm3): LayerNorm()
  )
  (sem_seg_head): OneFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): ContrastiveMultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (class_transformer): Transformer(
        (encoder): TransformerEncoder(
          (layers): ModuleList()
        )
        (decoder): TransformerDecoder(
          (layers): ModuleList(
            (0-1): 2 x TransformerDecoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (dropout3): Dropout(p=0.1, inplace=False)
            )
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_embed): Embedding(250, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (class_input_proj): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (class_embed): Linear(in_features=256, out_features=31, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (task_mlp): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=77, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
    )
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (text_projector): MLP(
    (layers): ModuleList(
      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
    )
  )
  (prompt_ctx): Embedding(16, 256)
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks', 'contrastive']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_contrastive': 0.5, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_contrastive_0': 0.5, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_contrastive_1': 0.5, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_contrastive_2': 0.5, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_contrastive_3': 0.5, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_contrastive_4': 0.5, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_contrastive_5': 0.5, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_contrastive_6': 0.5, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_contrastive_7': 0.5, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0, 'loss_contrastive_8': 0.5}
      num_classes: 30
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)> to CPU due to CUDA OOM
[10/03 23:23:46] d2.evaluation.evaluator INFO: Inference done 38/267. Dataloading: 0.0030 s/iter. Inference: 0.4866 s/iter. Eval: 3.6858 s/iter. Total: 4.1755 s/iter. ETA=0:15:56
[10/03 23:23:54] d2.evaluation.evaluator INFO: Inference done 40/267. Dataloading: 0.0030 s/iter. Inference: 0.4892 s/iter. Eval: 3.6831 s/iter. Total: 4.1754 s/iter. ETA=0:15:47
[10/03 23:23:55] d2.utils.memory INFO: Attempting to copy inputs of <bound method OneFormer.instance_inference of OneFormer(
  (backbone): D2ConvNeXt(
    (downsample_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(3, 256, kernel_size=(4, 4), stride=(4, 4))
        (1): LayerNorm()
      )
      (1): Sequential(
        (0): LayerNorm()
        (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))
      )
      (2): Sequential(
        (0): LayerNorm()
        (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))
      )
      (3): Sequential(
        (0): LayerNorm()
        (1): Conv2d(1024, 2048, kernel_size=(2, 2), stride=(2, 2))
      )
    )
    (stages): ModuleList(
      (0): Sequential(
        (0): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): Identity()
        )
        (1): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): DropPath()
        )
      )
      (1): Sequential(
        (0): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
      )
      (2): Sequential(
        (0): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (3): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (4): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (5): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (6): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (7): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (8): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (9): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (10): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (11): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (12): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (13): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (14): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (15): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (16): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (17): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (18): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (19): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (20): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (21): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (22): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (23): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (24): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (25): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (26): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
      )
      (3): Sequential(
        (0): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
      )
    )
    (norm0): LayerNorm()
    (norm1): LayerNorm()
    (norm2): LayerNorm()
    (norm3): LayerNorm()
  )
  (sem_seg_head): OneFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): ContrastiveMultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (class_transformer): Transformer(
        (encoder): TransformerEncoder(
          (layers): ModuleList()
        )
        (decoder): TransformerDecoder(
          (layers): ModuleList(
            (0-1): 2 x TransformerDecoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (dropout3): Dropout(p=0.1, inplace=False)
            )
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_embed): Embedding(250, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (class_input_proj): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (class_embed): Linear(in_features=256, out_features=31, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (task_mlp): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=77, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
    )
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (text_projector): MLP(
    (layers): ModuleList(
      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
    )
  )
  (prompt_ctx): Embedding(16, 256)
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks', 'contrastive']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_contrastive': 0.5, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_contrastive_0': 0.5, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_contrastive_1': 0.5, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_contrastive_2': 0.5, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_contrastive_3': 0.5, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_contrastive_4': 0.5, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_contrastive_5': 0.5, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_contrastive_6': 0.5, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_contrastive_7': 0.5, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0, 'loss_contrastive_8': 0.5}
      num_classes: 30
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)> to CPU due to CUDA OOM
[10/03 23:24:00] d2.evaluation.evaluator INFO: Inference done 41/267. Dataloading: 0.0030 s/iter. Inference: 0.5440 s/iter. Eval: 3.6728 s/iter. Total: 4.2199 s/iter. ETA=0:15:53
[10/03 23:24:09] d2.evaluation.evaluator INFO: Inference done 43/267. Dataloading: 0.0030 s/iter. Inference: 0.5424 s/iter. Eval: 3.6889 s/iter. Total: 4.2343 s/iter. ETA=0:15:48
[10/03 23:24:17] d2.evaluation.evaluator INFO: Inference done 45/267. Dataloading: 0.0030 s/iter. Inference: 0.5346 s/iter. Eval: 3.6910 s/iter. Total: 4.2287 s/iter. ETA=0:15:38
[10/03 23:24:26] d2.evaluation.evaluator INFO: Inference done 47/267. Dataloading: 0.0030 s/iter. Inference: 0.5280 s/iter. Eval: 3.7091 s/iter. Total: 4.2402 s/iter. ETA=0:15:32
[10/03 23:24:34] d2.evaluation.evaluator INFO: Inference done 49/267. Dataloading: 0.0030 s/iter. Inference: 0.5218 s/iter. Eval: 3.6883 s/iter. Total: 4.2132 s/iter. ETA=0:15:18
[10/03 23:24:41] d2.evaluation.evaluator INFO: Inference done 51/267. Dataloading: 0.0030 s/iter. Inference: 0.5161 s/iter. Eval: 3.6826 s/iter. Total: 4.2017 s/iter. ETA=0:15:07
[10/03 23:24:49] d2.evaluation.evaluator INFO: Inference done 53/267. Dataloading: 0.0030 s/iter. Inference: 0.5148 s/iter. Eval: 3.6754 s/iter. Total: 4.1933 s/iter. ETA=0:14:57
[10/03 23:24:58] d2.evaluation.evaluator INFO: Inference done 55/267. Dataloading: 0.0030 s/iter. Inference: 0.5147 s/iter. Eval: 3.6790 s/iter. Total: 4.1968 s/iter. ETA=0:14:49
[10/03 23:25:03] d2.utils.memory INFO: Attempting to copy inputs of <bound method OneFormer.instance_inference of OneFormer(
  (backbone): D2ConvNeXt(
    (downsample_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(3, 256, kernel_size=(4, 4), stride=(4, 4))
        (1): LayerNorm()
      )
      (1): Sequential(
        (0): LayerNorm()
        (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))
      )
      (2): Sequential(
        (0): LayerNorm()
        (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))
      )
      (3): Sequential(
        (0): LayerNorm()
        (1): Conv2d(1024, 2048, kernel_size=(2, 2), stride=(2, 2))
      )
    )
    (stages): ModuleList(
      (0): Sequential(
        (0): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): Identity()
        )
        (1): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): DropPath()
        )
      )
      (1): Sequential(
        (0): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
      )
      (2): Sequential(
        (0): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (3): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (4): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (5): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (6): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (7): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (8): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (9): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (10): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (11): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (12): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (13): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (14): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (15): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (16): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (17): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (18): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (19): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (20): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (21): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (22): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (23): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (24): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (25): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (26): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
      )
      (3): Sequential(
        (0): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
      )
    )
    (norm0): LayerNorm()
    (norm1): LayerNorm()
    (norm2): LayerNorm()
    (norm3): LayerNorm()
  )
  (sem_seg_head): OneFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): ContrastiveMultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (class_transformer): Transformer(
        (encoder): TransformerEncoder(
          (layers): ModuleList()
        )
        (decoder): TransformerDecoder(
          (layers): ModuleList(
            (0-1): 2 x TransformerDecoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (dropout3): Dropout(p=0.1, inplace=False)
            )
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_embed): Embedding(250, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (class_input_proj): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (class_embed): Linear(in_features=256, out_features=31, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (task_mlp): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=77, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
    )
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (text_projector): MLP(
    (layers): ModuleList(
      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
    )
  )
  (prompt_ctx): Embedding(16, 256)
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks', 'contrastive']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_contrastive': 0.5, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_contrastive_0': 0.5, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_contrastive_1': 0.5, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_contrastive_2': 0.5, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_contrastive_3': 0.5, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_contrastive_4': 0.5, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_contrastive_5': 0.5, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_contrastive_6': 0.5, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_contrastive_7': 0.5, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0, 'loss_contrastive_8': 0.5}
      num_classes: 30
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)> to CPU due to CUDA OOM
[10/03 23:25:08] d2.evaluation.evaluator INFO: Inference done 57/267. Dataloading: 0.0030 s/iter. Inference: 0.5514 s/iter. Eval: 3.6787 s/iter. Total: 4.2333 s/iter. ETA=0:14:48
[10/03 23:25:17] d2.evaluation.evaluator INFO: Inference done 59/267. Dataloading: 0.0030 s/iter. Inference: 0.5496 s/iter. Eval: 3.6774 s/iter. Total: 4.2302 s/iter. ETA=0:14:39
[10/03 23:25:25] d2.evaluation.evaluator INFO: Inference done 61/267. Dataloading: 0.0030 s/iter. Inference: 0.5483 s/iter. Eval: 3.6839 s/iter. Total: 4.2353 s/iter. ETA=0:14:32
[10/03 23:25:30] d2.utils.memory INFO: Attempting to copy inputs of <bound method OneFormer.instance_inference of OneFormer(
  (backbone): D2ConvNeXt(
    (downsample_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(3, 256, kernel_size=(4, 4), stride=(4, 4))
        (1): LayerNorm()
      )
      (1): Sequential(
        (0): LayerNorm()
        (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))
      )
      (2): Sequential(
        (0): LayerNorm()
        (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))
      )
      (3): Sequential(
        (0): LayerNorm()
        (1): Conv2d(1024, 2048, kernel_size=(2, 2), stride=(2, 2))
      )
    )
    (stages): ModuleList(
      (0): Sequential(
        (0): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): Identity()
        )
        (1): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): DropPath()
        )
      )
      (1): Sequential(
        (0): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
      )
      (2): Sequential(
        (0): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (3): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (4): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (5): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (6): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (7): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (8): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (9): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (10): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (11): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (12): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (13): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (14): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (15): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (16): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (17): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (18): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (19): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (20): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (21): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (22): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (23): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (24): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (25): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (26): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
      )
      (3): Sequential(
        (0): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
      )
    )
    (norm0): LayerNorm()
    (norm1): LayerNorm()
    (norm2): LayerNorm()
    (norm3): LayerNorm()
  )
  (sem_seg_head): OneFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): ContrastiveMultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (class_transformer): Transformer(
        (encoder): TransformerEncoder(
          (layers): ModuleList()
        )
        (decoder): TransformerDecoder(
          (layers): ModuleList(
            (0-1): 2 x TransformerDecoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (dropout3): Dropout(p=0.1, inplace=False)
            )
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_embed): Embedding(250, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (class_input_proj): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (class_embed): Linear(in_features=256, out_features=31, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (task_mlp): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=77, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
    )
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (text_projector): MLP(
    (layers): ModuleList(
      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
    )
  )
  (prompt_ctx): Embedding(16, 256)
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks', 'contrastive']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_contrastive': 0.5, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_contrastive_0': 0.5, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_contrastive_1': 0.5, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_contrastive_2': 0.5, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_contrastive_3': 0.5, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_contrastive_4': 0.5, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_contrastive_5': 0.5, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_contrastive_6': 0.5, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_contrastive_7': 0.5, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0, 'loss_contrastive_8': 0.5}
      num_classes: 30
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)> to CPU due to CUDA OOM
[10/03 23:25:35] d2.evaluation.evaluator INFO: Inference done 63/267. Dataloading: 0.0030 s/iter. Inference: 0.5795 s/iter. Eval: 3.6815 s/iter. Total: 4.2642 s/iter. ETA=0:14:29
[10/03 23:25:44] d2.evaluation.evaluator INFO: Inference done 65/267. Dataloading: 0.0030 s/iter. Inference: 0.5759 s/iter. Eval: 3.6801 s/iter. Total: 4.2592 s/iter. ETA=0:14:20
[10/03 23:25:51] d2.evaluation.evaluator INFO: Inference done 67/267. Dataloading: 0.0031 s/iter. Inference: 0.5730 s/iter. Eval: 3.6635 s/iter. Total: 4.2397 s/iter. ETA=0:14:07
[10/03 23:25:56] d2.utils.memory INFO: Attempting to copy inputs of <bound method OneFormer.instance_inference of OneFormer(
  (backbone): D2ConvNeXt(
    (downsample_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(3, 256, kernel_size=(4, 4), stride=(4, 4))
        (1): LayerNorm()
      )
      (1): Sequential(
        (0): LayerNorm()
        (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))
      )
      (2): Sequential(
        (0): LayerNorm()
        (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))
      )
      (3): Sequential(
        (0): LayerNorm()
        (1): Conv2d(1024, 2048, kernel_size=(2, 2), stride=(2, 2))
      )
    )
    (stages): ModuleList(
      (0): Sequential(
        (0): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): Identity()
        )
        (1): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): DropPath()
        )
      )
      (1): Sequential(
        (0): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
      )
      (2): Sequential(
        (0): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (3): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (4): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (5): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (6): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (7): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (8): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (9): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (10): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (11): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (12): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (13): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (14): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (15): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (16): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (17): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (18): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (19): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (20): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (21): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (22): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (23): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (24): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (25): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (26): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
      )
      (3): Sequential(
        (0): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
      )
    )
    (norm0): LayerNorm()
    (norm1): LayerNorm()
    (norm2): LayerNorm()
    (norm3): LayerNorm()
  )
  (sem_seg_head): OneFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): ContrastiveMultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (class_transformer): Transformer(
        (encoder): TransformerEncoder(
          (layers): ModuleList()
        )
        (decoder): TransformerDecoder(
          (layers): ModuleList(
            (0-1): 2 x TransformerDecoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (dropout3): Dropout(p=0.1, inplace=False)
            )
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_embed): Embedding(250, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (class_input_proj): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (class_embed): Linear(in_features=256, out_features=31, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (task_mlp): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=77, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
    )
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (text_projector): MLP(
    (layers): ModuleList(
      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
    )
  )
  (prompt_ctx): Embedding(16, 256)
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks', 'contrastive']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_contrastive': 0.5, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_contrastive_0': 0.5, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_contrastive_1': 0.5, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_contrastive_2': 0.5, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_contrastive_3': 0.5, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_contrastive_4': 0.5, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_contrastive_5': 0.5, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_contrastive_6': 0.5, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_contrastive_7': 0.5, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0, 'loss_contrastive_8': 0.5}
      num_classes: 30
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)> to CPU due to CUDA OOM
[10/03 23:26:02] d2.evaluation.evaluator INFO: Inference done 69/267. Dataloading: 0.0031 s/iter. Inference: 0.6030 s/iter. Eval: 3.6732 s/iter. Total: 4.2794 s/iter. ETA=0:14:07
[10/03 23:26:11] d2.evaluation.evaluator INFO: Inference done 71/267. Dataloading: 0.0031 s/iter. Inference: 0.5995 s/iter. Eval: 3.6817 s/iter. Total: 4.2844 s/iter. ETA=0:13:59
[10/03 23:26:20] d2.evaluation.evaluator INFO: Inference done 73/267. Dataloading: 0.0031 s/iter. Inference: 0.5944 s/iter. Eval: 3.6880 s/iter. Total: 4.2856 s/iter. ETA=0:13:51
[10/03 23:26:28] d2.evaluation.evaluator INFO: Inference done 75/267. Dataloading: 0.0031 s/iter. Inference: 0.5898 s/iter. Eval: 3.6935 s/iter. Total: 4.2864 s/iter. ETA=0:13:42
[10/03 23:26:36] d2.evaluation.evaluator INFO: Inference done 77/267. Dataloading: 0.0031 s/iter. Inference: 0.5841 s/iter. Eval: 3.6930 s/iter. Total: 4.2803 s/iter. ETA=0:13:33
[10/03 23:26:45] d2.evaluation.evaluator INFO: Inference done 79/267. Dataloading: 0.0031 s/iter. Inference: 0.5816 s/iter. Eval: 3.6949 s/iter. Total: 4.2798 s/iter. ETA=0:13:24
[10/03 23:26:54] d2.evaluation.evaluator INFO: Inference done 81/267. Dataloading: 0.0031 s/iter. Inference: 0.5797 s/iter. Eval: 3.6991 s/iter. Total: 4.2821 s/iter. ETA=0:13:16
[10/03 23:26:54] d2.utils.memory INFO: Attempting to copy inputs of <bound method OneFormer.instance_inference of OneFormer(
  (backbone): D2ConvNeXt(
    (downsample_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(3, 256, kernel_size=(4, 4), stride=(4, 4))
        (1): LayerNorm()
      )
      (1): Sequential(
        (0): LayerNorm()
        (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))
      )
      (2): Sequential(
        (0): LayerNorm()
        (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))
      )
      (3): Sequential(
        (0): LayerNorm()
        (1): Conv2d(1024, 2048, kernel_size=(2, 2), stride=(2, 2))
      )
    )
    (stages): ModuleList(
      (0): Sequential(
        (0): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): Identity()
        )
        (1): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): DropPath()
        )
      )
      (1): Sequential(
        (0): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
      )
      (2): Sequential(
        (0): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (3): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (4): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (5): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (6): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (7): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (8): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (9): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (10): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (11): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (12): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (13): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (14): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (15): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (16): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (17): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (18): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (19): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (20): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (21): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (22): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (23): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (24): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (25): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (26): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
      )
      (3): Sequential(
        (0): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
      )
    )
    (norm0): LayerNorm()
    (norm1): LayerNorm()
    (norm2): LayerNorm()
    (norm3): LayerNorm()
  )
  (sem_seg_head): OneFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): ContrastiveMultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (class_transformer): Transformer(
        (encoder): TransformerEncoder(
          (layers): ModuleList()
        )
        (decoder): TransformerDecoder(
          (layers): ModuleList(
            (0-1): 2 x TransformerDecoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (dropout3): Dropout(p=0.1, inplace=False)
            )
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_embed): Embedding(250, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (class_input_proj): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (class_embed): Linear(in_features=256, out_features=31, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (task_mlp): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=77, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
    )
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (text_projector): MLP(
    (layers): ModuleList(
      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
    )
  )
  (prompt_ctx): Embedding(16, 256)
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks', 'contrastive']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_contrastive': 0.5, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_contrastive_0': 0.5, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_contrastive_1': 0.5, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_contrastive_2': 0.5, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_contrastive_3': 0.5, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_contrastive_4': 0.5, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_contrastive_5': 0.5, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_contrastive_6': 0.5, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_contrastive_7': 0.5, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0, 'loss_contrastive_8': 0.5}
      num_classes: 30
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)> to CPU due to CUDA OOM
[10/03 23:26:59] d2.evaluation.evaluator INFO: Inference done 82/267. Dataloading: 0.0031 s/iter. Inference: 0.6052 s/iter. Eval: 3.6935 s/iter. Total: 4.3020 s/iter. ETA=0:13:15
[10/03 23:27:08] d2.evaluation.evaluator INFO: Inference done 84/267. Dataloading: 0.0031 s/iter. Inference: 0.6024 s/iter. Eval: 3.6917 s/iter. Total: 4.2973 s/iter. ETA=0:13:06
[10/03 23:27:16] d2.evaluation.evaluator INFO: Inference done 86/267. Dataloading: 0.0031 s/iter. Inference: 0.6004 s/iter. Eval: 3.6958 s/iter. Total: 4.2995 s/iter. ETA=0:12:58
[10/03 23:27:21] d2.utils.memory INFO: Attempting to copy inputs of <bound method OneFormer.instance_inference of OneFormer(
  (backbone): D2ConvNeXt(
    (downsample_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(3, 256, kernel_size=(4, 4), stride=(4, 4))
        (1): LayerNorm()
      )
      (1): Sequential(
        (0): LayerNorm()
        (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))
      )
      (2): Sequential(
        (0): LayerNorm()
        (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))
      )
      (3): Sequential(
        (0): LayerNorm()
        (1): Conv2d(1024, 2048, kernel_size=(2, 2), stride=(2, 2))
      )
    )
    (stages): ModuleList(
      (0): Sequential(
        (0): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): Identity()
        )
        (1): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): DropPath()
        )
      )
      (1): Sequential(
        (0): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
      )
      (2): Sequential(
        (0): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (3): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (4): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (5): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (6): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (7): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (8): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (9): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (10): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (11): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (12): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (13): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (14): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (15): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (16): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (17): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (18): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (19): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (20): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (21): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (22): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (23): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (24): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (25): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (26): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
      )
      (3): Sequential(
        (0): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
      )
    )
    (norm0): LayerNorm()
    (norm1): LayerNorm()
    (norm2): LayerNorm()
    (norm3): LayerNorm()
  )
  (sem_seg_head): OneFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): ContrastiveMultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (class_transformer): Transformer(
        (encoder): TransformerEncoder(
          (layers): ModuleList()
        )
        (decoder): TransformerDecoder(
          (layers): ModuleList(
            (0-1): 2 x TransformerDecoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (dropout3): Dropout(p=0.1, inplace=False)
            )
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_embed): Embedding(250, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (class_input_proj): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (class_embed): Linear(in_features=256, out_features=31, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (task_mlp): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=77, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
    )
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (text_projector): MLP(
    (layers): ModuleList(
      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
    )
  )
  (prompt_ctx): Embedding(16, 256)
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks', 'contrastive']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_contrastive': 0.5, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_contrastive_0': 0.5, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_contrastive_1': 0.5, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_contrastive_2': 0.5, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_contrastive_3': 0.5, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_contrastive_4': 0.5, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_contrastive_5': 0.5, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_contrastive_6': 0.5, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_contrastive_7': 0.5, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0, 'loss_contrastive_8': 0.5}
      num_classes: 30
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)> to CPU due to CUDA OOM
[10/03 23:27:26] d2.evaluation.evaluator INFO: Inference done 88/267. Dataloading: 0.0031 s/iter. Inference: 0.6209 s/iter. Eval: 3.6919 s/iter. Total: 4.3160 s/iter. ETA=0:12:52
[10/03 23:27:35] d2.evaluation.evaluator INFO: Inference done 90/267. Dataloading: 0.0031 s/iter. Inference: 0.6164 s/iter. Eval: 3.6973 s/iter. Total: 4.3169 s/iter. ETA=0:12:44
[10/03 23:27:43] d2.evaluation.evaluator INFO: Inference done 92/267. Dataloading: 0.0031 s/iter. Inference: 0.6111 s/iter. Eval: 3.6942 s/iter. Total: 4.3086 s/iter. ETA=0:12:33
[10/03 23:27:51] d2.evaluation.evaluator INFO: Inference done 94/267. Dataloading: 0.0031 s/iter. Inference: 0.6060 s/iter. Eval: 3.6944 s/iter. Total: 4.3036 s/iter. ETA=0:12:24
[10/03 23:27:59] d2.evaluation.evaluator INFO: Inference done 96/267. Dataloading: 0.0031 s/iter. Inference: 0.6012 s/iter. Eval: 3.6909 s/iter. Total: 4.2953 s/iter. ETA=0:12:14
[10/03 23:28:08] d2.evaluation.evaluator INFO: Inference done 98/267. Dataloading: 0.0031 s/iter. Inference: 0.5974 s/iter. Eval: 3.6973 s/iter. Total: 4.2980 s/iter. ETA=0:12:06
[10/03 23:28:17] d2.evaluation.evaluator INFO: Inference done 100/267. Dataloading: 0.0031 s/iter. Inference: 0.5935 s/iter. Eval: 3.7019 s/iter. Total: 4.2986 s/iter. ETA=0:11:57
[10/03 23:28:25] d2.evaluation.evaluator INFO: Inference done 102/267. Dataloading: 0.0031 s/iter. Inference: 0.5893 s/iter. Eval: 3.7002 s/iter. Total: 4.2927 s/iter. ETA=0:11:48
[10/03 23:28:32] d2.evaluation.evaluator INFO: Inference done 104/267. Dataloading: 0.0031 s/iter. Inference: 0.5854 s/iter. Eval: 3.6973 s/iter. Total: 4.2860 s/iter. ETA=0:11:38
[10/03 23:28:41] d2.evaluation.evaluator INFO: Inference done 106/267. Dataloading: 0.0031 s/iter. Inference: 0.5817 s/iter. Eval: 3.7030 s/iter. Total: 4.2879 s/iter. ETA=0:11:30
[10/03 23:28:50] d2.evaluation.evaluator INFO: Inference done 108/267. Dataloading: 0.0032 s/iter. Inference: 0.5780 s/iter. Eval: 3.7047 s/iter. Total: 4.2861 s/iter. ETA=0:11:21
[10/03 23:28:58] d2.evaluation.evaluator INFO: Inference done 110/267. Dataloading: 0.0032 s/iter. Inference: 0.5746 s/iter. Eval: 3.7058 s/iter. Total: 4.2837 s/iter. ETA=0:11:12
[10/03 23:29:05] d2.evaluation.evaluator INFO: Inference done 112/267. Dataloading: 0.0032 s/iter. Inference: 0.5712 s/iter. Eval: 3.6973 s/iter. Total: 4.2718 s/iter. ETA=0:11:02
[10/03 23:29:12] d2.evaluation.evaluator INFO: Inference done 114/267. Dataloading: 0.0032 s/iter. Inference: 0.5679 s/iter. Eval: 3.6888 s/iter. Total: 4.2600 s/iter. ETA=0:10:51
[10/03 23:29:22] d2.evaluation.evaluator INFO: Inference done 116/267. Dataloading: 0.0032 s/iter. Inference: 0.5655 s/iter. Eval: 3.6959 s/iter. Total: 4.2647 s/iter. ETA=0:10:43
[10/03 23:29:29] d2.evaluation.evaluator INFO: Inference done 118/267. Dataloading: 0.0032 s/iter. Inference: 0.5624 s/iter. Eval: 3.6910 s/iter. Total: 4.2567 s/iter. ETA=0:10:34
[10/03 23:29:37] d2.evaluation.evaluator INFO: Inference done 120/267. Dataloading: 0.0032 s/iter. Inference: 0.5595 s/iter. Eval: 3.6911 s/iter. Total: 4.2538 s/iter. ETA=0:10:25
[10/03 23:29:45] d2.evaluation.evaluator INFO: Inference done 122/267. Dataloading: 0.0032 s/iter. Inference: 0.5566 s/iter. Eval: 3.6852 s/iter. Total: 4.2451 s/iter. ETA=0:10:15
[10/03 23:29:53] d2.evaluation.evaluator INFO: Inference done 124/267. Dataloading: 0.0032 s/iter. Inference: 0.5539 s/iter. Eval: 3.6884 s/iter. Total: 4.2455 s/iter. ETA=0:10:07
[10/03 23:30:02] d2.evaluation.evaluator INFO: Inference done 126/267. Dataloading: 0.0032 s/iter. Inference: 0.5512 s/iter. Eval: 3.6904 s/iter. Total: 4.2449 s/iter. ETA=0:09:58
[10/03 23:30:11] d2.evaluation.evaluator INFO: Inference done 128/267. Dataloading: 0.0032 s/iter. Inference: 0.5501 s/iter. Eval: 3.6982 s/iter. Total: 4.2516 s/iter. ETA=0:09:50
[10/03 23:30:20] d2.evaluation.evaluator INFO: Inference done 130/267. Dataloading: 0.0032 s/iter. Inference: 0.5496 s/iter. Eval: 3.7040 s/iter. Total: 4.2570 s/iter. ETA=0:09:43
[10/03 23:30:29] d2.evaluation.evaluator INFO: Inference done 132/267. Dataloading: 0.0032 s/iter. Inference: 0.5477 s/iter. Eval: 3.7062 s/iter. Total: 4.2573 s/iter. ETA=0:09:34
[10/03 23:30:29] d2.utils.memory INFO: Attempting to copy inputs of <bound method OneFormer.instance_inference of OneFormer(
  (backbone): D2ConvNeXt(
    (downsample_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(3, 256, kernel_size=(4, 4), stride=(4, 4))
        (1): LayerNorm()
      )
      (1): Sequential(
        (0): LayerNorm()
        (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))
      )
      (2): Sequential(
        (0): LayerNorm()
        (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))
      )
      (3): Sequential(
        (0): LayerNorm()
        (1): Conv2d(1024, 2048, kernel_size=(2, 2), stride=(2, 2))
      )
    )
    (stages): ModuleList(
      (0): Sequential(
        (0): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): Identity()
        )
        (1): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): DropPath()
        )
      )
      (1): Sequential(
        (0): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
      )
      (2): Sequential(
        (0): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (3): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (4): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (5): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (6): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (7): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (8): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (9): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (10): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (11): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (12): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (13): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (14): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (15): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (16): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (17): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (18): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (19): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (20): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (21): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (22): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (23): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (24): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (25): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (26): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
      )
      (3): Sequential(
        (0): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
      )
    )
    (norm0): LayerNorm()
    (norm1): LayerNorm()
    (norm2): LayerNorm()
    (norm3): LayerNorm()
  )
  (sem_seg_head): OneFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): ContrastiveMultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (class_transformer): Transformer(
        (encoder): TransformerEncoder(
          (layers): ModuleList()
        )
        (decoder): TransformerDecoder(
          (layers): ModuleList(
            (0-1): 2 x TransformerDecoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (dropout3): Dropout(p=0.1, inplace=False)
            )
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_embed): Embedding(250, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (class_input_proj): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (class_embed): Linear(in_features=256, out_features=31, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (task_mlp): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=77, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
    )
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (text_projector): MLP(
    (layers): ModuleList(
      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
    )
  )
  (prompt_ctx): Embedding(16, 256)
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks', 'contrastive']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_contrastive': 0.5, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_contrastive_0': 0.5, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_contrastive_1': 0.5, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_contrastive_2': 0.5, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_contrastive_3': 0.5, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_contrastive_4': 0.5, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_contrastive_5': 0.5, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_contrastive_6': 0.5, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_contrastive_7': 0.5, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0, 'loss_contrastive_8': 0.5}
      num_classes: 30
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)> to CPU due to CUDA OOM
[10/03 23:30:34] d2.evaluation.evaluator INFO: Inference done 133/267. Dataloading: 0.0032 s/iter. Inference: 0.5626 s/iter. Eval: 3.7000 s/iter. Total: 4.2660 s/iter. ETA=0:09:31
[10/03 23:30:43] d2.evaluation.evaluator INFO: Inference done 135/267. Dataloading: 0.0032 s/iter. Inference: 0.5605 s/iter. Eval: 3.7005 s/iter. Total: 4.2643 s/iter. ETA=0:09:22
[10/03 23:30:51] d2.evaluation.evaluator INFO: Inference done 137/267. Dataloading: 0.0032 s/iter. Inference: 0.5589 s/iter. Eval: 3.7010 s/iter. Total: 4.2632 s/iter. ETA=0:09:14
[10/03 23:30:59] d2.evaluation.evaluator INFO: Inference done 139/267. Dataloading: 0.0032 s/iter. Inference: 0.5571 s/iter. Eval: 3.6998 s/iter. Total: 4.2602 s/iter. ETA=0:09:05
[10/03 23:31:07] d2.evaluation.evaluator INFO: Inference done 141/267. Dataloading: 0.0032 s/iter. Inference: 0.5555 s/iter. Eval: 3.6990 s/iter. Total: 4.2579 s/iter. ETA=0:08:56
[10/03 23:31:08] d2.utils.memory INFO: Attempting to copy inputs of <bound method OneFormer.instance_inference of OneFormer(
  (backbone): D2ConvNeXt(
    (downsample_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(3, 256, kernel_size=(4, 4), stride=(4, 4))
        (1): LayerNorm()
      )
      (1): Sequential(
        (0): LayerNorm()
        (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))
      )
      (2): Sequential(
        (0): LayerNorm()
        (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))
      )
      (3): Sequential(
        (0): LayerNorm()
        (1): Conv2d(1024, 2048, kernel_size=(2, 2), stride=(2, 2))
      )
    )
    (stages): ModuleList(
      (0): Sequential(
        (0): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): Identity()
        )
        (1): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): DropPath()
        )
      )
      (1): Sequential(
        (0): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
      )
      (2): Sequential(
        (0): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (3): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (4): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (5): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (6): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (7): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (8): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (9): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (10): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (11): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (12): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (13): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (14): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (15): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (16): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (17): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (18): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (19): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (20): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (21): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (22): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (23): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (24): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (25): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (26): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
      )
      (3): Sequential(
        (0): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
      )
    )
    (norm0): LayerNorm()
    (norm1): LayerNorm()
    (norm2): LayerNorm()
    (norm3): LayerNorm()
  )
  (sem_seg_head): OneFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): ContrastiveMultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (class_transformer): Transformer(
        (encoder): TransformerEncoder(
          (layers): ModuleList()
        )
        (decoder): TransformerDecoder(
          (layers): ModuleList(
            (0-1): 2 x TransformerDecoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (dropout3): Dropout(p=0.1, inplace=False)
            )
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_embed): Embedding(250, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (class_input_proj): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (class_embed): Linear(in_features=256, out_features=31, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (task_mlp): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=77, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
    )
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (text_projector): MLP(
    (layers): ModuleList(
      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
    )
  )
  (prompt_ctx): Embedding(16, 256)
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks', 'contrastive']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_contrastive': 0.5, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_contrastive_0': 0.5, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_contrastive_1': 0.5, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_contrastive_2': 0.5, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_contrastive_3': 0.5, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_contrastive_4': 0.5, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_contrastive_5': 0.5, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_contrastive_6': 0.5, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_contrastive_7': 0.5, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0, 'loss_contrastive_8': 0.5}
      num_classes: 30
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)> to CPU due to CUDA OOM
[10/03 23:31:13] d2.evaluation.evaluator INFO: Inference done 142/267. Dataloading: 0.0032 s/iter. Inference: 0.5697 s/iter. Eval: 3.6966 s/iter. Total: 4.2696 s/iter. ETA=0:08:53
[10/03 23:31:22] d2.evaluation.evaluator INFO: Inference done 144/267. Dataloading: 0.0032 s/iter. Inference: 0.5677 s/iter. Eval: 3.7021 s/iter. Total: 4.2732 s/iter. ETA=0:08:45
[10/03 23:31:29] d2.evaluation.evaluator INFO: Inference done 146/267. Dataloading: 0.0032 s/iter. Inference: 0.5653 s/iter. Eval: 3.6954 s/iter. Total: 4.2640 s/iter. ETA=0:08:35
[10/03 23:31:37] d2.evaluation.evaluator INFO: Inference done 148/267. Dataloading: 0.0032 s/iter. Inference: 0.5631 s/iter. Eval: 3.6911 s/iter. Total: 4.2575 s/iter. ETA=0:08:26
[10/03 23:31:46] d2.evaluation.evaluator INFO: Inference done 150/267. Dataloading: 0.0032 s/iter. Inference: 0.5622 s/iter. Eval: 3.6972 s/iter. Total: 4.2628 s/iter. ETA=0:08:18
[10/03 23:31:54] d2.evaluation.evaluator INFO: Inference done 152/267. Dataloading: 0.0032 s/iter. Inference: 0.5600 s/iter. Eval: 3.6961 s/iter. Total: 4.2595 s/iter. ETA=0:08:09
[10/03 23:32:00] d2.evaluation.evaluator INFO: Inference done 153/267. Dataloading: 0.0032 s/iter. Inference: 0.5594 s/iter. Eval: 3.7032 s/iter. Total: 4.2660 s/iter. ETA=0:08:06
[10/03 23:32:09] d2.evaluation.evaluator INFO: Inference done 155/267. Dataloading: 0.0032 s/iter. Inference: 0.5573 s/iter. Eval: 3.7098 s/iter. Total: 4.2704 s/iter. ETA=0:07:58
[10/03 23:32:17] d2.evaluation.evaluator INFO: Inference done 157/267. Dataloading: 0.0032 s/iter. Inference: 0.5551 s/iter. Eval: 3.7070 s/iter. Total: 4.2655 s/iter. ETA=0:07:49
[10/03 23:32:25] d2.evaluation.evaluator INFO: Inference done 159/267. Dataloading: 0.0032 s/iter. Inference: 0.5544 s/iter. Eval: 3.7059 s/iter. Total: 4.2636 s/iter. ETA=0:07:40
[10/03 23:32:33] d2.evaluation.evaluator INFO: Inference done 161/267. Dataloading: 0.0032 s/iter. Inference: 0.5531 s/iter. Eval: 3.7042 s/iter. Total: 4.2607 s/iter. ETA=0:07:31
[10/03 23:32:33] d2.utils.memory INFO: Attempting to copy inputs of <bound method OneFormer.instance_inference of OneFormer(
  (backbone): D2ConvNeXt(
    (downsample_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(3, 256, kernel_size=(4, 4), stride=(4, 4))
        (1): LayerNorm()
      )
      (1): Sequential(
        (0): LayerNorm()
        (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))
      )
      (2): Sequential(
        (0): LayerNorm()
        (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))
      )
      (3): Sequential(
        (0): LayerNorm()
        (1): Conv2d(1024, 2048, kernel_size=(2, 2), stride=(2, 2))
      )
    )
    (stages): ModuleList(
      (0): Sequential(
        (0): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): Identity()
        )
        (1): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): DropPath()
        )
      )
      (1): Sequential(
        (0): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
      )
      (2): Sequential(
        (0): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (3): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (4): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (5): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (6): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (7): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (8): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (9): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (10): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (11): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (12): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (13): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (14): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (15): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (16): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (17): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (18): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (19): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (20): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (21): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (22): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (23): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (24): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (25): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (26): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
      )
      (3): Sequential(
        (0): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
      )
    )
    (norm0): LayerNorm()
    (norm1): LayerNorm()
    (norm2): LayerNorm()
    (norm3): LayerNorm()
  )
  (sem_seg_head): OneFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): ContrastiveMultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (class_transformer): Transformer(
        (encoder): TransformerEncoder(
          (layers): ModuleList()
        )
        (decoder): TransformerDecoder(
          (layers): ModuleList(
            (0-1): 2 x TransformerDecoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (dropout3): Dropout(p=0.1, inplace=False)
            )
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_embed): Embedding(250, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (class_input_proj): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (class_embed): Linear(in_features=256, out_features=31, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (task_mlp): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=77, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
    )
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (text_projector): MLP(
    (layers): ModuleList(
      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
    )
  )
  (prompt_ctx): Embedding(16, 256)
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks', 'contrastive']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_contrastive': 0.5, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_contrastive_0': 0.5, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_contrastive_1': 0.5, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_contrastive_2': 0.5, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_contrastive_3': 0.5, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_contrastive_4': 0.5, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_contrastive_5': 0.5, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_contrastive_6': 0.5, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_contrastive_7': 0.5, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0, 'loss_contrastive_8': 0.5}
      num_classes: 30
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)> to CPU due to CUDA OOM
[10/03 23:32:39] d2.evaluation.evaluator INFO: Inference done 162/267. Dataloading: 0.0032 s/iter. Inference: 0.5654 s/iter. Eval: 3.7026 s/iter. Total: 4.2714 s/iter. ETA=0:07:28
[10/03 23:32:46] d2.evaluation.evaluator INFO: Inference done 164/267. Dataloading: 0.0032 s/iter. Inference: 0.5635 s/iter. Eval: 3.6986 s/iter. Total: 4.2655 s/iter. ETA=0:07:19
[10/03 23:32:55] d2.evaluation.evaluator INFO: Inference done 166/267. Dataloading: 0.0032 s/iter. Inference: 0.5616 s/iter. Eval: 3.6981 s/iter. Total: 4.2631 s/iter. ETA=0:07:10
[10/03 23:33:02] d2.evaluation.evaluator INFO: Inference done 168/267. Dataloading: 0.0032 s/iter. Inference: 0.5595 s/iter. Eval: 3.6951 s/iter. Total: 4.2580 s/iter. ETA=0:07:01
[10/03 23:33:10] d2.evaluation.evaluator INFO: Inference done 170/267. Dataloading: 0.0032 s/iter. Inference: 0.5574 s/iter. Eval: 3.6917 s/iter. Total: 4.2525 s/iter. ETA=0:06:52
[10/03 23:33:18] d2.evaluation.evaluator INFO: Inference done 172/267. Dataloading: 0.0032 s/iter. Inference: 0.5557 s/iter. Eval: 3.6926 s/iter. Total: 4.2516 s/iter. ETA=0:06:43
[10/03 23:33:19] d2.utils.memory INFO: Attempting to copy inputs of <bound method OneFormer.instance_inference of OneFormer(
  (backbone): D2ConvNeXt(
    (downsample_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(3, 256, kernel_size=(4, 4), stride=(4, 4))
        (1): LayerNorm()
      )
      (1): Sequential(
        (0): LayerNorm()
        (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))
      )
      (2): Sequential(
        (0): LayerNorm()
        (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))
      )
      (3): Sequential(
        (0): LayerNorm()
        (1): Conv2d(1024, 2048, kernel_size=(2, 2), stride=(2, 2))
      )
    )
    (stages): ModuleList(
      (0): Sequential(
        (0): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): Identity()
        )
        (1): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): DropPath()
        )
      )
      (1): Sequential(
        (0): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
      )
      (2): Sequential(
        (0): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (3): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (4): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (5): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (6): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (7): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (8): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (9): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (10): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (11): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (12): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (13): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (14): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (15): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (16): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (17): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (18): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (19): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (20): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (21): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (22): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (23): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (24): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (25): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (26): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
      )
      (3): Sequential(
        (0): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
      )
    )
    (norm0): LayerNorm()
    (norm1): LayerNorm()
    (norm2): LayerNorm()
    (norm3): LayerNorm()
  )
  (sem_seg_head): OneFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): ContrastiveMultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (class_transformer): Transformer(
        (encoder): TransformerEncoder(
          (layers): ModuleList()
        )
        (decoder): TransformerDecoder(
          (layers): ModuleList(
            (0-1): 2 x TransformerDecoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (dropout3): Dropout(p=0.1, inplace=False)
            )
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_embed): Embedding(250, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (class_input_proj): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (class_embed): Linear(in_features=256, out_features=31, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (task_mlp): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=77, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
    )
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (text_projector): MLP(
    (layers): ModuleList(
      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
    )
  )
  (prompt_ctx): Embedding(16, 256)
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks', 'contrastive']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_contrastive': 0.5, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_contrastive_0': 0.5, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_contrastive_1': 0.5, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_contrastive_2': 0.5, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_contrastive_3': 0.5, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_contrastive_4': 0.5, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_contrastive_5': 0.5, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_contrastive_6': 0.5, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_contrastive_7': 0.5, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0, 'loss_contrastive_8': 0.5}
      num_classes: 30
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)> to CPU due to CUDA OOM
[10/03 23:33:24] d2.evaluation.evaluator INFO: Inference done 173/267. Dataloading: 0.0032 s/iter. Inference: 0.5672 s/iter. Eval: 3.6917 s/iter. Total: 4.2623 s/iter. ETA=0:06:40
[10/03 23:33:32] d2.evaluation.evaluator INFO: Inference done 175/267. Dataloading: 0.0032 s/iter. Inference: 0.5654 s/iter. Eval: 3.6913 s/iter. Total: 4.2600 s/iter. ETA=0:06:31
[10/03 23:33:40] d2.evaluation.evaluator INFO: Inference done 177/267. Dataloading: 0.0033 s/iter. Inference: 0.5636 s/iter. Eval: 3.6894 s/iter. Total: 4.2564 s/iter. ETA=0:06:23
[10/03 23:33:47] d2.evaluation.evaluator INFO: Inference done 179/267. Dataloading: 0.0033 s/iter. Inference: 0.5616 s/iter. Eval: 3.6816 s/iter. Total: 4.2465 s/iter. ETA=0:06:13
[10/03 23:33:56] d2.evaluation.evaluator INFO: Inference done 181/267. Dataloading: 0.0033 s/iter. Inference: 0.5602 s/iter. Eval: 3.6850 s/iter. Total: 4.2486 s/iter. ETA=0:06:05
[10/03 23:34:05] d2.evaluation.evaluator INFO: Inference done 183/267. Dataloading: 0.0033 s/iter. Inference: 0.5583 s/iter. Eval: 3.6882 s/iter. Total: 4.2499 s/iter. ETA=0:05:56
[10/03 23:34:14] d2.evaluation.evaluator INFO: Inference done 185/267. Dataloading: 0.0033 s/iter. Inference: 0.5564 s/iter. Eval: 3.6961 s/iter. Total: 4.2559 s/iter. ETA=0:05:48
[10/03 23:34:23] d2.evaluation.evaluator INFO: Inference done 187/267. Dataloading: 0.0033 s/iter. Inference: 0.5548 s/iter. Eval: 3.6999 s/iter. Total: 4.2581 s/iter. ETA=0:05:40
[10/03 23:34:24] d2.utils.memory INFO: Attempting to copy inputs of <bound method OneFormer.instance_inference of OneFormer(
  (backbone): D2ConvNeXt(
    (downsample_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(3, 256, kernel_size=(4, 4), stride=(4, 4))
        (1): LayerNorm()
      )
      (1): Sequential(
        (0): LayerNorm()
        (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))
      )
      (2): Sequential(
        (0): LayerNorm()
        (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))
      )
      (3): Sequential(
        (0): LayerNorm()
        (1): Conv2d(1024, 2048, kernel_size=(2, 2), stride=(2, 2))
      )
    )
    (stages): ModuleList(
      (0): Sequential(
        (0): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): Identity()
        )
        (1): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): DropPath()
        )
      )
      (1): Sequential(
        (0): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
      )
      (2): Sequential(
        (0): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (3): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (4): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (5): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (6): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (7): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (8): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (9): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (10): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (11): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (12): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (13): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (14): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (15): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (16): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (17): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (18): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (19): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (20): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (21): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (22): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (23): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (24): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (25): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (26): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
      )
      (3): Sequential(
        (0): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
      )
    )
    (norm0): LayerNorm()
    (norm1): LayerNorm()
    (norm2): LayerNorm()
    (norm3): LayerNorm()
  )
  (sem_seg_head): OneFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): ContrastiveMultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (class_transformer): Transformer(
        (encoder): TransformerEncoder(
          (layers): ModuleList()
        )
        (decoder): TransformerDecoder(
          (layers): ModuleList(
            (0-1): 2 x TransformerDecoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (dropout3): Dropout(p=0.1, inplace=False)
            )
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_embed): Embedding(250, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (class_input_proj): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (class_embed): Linear(in_features=256, out_features=31, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (task_mlp): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=77, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
    )
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (text_projector): MLP(
    (layers): ModuleList(
      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
    )
  )
  (prompt_ctx): Embedding(16, 256)
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks', 'contrastive']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_contrastive': 0.5, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_contrastive_0': 0.5, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_contrastive_1': 0.5, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_contrastive_2': 0.5, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_contrastive_3': 0.5, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_contrastive_4': 0.5, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_contrastive_5': 0.5, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_contrastive_6': 0.5, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_contrastive_7': 0.5, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0, 'loss_contrastive_8': 0.5}
      num_classes: 30
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)> to CPU due to CUDA OOM
[10/03 23:34:29] d2.evaluation.evaluator INFO: Inference done 188/267. Dataloading: 0.0033 s/iter. Inference: 0.5656 s/iter. Eval: 3.6981 s/iter. Total: 4.2671 s/iter. ETA=0:05:37
[10/03 23:34:38] d2.evaluation.evaluator INFO: Inference done 190/267. Dataloading: 0.0033 s/iter. Inference: 0.5639 s/iter. Eval: 3.7001 s/iter. Total: 4.2673 s/iter. ETA=0:05:28
[10/03 23:34:46] d2.evaluation.evaluator INFO: Inference done 192/267. Dataloading: 0.0033 s/iter. Inference: 0.5619 s/iter. Eval: 3.7008 s/iter. Total: 4.2661 s/iter. ETA=0:05:19
[10/03 23:34:54] d2.evaluation.evaluator INFO: Inference done 194/267. Dataloading: 0.0033 s/iter. Inference: 0.5601 s/iter. Eval: 3.6994 s/iter. Total: 4.2629 s/iter. ETA=0:05:11
[10/03 23:35:02] d2.evaluation.evaluator INFO: Inference done 196/267. Dataloading: 0.0033 s/iter. Inference: 0.5583 s/iter. Eval: 3.7004 s/iter. Total: 4.2621 s/iter. ETA=0:05:02
[10/03 23:35:10] d2.evaluation.evaluator INFO: Inference done 198/267. Dataloading: 0.0033 s/iter. Inference: 0.5566 s/iter. Eval: 3.6998 s/iter. Total: 4.2598 s/iter. ETA=0:04:53
[10/03 23:35:19] d2.evaluation.evaluator INFO: Inference done 200/267. Dataloading: 0.0033 s/iter. Inference: 0.5548 s/iter. Eval: 3.7019 s/iter. Total: 4.2601 s/iter. ETA=0:04:45
[10/03 23:35:28] d2.evaluation.evaluator INFO: Inference done 202/267. Dataloading: 0.0033 s/iter. Inference: 0.5531 s/iter. Eval: 3.7055 s/iter. Total: 4.2621 s/iter. ETA=0:04:37
[10/03 23:35:36] d2.evaluation.evaluator INFO: Inference done 204/267. Dataloading: 0.0033 s/iter. Inference: 0.5515 s/iter. Eval: 3.7076 s/iter. Total: 4.2625 s/iter. ETA=0:04:28
[10/03 23:35:45] d2.evaluation.evaluator INFO: Inference done 206/267. Dataloading: 0.0032 s/iter. Inference: 0.5499 s/iter. Eval: 3.7105 s/iter. Total: 4.2638 s/iter. ETA=0:04:20
[10/03 23:35:53] d2.evaluation.evaluator INFO: Inference done 208/267. Dataloading: 0.0032 s/iter. Inference: 0.5483 s/iter. Eval: 3.7097 s/iter. Total: 4.2614 s/iter. ETA=0:04:11
[10/03 23:36:02] d2.evaluation.evaluator INFO: Inference done 210/267. Dataloading: 0.0032 s/iter. Inference: 0.5468 s/iter. Eval: 3.7112 s/iter. Total: 4.2614 s/iter. ETA=0:04:02
[10/03 23:36:11] d2.evaluation.evaluator INFO: Inference done 212/267. Dataloading: 0.0032 s/iter. Inference: 0.5453 s/iter. Eval: 3.7148 s/iter. Total: 4.2634 s/iter. ETA=0:03:54
[10/03 23:36:19] d2.evaluation.evaluator INFO: Inference done 214/267. Dataloading: 0.0032 s/iter. Inference: 0.5438 s/iter. Eval: 3.7155 s/iter. Total: 4.2627 s/iter. ETA=0:03:45
[10/03 23:36:29] d2.evaluation.evaluator INFO: Inference done 216/267. Dataloading: 0.0032 s/iter. Inference: 0.5426 s/iter. Eval: 3.7211 s/iter. Total: 4.2672 s/iter. ETA=0:03:37
[10/03 23:36:34] d2.evaluation.evaluator INFO: Inference done 217/267. Dataloading: 0.0032 s/iter. Inference: 0.5425 s/iter. Eval: 3.7253 s/iter. Total: 4.2711 s/iter. ETA=0:03:33
[10/03 23:36:43] d2.evaluation.evaluator INFO: Inference done 219/267. Dataloading: 0.0032 s/iter. Inference: 0.5418 s/iter. Eval: 3.7276 s/iter. Total: 4.2728 s/iter. ETA=0:03:25
[10/03 23:36:47] d2.utils.memory INFO: Attempting to copy inputs of <bound method OneFormer.instance_inference of OneFormer(
  (backbone): D2ConvNeXt(
    (downsample_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(3, 256, kernel_size=(4, 4), stride=(4, 4))
        (1): LayerNorm()
      )
      (1): Sequential(
        (0): LayerNorm()
        (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))
      )
      (2): Sequential(
        (0): LayerNorm()
        (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))
      )
      (3): Sequential(
        (0): LayerNorm()
        (1): Conv2d(1024, 2048, kernel_size=(2, 2), stride=(2, 2))
      )
    )
    (stages): ModuleList(
      (0): Sequential(
        (0): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): Identity()
        )
        (1): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): DropPath()
        )
      )
      (1): Sequential(
        (0): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
      )
      (2): Sequential(
        (0): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (3): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (4): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (5): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (6): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (7): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (8): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (9): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (10): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (11): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (12): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (13): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (14): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (15): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (16): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (17): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (18): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (19): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (20): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (21): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (22): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (23): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (24): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (25): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (26): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
      )
      (3): Sequential(
        (0): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
      )
    )
    (norm0): LayerNorm()
    (norm1): LayerNorm()
    (norm2): LayerNorm()
    (norm3): LayerNorm()
  )
  (sem_seg_head): OneFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): ContrastiveMultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (class_transformer): Transformer(
        (encoder): TransformerEncoder(
          (layers): ModuleList()
        )
        (decoder): TransformerDecoder(
          (layers): ModuleList(
            (0-1): 2 x TransformerDecoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (dropout3): Dropout(p=0.1, inplace=False)
            )
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_embed): Embedding(250, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (class_input_proj): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (class_embed): Linear(in_features=256, out_features=31, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (task_mlp): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=77, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
    )
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (text_projector): MLP(
    (layers): ModuleList(
      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
    )
  )
  (prompt_ctx): Embedding(16, 256)
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks', 'contrastive']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_contrastive': 0.5, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_contrastive_0': 0.5, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_contrastive_1': 0.5, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_contrastive_2': 0.5, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_contrastive_3': 0.5, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_contrastive_4': 0.5, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_contrastive_5': 0.5, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_contrastive_6': 0.5, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_contrastive_7': 0.5, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0, 'loss_contrastive_8': 0.5}
      num_classes: 30
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)> to CPU due to CUDA OOM
[10/03 23:36:53] d2.evaluation.evaluator INFO: Inference done 221/267. Dataloading: 0.0032 s/iter. Inference: 0.5505 s/iter. Eval: 3.7259 s/iter. Total: 4.2798 s/iter. ETA=0:03:16
[10/03 23:37:01] d2.evaluation.evaluator INFO: Inference done 223/267. Dataloading: 0.0032 s/iter. Inference: 0.5493 s/iter. Eval: 3.7260 s/iter. Total: 4.2786 s/iter. ETA=0:03:08
[10/03 23:37:09] d2.evaluation.evaluator INFO: Inference done 225/267. Dataloading: 0.0032 s/iter. Inference: 0.5478 s/iter. Eval: 3.7268 s/iter. Total: 4.2780 s/iter. ETA=0:02:59
[10/03 23:37:17] d2.evaluation.evaluator INFO: Inference done 227/267. Dataloading: 0.0032 s/iter. Inference: 0.5464 s/iter. Eval: 3.7245 s/iter. Total: 4.2743 s/iter. ETA=0:02:50
[10/03 23:37:25] d2.evaluation.evaluator INFO: Inference done 229/267. Dataloading: 0.0032 s/iter. Inference: 0.5450 s/iter. Eval: 3.7221 s/iter. Total: 4.2705 s/iter. ETA=0:02:42
[10/03 23:37:32] d2.evaluation.evaluator INFO: Inference done 231/267. Dataloading: 0.0032 s/iter. Inference: 0.5437 s/iter. Eval: 3.7186 s/iter. Total: 4.2656 s/iter. ETA=0:02:33
[10/03 23:37:41] d2.evaluation.evaluator INFO: Inference done 233/267. Dataloading: 0.0032 s/iter. Inference: 0.5425 s/iter. Eval: 3.7195 s/iter. Total: 4.2654 s/iter. ETA=0:02:25
[10/03 23:37:49] d2.evaluation.evaluator INFO: Inference done 235/267. Dataloading: 0.0032 s/iter. Inference: 0.5414 s/iter. Eval: 3.7214 s/iter. Total: 4.2662 s/iter. ETA=0:02:16
[10/03 23:37:58] d2.evaluation.evaluator INFO: Inference done 237/267. Dataloading: 0.0032 s/iter. Inference: 0.5403 s/iter. Eval: 3.7211 s/iter. Total: 4.2647 s/iter. ETA=0:02:07
[10/03 23:38:06] d2.evaluation.evaluator INFO: Inference done 239/267. Dataloading: 0.0032 s/iter. Inference: 0.5392 s/iter. Eval: 3.7238 s/iter. Total: 4.2664 s/iter. ETA=0:01:59
[10/03 23:38:15] d2.evaluation.evaluator INFO: Inference done 241/267. Dataloading: 0.0032 s/iter. Inference: 0.5379 s/iter. Eval: 3.7254 s/iter. Total: 4.2667 s/iter. ETA=0:01:50
[10/03 23:38:24] d2.evaluation.evaluator INFO: Inference done 243/267. Dataloading: 0.0032 s/iter. Inference: 0.5368 s/iter. Eval: 3.7283 s/iter. Total: 4.2684 s/iter. ETA=0:01:42
[10/03 23:38:31] d2.evaluation.evaluator INFO: Inference done 245/267. Dataloading: 0.0032 s/iter. Inference: 0.5355 s/iter. Eval: 3.7222 s/iter. Total: 4.2611 s/iter. ETA=0:01:33
[10/03 23:38:39] d2.evaluation.evaluator INFO: Inference done 247/267. Dataloading: 0.0032 s/iter. Inference: 0.5344 s/iter. Eval: 3.7225 s/iter. Total: 4.2603 s/iter. ETA=0:01:25
[10/03 23:38:46] d2.evaluation.evaluator INFO: Inference done 249/267. Dataloading: 0.0032 s/iter. Inference: 0.5333 s/iter. Eval: 3.7185 s/iter. Total: 4.2551 s/iter. ETA=0:01:16
[10/03 23:38:55] d2.evaluation.evaluator INFO: Inference done 251/267. Dataloading: 0.0032 s/iter. Inference: 0.5324 s/iter. Eval: 3.7204 s/iter. Total: 4.2562 s/iter. ETA=0:01:08
[10/03 23:39:04] d2.evaluation.evaluator INFO: Inference done 253/267. Dataloading: 0.0032 s/iter. Inference: 0.5321 s/iter. Eval: 3.7220 s/iter. Total: 4.2575 s/iter. ETA=0:00:59
[10/03 23:39:13] d2.evaluation.evaluator INFO: Inference done 255/267. Dataloading: 0.0032 s/iter. Inference: 0.5313 s/iter. Eval: 3.7239 s/iter. Total: 4.2586 s/iter. ETA=0:00:51
[10/03 23:39:21] d2.evaluation.evaluator INFO: Inference done 257/267. Dataloading: 0.0032 s/iter. Inference: 0.5302 s/iter. Eval: 3.7250 s/iter. Total: 4.2585 s/iter. ETA=0:00:42
[10/03 23:39:29] d2.evaluation.evaluator INFO: Inference done 259/267. Dataloading: 0.0032 s/iter. Inference: 0.5291 s/iter. Eval: 3.7227 s/iter. Total: 4.2551 s/iter. ETA=0:00:34
[10/03 23:39:38] d2.evaluation.evaluator INFO: Inference done 261/267. Dataloading: 0.0032 s/iter. Inference: 0.5284 s/iter. Eval: 3.7248 s/iter. Total: 4.2565 s/iter. ETA=0:00:25
[10/03 23:39:46] d2.evaluation.evaluator INFO: Inference done 263/267. Dataloading: 0.0032 s/iter. Inference: 0.5273 s/iter. Eval: 3.7247 s/iter. Total: 4.2553 s/iter. ETA=0:00:17
[10/03 23:39:55] d2.evaluation.evaluator INFO: Inference done 265/267. Dataloading: 0.0032 s/iter. Inference: 0.5263 s/iter. Eval: 3.7263 s/iter. Total: 4.2560 s/iter. ETA=0:00:08
[10/03 23:40:04] d2.evaluation.evaluator INFO: Inference done 267/267. Dataloading: 0.0032 s/iter. Inference: 0.5253 s/iter. Eval: 3.7290 s/iter. Total: 4.2577 s/iter. ETA=0:00:00
[10/03 23:40:04] d2.evaluation.evaluator INFO: Total inference time: 0:18:35.596498 (4.258002 s / iter per device, on 1 devices)
[10/03 23:40:04] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:02:17 (0.525305 s / iter per device, on 1 devices)
[10/03 23:40:04] d2.evaluation.panoptic_evaluation INFO: Writing all panoptic predictions to /tmp/panoptic_eval1fdfomnw ...
[10/03 23:40:11] d2.evaluation.panoptic_evaluation INFO: Panoptic Evaluation Results:
|        |   PQ   |   SQ   |   RQ   |  #categories  |
|:------:|:------:|:------:|:------:|:-------------:|
|  All   | 32.437 | 51.779 | 39.724 |      30       |
| Things | 14.115 | 34.101 | 18.034 |      19       |
| Stuff  | 64.085 | 82.314 | 77.187 |      11       |
[10/03 23:40:11] d2.evaluation.cityscapes_evaluation INFO: Evaluating results under /tmp/cityscapes_eval_xwgbfp_l ...
[10/03 23:40:47] oneformer.evaluation.cityscapes_evaluation INFO: Evaluating results under /tmp/cityscapes_eval_srbqw33h ...
[10/03 23:41:19] d2.engine.hooks INFO: Overall training speed: 4997 iterations in 0:26:40 (0.3203 s / it)
[10/03 23:41:19] d2.engine.hooks INFO: Total training time: 0:46:59 (0:20:18 on hooks)
[10/03 23:41:19] d2.utils.events INFO:  eta: 0:26:23  iter: 4999  total_loss: 22.15  loss_ce: 0.4248  loss_mask: 0.2549  loss_dice: 1.028  loss_contrastive: 0  loss_ce_0: 1.367  loss_mask_0: 0.3343  loss_dice_0: 1.298  loss_ce_1: 0.8164  loss_mask_1: 0.3445  loss_dice_1: 1.168  loss_ce_2: 0.6923  loss_mask_2: 0.3615  loss_dice_2: 1.139  loss_ce_3: 0.5834  loss_mask_3: 0.3178  loss_dice_3: 1.162  loss_ce_4: 0.5549  loss_mask_4: 0.2993  loss_dice_4: 1.121  loss_ce_5: 0.4843  loss_mask_5: 0.2794  loss_dice_5: 1.069  loss_ce_6: 0.4199  loss_mask_6: 0.2937  loss_dice_6: 0.9779  loss_ce_7: 0.4957  loss_mask_7: 0.2921  loss_dice_7: 0.9755  loss_ce_8: 0.4072  loss_mask_8: 0.2703  loss_dice_8: 0.9806    time: 0.3203  last_time: 0.3404  data_time: 0.0034  last_data_time: 0.0032   lr: 5.3598e-05  max_mem: 19509M
[12/04 23:41:51] detectron2 INFO: Rank of current process: 0. World size: 1
[12/04 23:41:52] detectron2 INFO: Environment info:
-------------------------------  -------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0]
numpy                            1.23.1
detectron2                       0.6 @/OneFormer/detectron2/detectron2
Compiler                         GCC 9.4
CUDA compiler                    CUDA 11.7
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.0.0 @/opt/miniconda3/envs/oneformer/lib/python3.10/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 4090 (arch=8.9)
Driver version                   530.41.03
CUDA_HOME                        /usr/local/cuda
Pillow                           9.4.0
torchvision                      0.15.0 @/opt/miniconda3/envs/oneformer/lib/python3.10/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.8.0
-------------------------------  -------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.5
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[12/04 23:41:52] detectron2 INFO: Command line arguments: Namespace(config_file='configs/cityscapes/convnext/mapillary_pretrain_oneformer_convnext_xlarge_bs16_90k.yaml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50165', opts=['OUTPUT_DIR', 'outputs/ceymo2', 'WANDB.NAME', 'ceymo'])
[12/04 23:41:52] detectron2 INFO: Contents of args.config_file=configs/cityscapes/convnext/mapillary_pretrain_oneformer_convnext_xlarge_bs16_90k.yaml:
[38;5;204m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m../oneformer_R50_bs16_90k.yaml[39m
[38;5;204mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;204mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mD2ConvNeXt[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;204mCONVNEXT[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mIN_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;204mDEPTHS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m3[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m3[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m27[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m3[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;204mDIMS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m256[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m512[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1024[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m2048[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;204mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;204mLSIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;204mOUT_INDICES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m0[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m1[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m2[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m3[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;204mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186m/mnt/source/OneFormer/mapillary_pretrain_250_16_convnext_xl_oneformer_mapillary_300k.pth[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;204mPIXEL_MEAN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m123.675[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m116.280[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m103.530[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;204mPIXEL_STD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m58.395[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m57.120[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15m57.375[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;204mONE_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m250[39m
[38;5;204mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;204mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m250[39m

[12/04 23:41:52] detectron2 INFO: Running with full config:
[38;5;204mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;204mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;204mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;204mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;204mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;204mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;204mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;204mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;204mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;204mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;204mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;204mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;204mTEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;204mTEST_INSTANCE[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mcityscapes_fine_instance_seg_val[39m
[38;5;15m  [39m[38;5;204mTEST_PANOPTIC[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mcityscapes_fine_panoptic_val[39m
[38;5;15m  [39m[38;5;204mTEST_SEMANTIC[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mcityscapes_fine_sem_seg_val[39m
[38;5;15m  [39m[38;5;204mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mcityscapes_fine_panoptic_train[39m
[38;5;204mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;204mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;204mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;204mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;204mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;204mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;204mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;204mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mabsolute[39m
[38;5;15m  [39m[38;5;204mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141moneformer_unified[39m
[38;5;15m  [39m[38;5;204mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;204mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;204mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;204mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;204mMAX_SEQ_LEN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m77[39m
[38;5;15m  [39m[38;5;204mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;204mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m  [39m[38;5;204mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;204mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;204mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m614[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m716[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m819[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m921[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1126[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1228[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1331[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1433[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1638[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1740[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1843[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1945[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m  [39m[38;5;204mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;204mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;204mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m  [39m[38;5;204mTASK_PROB[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mINSTANCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.66[39m
[38;5;15m    [39m[38;5;204mSEMANTIC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.33[39m
[38;5;15m  [39m[38;5;204mTASK_SEQ_LEN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m77[39m
[38;5;204mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;204mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;204mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;204mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;204mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;204mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;204mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;204mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mD2ConvNeXt[39m
[38;5;15m  [39m[38;5;204mCONVNEXT[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m27[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;204mDIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;204mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;204mIN_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;204mLSIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;204mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;204mOUT_INDICES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m  [39m[38;5;204mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;204mDiNAT[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;204mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5[39m
[38;5;15m    [39m[38;5;204mDILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;204mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.2[39m
[38;5;15m    [39m[38;5;204mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;204mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;204mIN_PATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;204mKERNEL_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m    [39m[38;5;204mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;204mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m    [39m[38;5;204mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;204mOUT_INDICES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;204mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;204mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;204mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;204mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;204mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;204mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;204mIS_DEMO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;204mIS_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;204mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;204mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;204mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;204mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mOneFormer[39m
[38;5;15m  [39m[38;5;204mONE_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mCLASS_DEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;204mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;204mCONTRASTIVE_TEMPERATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.07[39m
[38;5;15m    [39m[38;5;204mCONTRASTIVE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;204mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;204mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;204mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;204mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;204mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;204mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;204mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;204mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;204mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;204mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;204mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;204mNUM_OBJECT_CTX[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m    [39m[38;5;204mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m250[39m
[38;5;15m    [39m[38;5;204mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;204mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;204mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;204mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mContrastiveMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;204mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m    [39m[38;5;204mUSE_TASK_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;204mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;204mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;204mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;204mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;204mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;204mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;204mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;204mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;204mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;204mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;204mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;204mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;15m    [39m[38;5;204mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mSyncBN[39m
[38;5;15m    [39m[38;5;204mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;204mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;204mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;204mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;204mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;204mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;204mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;204mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;204mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;204mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;204mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m&id002[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;204mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;204mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;204mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;204mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;204mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;204mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;204mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;204mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;204mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;204mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;204mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;204mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;204mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;204mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m&id001[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;204mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;204mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;204mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;204mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id001[39m
[38;5;15m    [39m[38;5;204mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;204mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;204mFED_LOSS_FREQ_WEIGHT_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;204mFED_LOSS_NUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;15m    [39m[38;5;204mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;204mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;204mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;204mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;204mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;204mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;204mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;204mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;204mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mUSE_FED_LOSS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mUSE_SIGMOID_CE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;204mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;204mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;204mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;204mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;204mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;204mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;204mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;204mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;204mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;204mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;204mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;204mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;204mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;204mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;204mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;204mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;204mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;204mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;204mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;204mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;204mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;204mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;204mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;204mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;204mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;204mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;204mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;204mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;204mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;204mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id002[39m
[38;5;15m    [39m[38;5;204mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;204mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;204mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;204mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;204mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;204mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;204mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;204mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;204mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;204mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;204mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;204mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;204mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;204mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;204mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;204mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;204mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;204mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;204mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;204mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;204mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;204mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;204mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m255[39m
[38;5;15m    [39m[38;5;204mINST_EMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;204mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;204mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;204mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;204mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;204mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mOneFormerHead[39m
[38;5;15m    [39m[38;5;204mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;204mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m30[39m
[38;5;15m    [39m[38;5;204mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;204mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;204mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;204mSEM_EMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;204mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;204mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;204mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;204mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;204mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;204mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;204mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;204mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;204mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;204mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;204mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;204mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;204mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;204mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;204mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;204mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;204mTEST[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mDETECTION_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;204mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m    [39m[38;5;204mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m    [39m[38;5;204mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;204mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;204mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mTASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpanoptic[39m
[38;5;15m  [39m[38;5;204mTEXT_ENCODER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mCONTEXT_LENGTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m77[39m
[38;5;15m    [39m[38;5;204mNUM_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;204mN_CTX[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15m    [39m[38;5;204mPROJ_NUM_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;204mVOCAB_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m49408[39m
[38;5;15m    [39m[38;5;204mWIDTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;204mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/mnt/source/OneFormer/mapillary_pretrain_250_16_convnext_xl_oneformer_mapillary_300k.pth[39m
[38;5;204mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141moutputs/ceymo2[39m
[38;5;204mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;204mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;204mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;204mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;204mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;204mBASE_LR_END[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;204mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;204mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;204mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;204mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;204mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;204mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;204mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;204mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m  [39m[38;5;204mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupPolyLR[39m
[38;5;15m  [39m[38;5;204mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10000[39m
[38;5;15m  [39m[38;5;204mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;204mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;204mNUM_DECAYS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m  [39m[38;5;204mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;204mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;204mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;204mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;204mRESCALE_INTERVAL[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;204mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;204mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;204mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;204mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;204mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;204mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;204mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;204mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;204mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;204mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mCROP_SIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m640[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m640[39m
[38;5;15m    [39m[38;5;204mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;204mIS_SLIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mKEEP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;204mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;204mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1280[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1792[39m
[38;5;15m    [39m[38;5;204mSCALE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m640[39m
[38;5;15m    [39m[38;5;204mSETR_MULTI_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;204mSIZE_DIVISOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;204mSTRIDE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m426[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m426[39m
[38;5;15m  [39m[38;5;204mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m250[39m
[38;5;15m  [39m[38;5;204mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;204mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;204mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;204mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;204mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;204mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;204mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;204mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;204mWANDB[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;204mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mceymo[39m
[38;5;15m  [39m[38;5;204mPROJECT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mOneFormer[39m

[12/04 23:41:52] detectron2 INFO: Full config saved to outputs/ceymo2/config.yaml
[12/04 23:41:52] d2.utils.env INFO: Using a generated random seed 52266317
[12/04 23:41:59] d2.engine.defaults INFO: Model:
OneFormer(
  (backbone): D2ConvNeXt(
    (downsample_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(3, 256, kernel_size=(4, 4), stride=(4, 4))
        (1): LayerNorm()
      )
      (1): Sequential(
        (0): LayerNorm()
        (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))
      )
      (2): Sequential(
        (0): LayerNorm()
        (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))
      )
      (3): Sequential(
        (0): LayerNorm()
        (1): Conv2d(1024, 2048, kernel_size=(2, 2), stride=(2, 2))
      )
    )
    (stages): ModuleList(
      (0): Sequential(
        (0): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): Identity()
        )
        (1): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): DropPath()
        )
      )
      (1): Sequential(
        (0): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
      )
      (2): Sequential(
        (0): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (3): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (4): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (5): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (6): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (7): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (8): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (9): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (10): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (11): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (12): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (13): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (14): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (15): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (16): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (17): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (18): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (19): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (20): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (21): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (22): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (23): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (24): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (25): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (26): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
      )
      (3): Sequential(
        (0): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
      )
    )
    (norm0): LayerNorm()
    (norm1): LayerNorm()
    (norm2): LayerNorm()
    (norm3): LayerNorm()
  )
  (sem_seg_head): OneFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): ContrastiveMultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (class_transformer): Transformer(
        (encoder): TransformerEncoder(
          (layers): ModuleList()
        )
        (decoder): TransformerDecoder(
          (layers): ModuleList(
            (0-1): 2 x TransformerDecoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (dropout3): Dropout(p=0.1, inplace=False)
            )
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_embed): Embedding(250, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (class_input_proj): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (class_embed): Linear(in_features=256, out_features=31, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (task_mlp): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=77, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
    )
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (text_projector): MLP(
    (layers): ModuleList(
      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
    )
  )
  (prompt_ctx): Embedding(16, 256)
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks', 'contrastive']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_contrastive': 0.5, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_contrastive_0': 0.5, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_contrastive_1': 0.5, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_contrastive_2': 0.5, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_contrastive_3': 0.5, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_contrastive_4': 0.5, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_contrastive_5': 0.5, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_contrastive_6': 0.5, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_contrastive_7': 0.5, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0, 'loss_contrastive_8': 0.5}
      num_classes: 30
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[12/04 23:41:59] oneformer.data.dataset_mappers.oneformer_unified_dataset_mapper INFO: [OneFormerUnifiedDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=..., max_size=4096, sample_style='choice'), RandomCrop_CategoryAreaConstraint(crop_type='absolute', crop_size=[512, 1024], single_category_max_area=1.0, ignored_category=255), <detectron2.projects.point_rend.color_augmentation.ColorAugSSDTransform object at 0x7fa49a8eba00>, RandomFlip()]
[12/04 23:41:59] oneformer.data.datasets.register_cityscapes_panoptic INFO: 2 cities found in '/mnt/source/datasets/cityscapes/leftImg8bit/train'.
[12/04 23:41:59] d2.data.build INFO: Using training sampler TrainingSampler
[12/04 23:41:59] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[12/04 23:41:59] d2.data.common INFO: Serializing 181 elements to byte tensors and concatenating them all ...
[12/04 23:41:59] d2.data.common INFO: Serialized dataset takes 0.26 MiB
[12/04 23:41:59] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /mnt/source/OneFormer/mapillary_pretrain_250_16_convnext_xl_oneformer_mapillary_300k.pth ...
[12/04 23:41:59] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /mnt/source/OneFormer/mapillary_pretrain_250_16_convnext_xl_oneformer_mapillary_300k.pth ...
[12/04 23:42:00] fvcore.common.checkpoint WARNING: Skip loading parameter 'sem_seg_head.predictor.class_embed.weight' to the model due to incompatible shapes: (66, 256) in the checkpoint but (31, 256) in the model! You might want to double check if this is expected.
[12/04 23:42:00] fvcore.common.checkpoint WARNING: Skip loading parameter 'sem_seg_head.predictor.class_embed.bias' to the model due to incompatible shapes: (66,) in the checkpoint but (31,) in the model! You might want to double check if this is expected.
[12/04 23:42:00] fvcore.common.checkpoint WARNING: Skip loading parameter 'criterion.empty_weight' to the model due to incompatible shapes: (66,) in the checkpoint but (31,) in the model! You might want to double check if this is expected.
[12/04 23:42:00] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mcriterion.empty_weight[0m
[34msem_seg_head.predictor.class_embed.{bias, weight}[0m
[12/04 23:42:03] d2.engine.train_loop INFO: Starting training from iteration 0
[12/04 23:42:12] d2.utils.events INFO:  eta: 0:51:10  iter: 19  total_loss: 68.71  loss_ce: 5.52  loss_mask: 0.3893  loss_dice: 1.093  loss_contrastive: 0  loss_ce_0: 7.292  loss_mask_0: 0.5422  loss_dice_0: 1.214  loss_ce_1: 6.628  loss_mask_1: 0.4828  loss_dice_1: 1.135  loss_ce_2: 6.567  loss_mask_2: 0.394  loss_dice_2: 1.028  loss_ce_3: 6.204  loss_mask_3: 0.3974  loss_dice_3: 1.086  loss_ce_4: 5.187  loss_mask_4: 0.4218  loss_dice_4: 1.127  loss_ce_5: 4.976  loss_mask_5: 0.4298  loss_dice_5: 1.07  loss_ce_6: 5.084  loss_mask_6: 0.4503  loss_dice_6: 1.028  loss_ce_7: 4.81  loss_mask_7: 0.4366  loss_dice_7: 1.01  loss_ce_8: 4.927  loss_mask_8: 0.4222  loss_dice_8: 1.034    time: 0.3054  last_time: 0.3093  data_time: 0.0306  last_data_time: 0.0029   lr: 9.9829e-05  max_mem: 14538M
[12/04 23:42:19] d2.utils.events INFO:  eta: 0:51:21  iter: 39  total_loss: 44.25  loss_ce: 2.435  loss_mask: 0.4068  loss_dice: 1.297  loss_contrastive: 0  loss_ce_0: 3.088  loss_mask_0: 0.4633  loss_dice_0: 1.81  loss_ce_1: 2.805  loss_mask_1: 0.4613  loss_dice_1: 1.527  loss_ce_2: 2.643  loss_mask_2: 0.4236  loss_dice_2: 1.436  loss_ce_3: 2.586  loss_mask_3: 0.4544  loss_dice_3: 1.364  loss_ce_4: 2.649  loss_mask_4: 0.3974  loss_dice_4: 1.353  loss_ce_5: 2.542  loss_mask_5: 0.4337  loss_dice_5: 1.367  loss_ce_6: 2.362  loss_mask_6: 0.4214  loss_dice_6: 1.211  loss_ce_7: 2.388  loss_mask_7: 0.4041  loss_dice_7: 1.293  loss_ce_8: 2.402  loss_mask_8: 0.4608  loss_dice_8: 1.281    time: 0.3130  last_time: 0.3072  data_time: 0.0033  last_data_time: 0.0032   lr: 9.9649e-05  max_mem: 14538M
[12/04 23:42:25] d2.utils.events INFO:  eta: 0:51:09  iter: 59  total_loss: 35.77  loss_ce: 1.089  loss_mask: 0.4572  loss_dice: 0.8982  loss_contrastive: 0  loss_ce_0: 2.101  loss_mask_0: 0.4574  loss_dice_0: 1.481  loss_ce_1: 1.883  loss_mask_1: 0.4665  loss_dice_1: 1.054  loss_ce_2: 1.759  loss_mask_2: 0.5531  loss_dice_2: 1.115  loss_ce_3: 1.587  loss_mask_3: 0.5353  loss_dice_3: 1.013  loss_ce_4: 1.425  loss_mask_4: 0.4825  loss_dice_4: 1.063  loss_ce_5: 1.382  loss_mask_5: 0.5519  loss_dice_5: 0.9407  loss_ce_6: 1.176  loss_mask_6: 0.5184  loss_dice_6: 0.9969  loss_ce_7: 1.124  loss_mask_7: 0.5184  loss_dice_7: 0.9302  loss_ce_8: 1.181  loss_mask_8: 0.4894  loss_dice_8: 0.9757    time: 0.3119  last_time: 0.2555  data_time: 0.0033  last_data_time: 0.0029   lr: 9.9469e-05  max_mem: 14538M
[12/04 23:42:32] d2.utils.events INFO:  eta: 0:51:18  iter: 79  total_loss: 36.43  loss_ce: 1.208  loss_mask: 0.5817  loss_dice: 1.302  loss_contrastive: 0  loss_ce_0: 2.529  loss_mask_0: 0.5715  loss_dice_0: 1.91  loss_ce_1: 2.308  loss_mask_1: 0.4982  loss_dice_1: 1.354  loss_ce_2: 2.058  loss_mask_2: 0.5384  loss_dice_2: 1.3  loss_ce_3: 1.744  loss_mask_3: 0.5095  loss_dice_3: 1.164  loss_ce_4: 1.566  loss_mask_4: 0.5111  loss_dice_4: 1.158  loss_ce_5: 1.439  loss_mask_5: 0.5941  loss_dice_5: 1.274  loss_ce_6: 1.368  loss_mask_6: 0.6262  loss_dice_6: 1.107  loss_ce_7: 1.271  loss_mask_7: 0.5686  loss_dice_7: 1.114  loss_ce_8: 1.144  loss_mask_8: 0.547  loss_dice_8: 1.225    time: 0.3179  last_time: 0.3089  data_time: 0.0033  last_data_time: 0.0025   lr: 9.9289e-05  max_mem: 14557M
[12/04 23:42:38] d2.utils.events INFO:  eta: 0:51:19  iter: 99  total_loss: 30.64  loss_ce: 1.079  loss_mask: 0.5187  loss_dice: 1.124  loss_contrastive: 0  loss_ce_0: 2.511  loss_mask_0: 0.5393  loss_dice_0: 1.657  loss_ce_1: 1.977  loss_mask_1: 0.5285  loss_dice_1: 1.115  loss_ce_2: 1.783  loss_mask_2: 0.5122  loss_dice_2: 1.112  loss_ce_3: 1.584  loss_mask_3: 0.5195  loss_dice_3: 1.111  loss_ce_4: 1.407  loss_mask_4: 0.4621  loss_dice_4: 0.9054  loss_ce_5: 1.291  loss_mask_5: 0.5066  loss_dice_5: 1.075  loss_ce_6: 1.164  loss_mask_6: 0.4587  loss_dice_6: 1.064  loss_ce_7: 1.167  loss_mask_7: 0.5017  loss_dice_7: 0.9354  loss_ce_8: 1.101  loss_mask_8: 0.5056  loss_dice_8: 1.062    time: 0.3183  last_time: 0.3022  data_time: 0.0031  last_data_time: 0.0033   lr: 9.9109e-05  max_mem: 14557M
[12/04 23:42:45] d2.utils.events INFO:  eta: 0:51:26  iter: 119  total_loss: 31.56  loss_ce: 1.155  loss_mask: 0.3979  loss_dice: 1.181  loss_contrastive: 0  loss_ce_0: 1.7  loss_mask_0: 0.5311  loss_dice_0: 1.625  loss_ce_1: 1.528  loss_mask_1: 0.3755  loss_dice_1: 1.214  loss_ce_2: 1.463  loss_mask_2: 0.3709  loss_dice_2: 1.146  loss_ce_3: 1.331  loss_mask_3: 0.3922  loss_dice_3: 1.049  loss_ce_4: 1.314  loss_mask_4: 0.3755  loss_dice_4: 0.9483  loss_ce_5: 1.215  loss_mask_5: 0.3623  loss_dice_5: 0.9106  loss_ce_6: 1.164  loss_mask_6: 0.3513  loss_dice_6: 0.9403  loss_ce_7: 1.032  loss_mask_7: 0.3651  loss_dice_7: 1.025  loss_ce_8: 0.9906  loss_mask_8: 0.3939  loss_dice_8: 1.041    time: 0.3193  last_time: 0.3106  data_time: 0.0030  last_data_time: 0.0031   lr: 9.8928e-05  max_mem: 14620M
[12/04 23:42:51] d2.utils.events INFO:  eta: 0:51:29  iter: 139  total_loss: 37.11  loss_ce: 1.362  loss_mask: 0.5569  loss_dice: 1.249  loss_contrastive: 0  loss_ce_0: 2.716  loss_mask_0: 0.5311  loss_dice_0: 1.738  loss_ce_1: 2.358  loss_mask_1: 0.5079  loss_dice_1: 1.339  loss_ce_2: 1.924  loss_mask_2: 0.5278  loss_dice_2: 1.273  loss_ce_3: 1.767  loss_mask_3: 0.5005  loss_dice_3: 1.217  loss_ce_4: 1.58  loss_mask_4: 0.4341  loss_dice_4: 1.06  loss_ce_5: 1.5  loss_mask_5: 0.4706  loss_dice_5: 1.208  loss_ce_6: 1.41  loss_mask_6: 0.5319  loss_dice_6: 1.236  loss_ce_7: 1.518  loss_mask_7: 0.5151  loss_dice_7: 1.273  loss_ce_8: 1.302  loss_mask_8: 0.471  loss_dice_8: 1.384    time: 0.3197  last_time: 0.3096  data_time: 0.0039  last_data_time: 0.0033   lr: 9.8748e-05  max_mem: 14620M
[12/04 23:42:58] d2.utils.events INFO:  eta: 0:51:23  iter: 159  total_loss: 33.88  loss_ce: 0.997  loss_mask: 0.3826  loss_dice: 1.383  loss_contrastive: 0  loss_ce_0: 2.774  loss_mask_0: 0.4798  loss_dice_0: 2.065  loss_ce_1: 1.918  loss_mask_1: 0.4431  loss_dice_1: 1.53  loss_ce_2: 1.641  loss_mask_2: 0.4077  loss_dice_2: 1.51  loss_ce_3: 1.444  loss_mask_3: 0.347  loss_dice_3: 1.431  loss_ce_4: 1.184  loss_mask_4: 0.4046  loss_dice_4: 1.364  loss_ce_5: 1.176  loss_mask_5: 0.397  loss_dice_5: 1.316  loss_ce_6: 1.116  loss_mask_6: 0.3428  loss_dice_6: 1.331  loss_ce_7: 1.136  loss_mask_7: 0.3308  loss_dice_7: 1.361  loss_ce_8: 0.9758  loss_mask_8: 0.3698  loss_dice_8: 1.424    time: 0.3198  last_time: 0.3121  data_time: 0.0036  last_data_time: 0.0032   lr: 9.8568e-05  max_mem: 14620M
[12/04 23:43:04] d2.utils.events INFO:  eta: 0:51:20  iter: 179  total_loss: 34.24  loss_ce: 0.9275  loss_mask: 0.4122  loss_dice: 1.143  loss_contrastive: 0  loss_ce_0: 2.564  loss_mask_0: 0.4626  loss_dice_0: 1.738  loss_ce_1: 1.712  loss_mask_1: 0.4813  loss_dice_1: 1.451  loss_ce_2: 1.383  loss_mask_2: 0.5001  loss_dice_2: 1.309  loss_ce_3: 1.266  loss_mask_3: 0.4555  loss_dice_3: 1.219  loss_ce_4: 1.174  loss_mask_4: 0.4037  loss_dice_4: 1.178  loss_ce_5: 1.127  loss_mask_5: 0.3542  loss_dice_5: 1.246  loss_ce_6: 0.973  loss_mask_6: 0.478  loss_dice_6: 1.183  loss_ce_7: 0.8942  loss_mask_7: 0.4819  loss_dice_7: 1.268  loss_ce_8: 0.9322  loss_mask_8: 0.4214  loss_dice_8: 1.096    time: 0.3203  last_time: 0.3463  data_time: 0.0038  last_data_time: 0.0093   lr: 9.8388e-05  max_mem: 14620M
[12/04 23:43:11] d2.utils.events INFO:  eta: 0:51:27  iter: 199  total_loss: 27.55  loss_ce: 0.7654  loss_mask: 0.3739  loss_dice: 1.288  loss_contrastive: 0  loss_ce_0: 1.95  loss_mask_0: 0.4835  loss_dice_0: 1.855  loss_ce_1: 1.371  loss_mask_1: 0.4023  loss_dice_1: 1.359  loss_ce_2: 1.108  loss_mask_2: 0.3689  loss_dice_2: 1.373  loss_ce_3: 1.043  loss_mask_3: 0.3729  loss_dice_3: 1.296  loss_ce_4: 1.057  loss_mask_4: 0.413  loss_dice_4: 1.276  loss_ce_5: 1.089  loss_mask_5: 0.3987  loss_dice_5: 1.242  loss_ce_6: 0.9365  loss_mask_6: 0.4112  loss_dice_6: 1.148  loss_ce_7: 0.887  loss_mask_7: 0.3484  loss_dice_7: 1.367  loss_ce_8: 0.84  loss_mask_8: 0.3438  loss_dice_8: 1.313    time: 0.3215  last_time: 0.3203  data_time: 0.0035  last_data_time: 0.0032   lr: 9.8207e-05  max_mem: 14629M
[12/04 23:43:17] d2.utils.events INFO:  eta: 0:51:21  iter: 219  total_loss: 26.1  loss_ce: 0.7204  loss_mask: 0.4594  loss_dice: 0.9541  loss_contrastive: 0  loss_ce_0: 2.35  loss_mask_0: 0.6336  loss_dice_0: 1.676  loss_ce_1: 1.415  loss_mask_1: 0.375  loss_dice_1: 1.102  loss_ce_2: 1.067  loss_mask_2: 0.392  loss_dice_2: 0.9435  loss_ce_3: 1.102  loss_mask_3: 0.335  loss_dice_3: 1.117  loss_ce_4: 0.7901  loss_mask_4: 0.3732  loss_dice_4: 1.037  loss_ce_5: 0.6803  loss_mask_5: 0.3841  loss_dice_5: 1.055  loss_ce_6: 0.7066  loss_mask_6: 0.4756  loss_dice_6: 0.9724  loss_ce_7: 0.739  loss_mask_7: 0.47  loss_dice_7: 0.9955  loss_ce_8: 0.7392  loss_mask_8: 0.472  loss_dice_8: 1.059    time: 0.3214  last_time: 0.3271  data_time: 0.0036  last_data_time: 0.0024   lr: 9.8027e-05  max_mem: 14629M
[12/04 23:43:23] d2.utils.events INFO:  eta: 0:51:17  iter: 239  total_loss: 25.14  loss_ce: 0.5177  loss_mask: 0.3819  loss_dice: 1.057  loss_contrastive: 0  loss_ce_0: 2.299  loss_mask_0: 0.6585  loss_dice_0: 1.842  loss_ce_1: 1.217  loss_mask_1: 0.3256  loss_dice_1: 1.127  loss_ce_2: 1.139  loss_mask_2: 0.2994  loss_dice_2: 1.086  loss_ce_3: 0.9119  loss_mask_3: 0.2531  loss_dice_3: 1.138  loss_ce_4: 0.7926  loss_mask_4: 0.2482  loss_dice_4: 1.024  loss_ce_5: 0.7789  loss_mask_5: 0.3364  loss_dice_5: 1.023  loss_ce_6: 0.6828  loss_mask_6: 0.3107  loss_dice_6: 1.002  loss_ce_7: 0.6265  loss_mask_7: 0.3731  loss_dice_7: 0.9896  loss_ce_8: 0.5477  loss_mask_8: 0.3938  loss_dice_8: 1.062    time: 0.3211  last_time: 0.3221  data_time: 0.0032  last_data_time: 0.0024   lr: 9.7846e-05  max_mem: 14629M
[12/04 23:43:30] d2.utils.events INFO:  eta: 0:51:17  iter: 259  total_loss: 39.14  loss_ce: 0.9698  loss_mask: 0.4763  loss_dice: 1.255  loss_contrastive: 0  loss_ce_0: 2.789  loss_mask_0: 1.054  loss_dice_0: 2.938  loss_ce_1: 1.606  loss_mask_1: 0.5282  loss_dice_1: 1.702  loss_ce_2: 1.26  loss_mask_2: 0.474  loss_dice_2: 1.404  loss_ce_3: 1.292  loss_mask_3: 0.4379  loss_dice_3: 1.346  loss_ce_4: 1.102  loss_mask_4: 0.5326  loss_dice_4: 1.375  loss_ce_5: 1.176  loss_mask_5: 0.4477  loss_dice_5: 1.392  loss_ce_6: 1.24  loss_mask_6: 0.4552  loss_dice_6: 1.318  loss_ce_7: 1.195  loss_mask_7: 0.4637  loss_dice_7: 1.226  loss_ce_8: 1.048  loss_mask_8: 0.478  loss_dice_8: 1.199    time: 0.3217  last_time: 0.3382  data_time: 0.0031  last_data_time: 0.0043   lr: 9.7666e-05  max_mem: 14629M
[12/04 23:43:37] d2.utils.events INFO:  eta: 0:51:13  iter: 279  total_loss: 28.39  loss_ce: 0.721  loss_mask: 0.366  loss_dice: 1.139  loss_contrastive: 0  loss_ce_0: 1.957  loss_mask_0: 0.6174  loss_dice_0: 1.59  loss_ce_1: 1.403  loss_mask_1: 0.4383  loss_dice_1: 1.186  loss_ce_2: 1.1  loss_mask_2: 0.3709  loss_dice_2: 1.041  loss_ce_3: 0.9268  loss_mask_3: 0.3668  loss_dice_3: 1.044  loss_ce_4: 0.7785  loss_mask_4: 0.4118  loss_dice_4: 1.07  loss_ce_5: 0.7042  loss_mask_5: 0.4034  loss_dice_5: 1.117  loss_ce_6: 0.668  loss_mask_6: 0.3809  loss_dice_6: 1.06  loss_ce_7: 0.6331  loss_mask_7: 0.3823  loss_dice_7: 1.188  loss_ce_8: 0.6769  loss_mask_8: 0.3954  loss_dice_8: 1.094    time: 0.3220  last_time: 0.3085  data_time: 0.0036  last_data_time: 0.0029   lr: 9.7485e-05  max_mem: 14641M
[12/04 23:43:43] d2.utils.events INFO:  eta: 0:51:11  iter: 299  total_loss: 32.75  loss_ce: 0.8467  loss_mask: 0.4622  loss_dice: 1.283  loss_contrastive: 0  loss_ce_0: 2.636  loss_mask_0: 0.4898  loss_dice_0: 1.878  loss_ce_1: 1.567  loss_mask_1: 0.4177  loss_dice_1: 1.505  loss_ce_2: 1.308  loss_mask_2: 0.334  loss_dice_2: 1.317  loss_ce_3: 1.111  loss_mask_3: 0.3486  loss_dice_3: 1.165  loss_ce_4: 0.8994  loss_mask_4: 0.4169  loss_dice_4: 1.267  loss_ce_5: 1.001  loss_mask_5: 0.3547  loss_dice_5: 1.209  loss_ce_6: 0.8797  loss_mask_6: 0.4364  loss_dice_6: 1.256  loss_ce_7: 0.8694  loss_mask_7: 0.3981  loss_dice_7: 1.358  loss_ce_8: 0.8732  loss_mask_8: 0.4045  loss_dice_8: 1.328    time: 0.3221  last_time: 0.3071  data_time: 0.0036  last_data_time: 0.0031   lr: 9.7305e-05  max_mem: 14641M
[12/04 23:43:50] d2.utils.events INFO:  eta: 0:51:06  iter: 319  total_loss: 25.47  loss_ce: 0.7007  loss_mask: 0.4234  loss_dice: 1.086  loss_contrastive: 0  loss_ce_0: 2.054  loss_mask_0: 0.4023  loss_dice_0: 1.779  loss_ce_1: 1.491  loss_mask_1: 0.4102  loss_dice_1: 1.126  loss_ce_2: 1.074  loss_mask_2: 0.3535  loss_dice_2: 1.145  loss_ce_3: 1.046  loss_mask_3: 0.3247  loss_dice_3: 1.189  loss_ce_4: 0.7485  loss_mask_4: 0.3128  loss_dice_4: 1.109  loss_ce_5: 0.6981  loss_mask_5: 0.347  loss_dice_5: 1.092  loss_ce_6: 0.6477  loss_mask_6: 0.3235  loss_dice_6: 1.033  loss_ce_7: 0.6151  loss_mask_7: 0.3373  loss_dice_7: 1.049  loss_ce_8: 0.6791  loss_mask_8: 0.3252  loss_dice_8: 1.08    time: 0.3223  last_time: 0.3590  data_time: 0.0037  last_data_time: 0.0055   lr: 9.7124e-05  max_mem: 14641M
[12/04 23:43:56] d2.utils.events INFO:  eta: 0:50:59  iter: 339  total_loss: 31.93  loss_ce: 1.066  loss_mask: 0.5028  loss_dice: 1.382  loss_contrastive: 0  loss_ce_0: 2.159  loss_mask_0: 0.5405  loss_dice_0: 1.684  loss_ce_1: 1.27  loss_mask_1: 0.5407  loss_dice_1: 1.393  loss_ce_2: 1.064  loss_mask_2: 0.5181  loss_dice_2: 1.25  loss_ce_3: 1.042  loss_mask_3: 0.4897  loss_dice_3: 1.238  loss_ce_4: 0.8498  loss_mask_4: 0.4692  loss_dice_4: 1.24  loss_ce_5: 0.9394  loss_mask_5: 0.4956  loss_dice_5: 1.201  loss_ce_6: 0.7929  loss_mask_6: 0.4832  loss_dice_6: 1.236  loss_ce_7: 0.8862  loss_mask_7: 0.4666  loss_dice_7: 1.288  loss_ce_8: 0.9386  loss_mask_8: 0.482  loss_dice_8: 1.264    time: 0.3223  last_time: 0.3186  data_time: 0.0035  last_data_time: 0.0027   lr: 9.6944e-05  max_mem: 14641M
[12/04 23:44:03] d2.utils.events INFO:  eta: 0:50:58  iter: 359  total_loss: 23.43  loss_ce: 0.6226  loss_mask: 0.3819  loss_dice: 0.9035  loss_contrastive: 0  loss_ce_0: 1.738  loss_mask_0: 0.3907  loss_dice_0: 1.272  loss_ce_1: 1.272  loss_mask_1: 0.3587  loss_dice_1: 0.9469  loss_ce_2: 0.8167  loss_mask_2: 0.3793  loss_dice_2: 0.9689  loss_ce_3: 0.7452  loss_mask_3: 0.4041  loss_dice_3: 0.8345  loss_ce_4: 0.6335  loss_mask_4: 0.4247  loss_dice_4: 0.9487  loss_ce_5: 0.5883  loss_mask_5: 0.3669  loss_dice_5: 0.951  loss_ce_6: 0.7396  loss_mask_6: 0.3615  loss_dice_6: 0.9273  loss_ce_7: 0.6648  loss_mask_7: 0.3367  loss_dice_7: 0.9071  loss_ce_8: 0.6474  loss_mask_8: 0.3683  loss_dice_8: 0.9095    time: 0.3223  last_time: 0.3266  data_time: 0.0032  last_data_time: 0.0030   lr: 9.6763e-05  max_mem: 14641M
[12/04 23:44:09] d2.utils.events INFO:  eta: 0:50:55  iter: 379  total_loss: 26.6  loss_ce: 0.6275  loss_mask: 0.4026  loss_dice: 1.048  loss_contrastive: 0  loss_ce_0: 2.005  loss_mask_0: 0.4042  loss_dice_0: 1.42  loss_ce_1: 1.355  loss_mask_1: 0.3808  loss_dice_1: 1.066  loss_ce_2: 1.144  loss_mask_2: 0.4036  loss_dice_2: 1.141  loss_ce_3: 0.7594  loss_mask_3: 0.3461  loss_dice_3: 1.007  loss_ce_4: 0.6106  loss_mask_4: 0.4164  loss_dice_4: 1.08  loss_ce_5: 0.6995  loss_mask_5: 0.4163  loss_dice_5: 1.013  loss_ce_6: 0.6857  loss_mask_6: 0.4034  loss_dice_6: 0.9754  loss_ce_7: 0.606  loss_mask_7: 0.3919  loss_dice_7: 0.9867  loss_ce_8: 0.6522  loss_mask_8: 0.3913  loss_dice_8: 1.082    time: 0.3224  last_time: 0.3084  data_time: 0.0034  last_data_time: 0.0037   lr: 9.6582e-05  max_mem: 14641M
[12/04 23:44:16] d2.utils.events INFO:  eta: 0:50:48  iter: 399  total_loss: 24.73  loss_ce: 0.6835  loss_mask: 0.3548  loss_dice: 1.041  loss_contrastive: 0  loss_ce_0: 2.389  loss_mask_0: 0.4768  loss_dice_0: 1.381  loss_ce_1: 1.399  loss_mask_1: 0.4098  loss_dice_1: 1.176  loss_ce_2: 1.087  loss_mask_2: 0.4588  loss_dice_2: 0.9779  loss_ce_3: 0.9393  loss_mask_3: 0.3686  loss_dice_3: 0.9655  loss_ce_4: 0.7252  loss_mask_4: 0.3547  loss_dice_4: 0.9362  loss_ce_5: 0.8899  loss_mask_5: 0.3605  loss_dice_5: 0.8987  loss_ce_6: 0.8181  loss_mask_6: 0.3492  loss_dice_6: 0.9759  loss_ce_7: 0.7538  loss_mask_7: 0.3437  loss_dice_7: 1.032  loss_ce_8: 0.7936  loss_mask_8: 0.3567  loss_dice_8: 0.993    time: 0.3224  last_time: 0.3384  data_time: 0.0034  last_data_time: 0.0029   lr: 9.6402e-05  max_mem: 14641M
[12/04 23:44:22] d2.utils.events INFO:  eta: 0:50:41  iter: 419  total_loss: 24.93  loss_ce: 0.5897  loss_mask: 0.3589  loss_dice: 1.046  loss_contrastive: 0  loss_ce_0: 2.324  loss_mask_0: 0.3915  loss_dice_0: 1.499  loss_ce_1: 1.267  loss_mask_1: 0.3858  loss_dice_1: 1.184  loss_ce_2: 0.9502  loss_mask_2: 0.3495  loss_dice_2: 1.215  loss_ce_3: 0.8481  loss_mask_3: 0.3415  loss_dice_3: 1.197  loss_ce_4: 0.7184  loss_mask_4: 0.3749  loss_dice_4: 1.218  loss_ce_5: 0.6128  loss_mask_5: 0.3555  loss_dice_5: 1.118  loss_ce_6: 0.5652  loss_mask_6: 0.361  loss_dice_6: 1.164  loss_ce_7: 0.5724  loss_mask_7: 0.3727  loss_dice_7: 1.127  loss_ce_8: 0.5685  loss_mask_8: 0.3439  loss_dice_8: 1.167    time: 0.3223  last_time: 0.3332  data_time: 0.0034  last_data_time: 0.0060   lr: 9.6221e-05  max_mem: 14641M
[12/04 23:44:28] d2.utils.events INFO:  eta: 0:50:30  iter: 439  total_loss: 20.22  loss_ce: 0.315  loss_mask: 0.271  loss_dice: 0.9273  loss_contrastive: 0  loss_ce_0: 1.63  loss_mask_0: 0.3809  loss_dice_0: 1.142  loss_ce_1: 0.8353  loss_mask_1: 0.3197  loss_dice_1: 0.9719  loss_ce_2: 0.653  loss_mask_2: 0.2422  loss_dice_2: 0.8846  loss_ce_3: 0.5255  loss_mask_3: 0.2254  loss_dice_3: 0.881  loss_ce_4: 0.4863  loss_mask_4: 0.2087  loss_dice_4: 0.9346  loss_ce_5: 0.4149  loss_mask_5: 0.2304  loss_dice_5: 0.8422  loss_ce_6: 0.3677  loss_mask_6: 0.2337  loss_dice_6: 0.8  loss_ce_7: 0.3006  loss_mask_7: 0.2516  loss_dice_7: 0.8254  loss_ce_8: 0.4115  loss_mask_8: 0.2407  loss_dice_8: 0.813    time: 0.3218  last_time: 0.2991  data_time: 0.0031  last_data_time: 0.0024   lr: 9.604e-05  max_mem: 14641M
[12/04 23:44:35] d2.utils.events INFO:  eta: 0:50:21  iter: 459  total_loss: 21.26  loss_ce: 0.5094  loss_mask: 0.3258  loss_dice: 0.7897  loss_contrastive: 0  loss_ce_0: 1.56  loss_mask_0: 0.4171  loss_dice_0: 1.065  loss_ce_1: 1.099  loss_mask_1: 0.3244  loss_dice_1: 0.8217  loss_ce_2: 0.8563  loss_mask_2: 0.308  loss_dice_2: 0.8847  loss_ce_3: 0.5997  loss_mask_3: 0.3232  loss_dice_3: 0.787  loss_ce_4: 0.5712  loss_mask_4: 0.3555  loss_dice_4: 0.8126  loss_ce_5: 0.4854  loss_mask_5: 0.3213  loss_dice_5: 0.819  loss_ce_6: 0.5927  loss_mask_6: 0.31  loss_dice_6: 0.8284  loss_ce_7: 0.492  loss_mask_7: 0.3274  loss_dice_7: 0.8222  loss_ce_8: 0.5321  loss_mask_8: 0.3184  loss_dice_8: 0.7979    time: 0.3220  last_time: 0.3064  data_time: 0.0032  last_data_time: 0.0033   lr: 9.5859e-05  max_mem: 14641M
[12/04 23:44:41] d2.utils.events INFO:  eta: 0:50:13  iter: 479  total_loss: 26.81  loss_ce: 0.5312  loss_mask: 0.6607  loss_dice: 1.018  loss_contrastive: 0  loss_ce_0: 1.569  loss_mask_0: 0.6082  loss_dice_0: 1.377  loss_ce_1: 1.125  loss_mask_1: 0.6322  loss_dice_1: 1.051  loss_ce_2: 0.8119  loss_mask_2: 0.5726  loss_dice_2: 1.143  loss_ce_3: 0.5893  loss_mask_3: 0.6172  loss_dice_3: 1.062  loss_ce_4: 0.547  loss_mask_4: 0.6179  loss_dice_4: 1.041  loss_ce_5: 0.5159  loss_mask_5: 0.5988  loss_dice_5: 1.059  loss_ce_6: 0.5895  loss_mask_6: 0.6177  loss_dice_6: 1.007  loss_ce_7: 0.5735  loss_mask_7: 0.5972  loss_dice_7: 1.035  loss_ce_8: 0.4704  loss_mask_8: 0.6349  loss_dice_8: 1.07    time: 0.3217  last_time: 0.3088  data_time: 0.0034  last_data_time: 0.0027   lr: 9.5678e-05  max_mem: 14641M
[12/04 23:44:47] d2.utils.events INFO:  eta: 0:50:07  iter: 499  total_loss: 23.24  loss_ce: 0.4728  loss_mask: 0.3654  loss_dice: 1.03  loss_contrastive: 0  loss_ce_0: 1.66  loss_mask_0: 0.5293  loss_dice_0: 1.762  loss_ce_1: 1.06  loss_mask_1: 0.5033  loss_dice_1: 1.254  loss_ce_2: 0.7968  loss_mask_2: 0.3359  loss_dice_2: 1.21  loss_ce_3: 0.7989  loss_mask_3: 0.3854  loss_dice_3: 1.072  loss_ce_4: 0.6607  loss_mask_4: 0.3699  loss_dice_4: 1.034  loss_ce_5: 0.6458  loss_mask_5: 0.3374  loss_dice_5: 1.121  loss_ce_6: 0.6527  loss_mask_6: 0.3221  loss_dice_6: 1.103  loss_ce_7: 0.582  loss_mask_7: 0.3568  loss_dice_7: 1.028  loss_ce_8: 0.5939  loss_mask_8: 0.3424  loss_dice_8: 1.144    time: 0.3217  last_time: 0.3249  data_time: 0.0038  last_data_time: 0.0025   lr: 9.5498e-05  max_mem: 14641M
[12/04 23:44:54] d2.utils.events INFO:  eta: 0:49:59  iter: 519  total_loss: 30.89  loss_ce: 0.7482  loss_mask: 0.5719  loss_dice: 1.345  loss_contrastive: 0  loss_ce_0: 1.735  loss_mask_0: 0.6636  loss_dice_0: 1.866  loss_ce_1: 1.246  loss_mask_1: 0.4962  loss_dice_1: 1.348  loss_ce_2: 1.168  loss_mask_2: 0.5091  loss_dice_2: 1.587  loss_ce_3: 0.9746  loss_mask_3: 0.5432  loss_dice_3: 1.384  loss_ce_4: 0.9827  loss_mask_4: 0.5873  loss_dice_4: 1.443  loss_ce_5: 0.8498  loss_mask_5: 0.5278  loss_dice_5: 1.302  loss_ce_6: 0.825  loss_mask_6: 0.5549  loss_dice_6: 1.351  loss_ce_7: 0.7311  loss_mask_7: 0.5316  loss_dice_7: 1.367  loss_ce_8: 0.7402  loss_mask_8: 0.5619  loss_dice_8: 1.374    time: 0.3215  last_time: 0.3509  data_time: 0.0030  last_data_time: 0.0036   lr: 9.5317e-05  max_mem: 14641M
[12/04 23:45:00] d2.utils.events INFO:  eta: 0:49:55  iter: 539  total_loss: 23.26  loss_ce: 0.6679  loss_mask: 0.4277  loss_dice: 0.8558  loss_contrastive: 0  loss_ce_0: 1.289  loss_mask_0: 0.3799  loss_dice_0: 1.585  loss_ce_1: 0.8412  loss_mask_1: 0.3513  loss_dice_1: 0.9791  loss_ce_2: 0.6418  loss_mask_2: 0.2974  loss_dice_2: 0.9598  loss_ce_3: 0.6144  loss_mask_3: 0.328  loss_dice_3: 0.8079  loss_ce_4: 0.677  loss_mask_4: 0.3664  loss_dice_4: 0.9802  loss_ce_5: 0.7129  loss_mask_5: 0.2967  loss_dice_5: 0.8327  loss_ce_6: 0.7156  loss_mask_6: 0.284  loss_dice_6: 0.8571  loss_ce_7: 0.7145  loss_mask_7: 0.333  loss_dice_7: 0.7749  loss_ce_8: 0.6863  loss_mask_8: 0.4478  loss_dice_8: 0.9353    time: 0.3215  last_time: 0.3303  data_time: 0.0039  last_data_time: 0.0032   lr: 9.5136e-05  max_mem: 14641M
[12/04 23:45:07] d2.utils.events INFO:  eta: 0:49:48  iter: 559  total_loss: 27.9  loss_ce: 0.8455  loss_mask: 0.3049  loss_dice: 1.297  loss_contrastive: 0  loss_ce_0: 1.806  loss_mask_0: 0.4756  loss_dice_0: 2.036  loss_ce_1: 1.087  loss_mask_1: 0.3572  loss_dice_1: 1.377  loss_ce_2: 1.088  loss_mask_2: 0.3138  loss_dice_2: 1.398  loss_ce_3: 0.9278  loss_mask_3: 0.3553  loss_dice_3: 1.383  loss_ce_4: 1.012  loss_mask_4: 0.3694  loss_dice_4: 1.265  loss_ce_5: 0.8492  loss_mask_5: 0.3566  loss_dice_5: 1.428  loss_ce_6: 0.7024  loss_mask_6: 0.3113  loss_dice_6: 1.358  loss_ce_7: 0.6029  loss_mask_7: 0.2889  loss_dice_7: 1.348  loss_ce_8: 0.6693  loss_mask_8: 0.3667  loss_dice_8: 1.338    time: 0.3215  last_time: 0.3067  data_time: 0.0035  last_data_time: 0.0040   lr: 9.4955e-05  max_mem: 14641M
[12/04 23:45:13] d2.utils.events INFO:  eta: 0:49:43  iter: 579  total_loss: 29.62  loss_ce: 0.5821  loss_mask: 0.4541  loss_dice: 0.9206  loss_contrastive: 0  loss_ce_0: 1.85  loss_mask_0: 0.414  loss_dice_0: 1.662  loss_ce_1: 1.014  loss_mask_1: 0.4008  loss_dice_1: 0.9206  loss_ce_2: 0.8311  loss_mask_2: 0.4759  loss_dice_2: 1.189  loss_ce_3: 0.7846  loss_mask_3: 0.4121  loss_dice_3: 0.9681  loss_ce_4: 0.749  loss_mask_4: 0.4382  loss_dice_4: 1.062  loss_ce_5: 0.7894  loss_mask_5: 0.4513  loss_dice_5: 1.104  loss_ce_6: 0.6862  loss_mask_6: 0.4266  loss_dice_6: 1.161  loss_ce_7: 0.6395  loss_mask_7: 0.403  loss_dice_7: 1.153  loss_ce_8: 0.6899  loss_mask_8: 0.4025  loss_dice_8: 1.097    time: 0.3217  last_time: 0.3225  data_time: 0.0034  last_data_time: 0.0027   lr: 9.4774e-05  max_mem: 14641M
[12/04 23:45:20] d2.utils.events INFO:  eta: 0:49:38  iter: 599  total_loss: 22.47  loss_ce: 0.3419  loss_mask: 0.4262  loss_dice: 1.079  loss_contrastive: 0  loss_ce_0: 1.727  loss_mask_0: 0.4832  loss_dice_0: 1.518  loss_ce_1: 0.7476  loss_mask_1: 0.4373  loss_dice_1: 1.05  loss_ce_2: 0.6347  loss_mask_2: 0.4727  loss_dice_2: 1.114  loss_ce_3: 0.5348  loss_mask_3: 0.4696  loss_dice_3: 1.003  loss_ce_4: 0.3791  loss_mask_4: 0.4835  loss_dice_4: 1.016  loss_ce_5: 0.3621  loss_mask_5: 0.4657  loss_dice_5: 0.9806  loss_ce_6: 0.3457  loss_mask_6: 0.4335  loss_dice_6: 1.014  loss_ce_7: 0.3183  loss_mask_7: 0.4442  loss_dice_7: 0.928  loss_ce_8: 0.2862  loss_mask_8: 0.4623  loss_dice_8: 0.9438    time: 0.3218  last_time: 0.3179  data_time: 0.0034  last_data_time: 0.0031   lr: 9.4592e-05  max_mem: 14641M
[12/04 23:45:26] d2.utils.events INFO:  eta: 0:49:35  iter: 619  total_loss: 24.23  loss_ce: 0.5376  loss_mask: 0.4541  loss_dice: 1.162  loss_contrastive: 0  loss_ce_0: 1.749  loss_mask_0: 0.4833  loss_dice_0: 1.588  loss_ce_1: 1.043  loss_mask_1: 0.4652  loss_dice_1: 1.226  loss_ce_2: 0.9668  loss_mask_2: 0.4205  loss_dice_2: 1.131  loss_ce_3: 0.8683  loss_mask_3: 0.4138  loss_dice_3: 1.191  loss_ce_4: 0.8093  loss_mask_4: 0.4412  loss_dice_4: 1.123  loss_ce_5: 0.7061  loss_mask_5: 0.4196  loss_dice_5: 1.291  loss_ce_6: 0.6949  loss_mask_6: 0.4654  loss_dice_6: 1.247  loss_ce_7: 0.7104  loss_mask_7: 0.4233  loss_dice_7: 1.256  loss_ce_8: 0.5817  loss_mask_8: 0.5056  loss_dice_8: 1.179    time: 0.3218  last_time: 0.3194  data_time: 0.0036  last_data_time: 0.0031   lr: 9.4411e-05  max_mem: 14641M
[12/04 23:45:33] d2.utils.events INFO:  eta: 0:49:29  iter: 639  total_loss: 22.92  loss_ce: 0.4224  loss_mask: 0.5628  loss_dice: 1.038  loss_contrastive: 0  loss_ce_0: 1.326  loss_mask_0: 0.5367  loss_dice_0: 1.347  loss_ce_1: 0.8157  loss_mask_1: 0.4899  loss_dice_1: 1.121  loss_ce_2: 0.7226  loss_mask_2: 0.5484  loss_dice_2: 1.074  loss_ce_3: 0.4921  loss_mask_3: 0.5057  loss_dice_3: 1.061  loss_ce_4: 0.4431  loss_mask_4: 0.5604  loss_dice_4: 1.061  loss_ce_5: 0.4096  loss_mask_5: 0.5458  loss_dice_5: 1.043  loss_ce_6: 0.3731  loss_mask_6: 0.5621  loss_dice_6: 1.06  loss_ce_7: 0.4105  loss_mask_7: 0.4861  loss_dice_7: 1.024  loss_ce_8: 0.4068  loss_mask_8: 0.623  loss_dice_8: 1.094    time: 0.3217  last_time: 0.3195  data_time: 0.0036  last_data_time: 0.0037   lr: 9.423e-05  max_mem: 14641M
[12/04 23:45:39] d2.utils.events INFO:  eta: 0:49:21  iter: 659  total_loss: 22.53  loss_ce: 0.6456  loss_mask: 0.5165  loss_dice: 0.9325  loss_contrastive: 0  loss_ce_0: 1.646  loss_mask_0: 0.5361  loss_dice_0: 1.222  loss_ce_1: 0.9795  loss_mask_1: 0.5082  loss_dice_1: 1.066  loss_ce_2: 0.8497  loss_mask_2: 0.4731  loss_dice_2: 1.047  loss_ce_3: 0.8953  loss_mask_3: 0.5039  loss_dice_3: 0.9508  loss_ce_4: 0.6507  loss_mask_4: 0.4187  loss_dice_4: 1.042  loss_ce_5: 0.6579  loss_mask_5: 0.4131  loss_dice_5: 0.96  loss_ce_6: 0.6611  loss_mask_6: 0.5058  loss_dice_6: 0.9564  loss_ce_7: 0.5505  loss_mask_7: 0.4919  loss_dice_7: 0.9689  loss_ce_8: 0.5922  loss_mask_8: 0.4941  loss_dice_8: 0.9569    time: 0.3217  last_time: 0.3192  data_time: 0.0032  last_data_time: 0.0029   lr: 9.4049e-05  max_mem: 14641M
[12/04 23:45:46] d2.utils.events INFO:  eta: 0:49:14  iter: 679  total_loss: 24.96  loss_ce: 0.5026  loss_mask: 0.3926  loss_dice: 1.068  loss_contrastive: 0  loss_ce_0: 1.81  loss_mask_0: 0.4163  loss_dice_0: 1.639  loss_ce_1: 1.136  loss_mask_1: 0.3582  loss_dice_1: 1.262  loss_ce_2: 0.8165  loss_mask_2: 0.3597  loss_dice_2: 1.112  loss_ce_3: 0.681  loss_mask_3: 0.4261  loss_dice_3: 1.077  loss_ce_4: 0.5407  loss_mask_4: 0.4098  loss_dice_4: 1.076  loss_ce_5: 0.6177  loss_mask_5: 0.3847  loss_dice_5: 1.07  loss_ce_6: 0.5621  loss_mask_6: 0.3809  loss_dice_6: 1.022  loss_ce_7: 0.4654  loss_mask_7: 0.3633  loss_dice_7: 1.031  loss_ce_8: 0.5197  loss_mask_8: 0.3736  loss_dice_8: 0.9839    time: 0.3218  last_time: 0.3156  data_time: 0.0037  last_data_time: 0.0030   lr: 9.3868e-05  max_mem: 14641M
[12/04 23:45:52] d2.utils.events INFO:  eta: 0:49:06  iter: 699  total_loss: 26.92  loss_ce: 0.6413  loss_mask: 0.5365  loss_dice: 1.113  loss_contrastive: 0  loss_ce_0: 1.974  loss_mask_0: 0.6206  loss_dice_0: 1.587  loss_ce_1: 1.366  loss_mask_1: 0.4925  loss_dice_1: 1.359  loss_ce_2: 1.07  loss_mask_2: 0.5029  loss_dice_2: 1.261  loss_ce_3: 0.9204  loss_mask_3: 0.5184  loss_dice_3: 1.133  loss_ce_4: 0.8093  loss_mask_4: 0.5906  loss_dice_4: 1.034  loss_ce_5: 0.787  loss_mask_5: 0.6056  loss_dice_5: 1.069  loss_ce_6: 0.7602  loss_mask_6: 0.5714  loss_dice_6: 1.043  loss_ce_7: 0.7376  loss_mask_7: 0.527  loss_dice_7: 1.078  loss_ce_8: 0.7827  loss_mask_8: 0.5582  loss_dice_8: 1.114    time: 0.3217  last_time: 0.3222  data_time: 0.0037  last_data_time: 0.0035   lr: 9.3686e-05  max_mem: 14641M
[12/04 23:45:58] d2.utils.events INFO:  eta: 0:48:59  iter: 719  total_loss: 28.41  loss_ce: 0.6083  loss_mask: 0.466  loss_dice: 1.06  loss_contrastive: 0  loss_ce_0: 1.989  loss_mask_0: 0.634  loss_dice_0: 1.739  loss_ce_1: 1.197  loss_mask_1: 0.6241  loss_dice_1: 1.413  loss_ce_2: 1.036  loss_mask_2: 0.452  loss_dice_2: 1.219  loss_ce_3: 0.8632  loss_mask_3: 0.4897  loss_dice_3: 1.188  loss_ce_4: 0.7472  loss_mask_4: 0.4865  loss_dice_4: 1.224  loss_ce_5: 0.6006  loss_mask_5: 0.5165  loss_dice_5: 1.203  loss_ce_6: 0.7264  loss_mask_6: 0.4783  loss_dice_6: 1.022  loss_ce_7: 0.6227  loss_mask_7: 0.4923  loss_dice_7: 1.077  loss_ce_8: 0.7053  loss_mask_8: 0.4634  loss_dice_8: 1.078    time: 0.3216  last_time: 0.3088  data_time: 0.0037  last_data_time: 0.0033   lr: 9.3505e-05  max_mem: 14641M
[12/04 23:46:05] d2.utils.events INFO:  eta: 0:48:52  iter: 739  total_loss: 34.03  loss_ce: 1.045  loss_mask: 0.4192  loss_dice: 1.53  loss_contrastive: 0  loss_ce_0: 2.653  loss_mask_0: 0.852  loss_dice_0: 2.479  loss_ce_1: 1.782  loss_mask_1: 0.5031  loss_dice_1: 1.773  loss_ce_2: 1.54  loss_mask_2: 0.4749  loss_dice_2: 1.745  loss_ce_3: 1.2  loss_mask_3: 0.4444  loss_dice_3: 1.629  loss_ce_4: 1.054  loss_mask_4: 0.4181  loss_dice_4: 1.581  loss_ce_5: 1.024  loss_mask_5: 0.4061  loss_dice_5: 1.534  loss_ce_6: 0.9537  loss_mask_6: 0.3896  loss_dice_6: 1.605  loss_ce_7: 1.012  loss_mask_7: 0.3873  loss_dice_7: 1.585  loss_ce_8: 0.9715  loss_mask_8: 0.3942  loss_dice_8: 1.482    time: 0.3216  last_time: 0.3378  data_time: 0.0035  last_data_time: 0.0028   lr: 9.3324e-05  max_mem: 14641M
[12/04 23:46:11] d2.utils.events INFO:  eta: 0:48:46  iter: 759  total_loss: 28.49  loss_ce: 0.5262  loss_mask: 0.5682  loss_dice: 1.245  loss_contrastive: 0  loss_ce_0: 1.562  loss_mask_0: 0.8154  loss_dice_0: 1.809  loss_ce_1: 0.957  loss_mask_1: 0.6853  loss_dice_1: 1.177  loss_ce_2: 0.951  loss_mask_2: 0.5207  loss_dice_2: 1.392  loss_ce_3: 0.7116  loss_mask_3: 0.5883  loss_dice_3: 1.244  loss_ce_4: 0.701  loss_mask_4: 0.5954  loss_dice_4: 1.342  loss_ce_5: 0.7076  loss_mask_5: 0.5791  loss_dice_5: 1.226  loss_ce_6: 0.7312  loss_mask_6: 0.5836  loss_dice_6: 1.242  loss_ce_7: 0.6471  loss_mask_7: 0.5365  loss_dice_7: 1.219  loss_ce_8: 0.5659  loss_mask_8: 0.5374  loss_dice_8: 1.265    time: 0.3217  last_time: 0.3474  data_time: 0.0032  last_data_time: 0.0027   lr: 9.3142e-05  max_mem: 14641M
[12/04 23:46:18] d2.utils.events INFO:  eta: 0:48:40  iter: 779  total_loss: 29.29  loss_ce: 0.5589  loss_mask: 0.3526  loss_dice: 1.389  loss_contrastive: 0  loss_ce_0: 2.104  loss_mask_0: 0.5695  loss_dice_0: 2.414  loss_ce_1: 1.24  loss_mask_1: 0.3473  loss_dice_1: 1.686  loss_ce_2: 1.053  loss_mask_2: 0.2884  loss_dice_2: 1.436  loss_ce_3: 0.8774  loss_mask_3: 0.2956  loss_dice_3: 1.407  loss_ce_4: 0.8053  loss_mask_4: 0.2904  loss_dice_4: 1.27  loss_ce_5: 0.7251  loss_mask_5: 0.327  loss_dice_5: 1.374  loss_ce_6: 0.6225  loss_mask_6: 0.3182  loss_dice_6: 1.305  loss_ce_7: 0.6038  loss_mask_7: 0.3544  loss_dice_7: 1.267  loss_ce_8: 0.6086  loss_mask_8: 0.3181  loss_dice_8: 1.382    time: 0.3217  last_time: 0.3171  data_time: 0.0034  last_data_time: 0.0054   lr: 9.2961e-05  max_mem: 14641M
[12/04 23:46:24] d2.utils.events INFO:  eta: 0:48:33  iter: 799  total_loss: 29.53  loss_ce: 0.6324  loss_mask: 0.4252  loss_dice: 1.217  loss_contrastive: 0  loss_ce_0: 1.758  loss_mask_0: 0.6176  loss_dice_0: 2.301  loss_ce_1: 1.175  loss_mask_1: 0.4152  loss_dice_1: 1.57  loss_ce_2: 0.8792  loss_mask_2: 0.3916  loss_dice_2: 1.4  loss_ce_3: 0.7077  loss_mask_3: 0.359  loss_dice_3: 1.274  loss_ce_4: 0.6407  loss_mask_4: 0.3825  loss_dice_4: 1.373  loss_ce_5: 0.6608  loss_mask_5: 0.3724  loss_dice_5: 1.197  loss_ce_6: 0.6416  loss_mask_6: 0.348  loss_dice_6: 1.298  loss_ce_7: 0.5844  loss_mask_7: 0.4305  loss_dice_7: 1.33  loss_ce_8: 0.6458  loss_mask_8: 0.4455  loss_dice_8: 1.226    time: 0.3216  last_time: 0.3059  data_time: 0.0037  last_data_time: 0.0028   lr: 9.2779e-05  max_mem: 14641M
[12/04 23:46:31] d2.utils.events INFO:  eta: 0:48:27  iter: 819  total_loss: 26.82  loss_ce: 0.6185  loss_mask: 0.3768  loss_dice: 1.121  loss_contrastive: 0  loss_ce_0: 2.25  loss_mask_0: 0.5499  loss_dice_0: 2.337  loss_ce_1: 1.178  loss_mask_1: 0.4552  loss_dice_1: 1.625  loss_ce_2: 1.041  loss_mask_2: 0.4565  loss_dice_2: 1.433  loss_ce_3: 0.8953  loss_mask_3: 0.4331  loss_dice_3: 1.136  loss_ce_4: 0.8476  loss_mask_4: 0.4076  loss_dice_4: 1.157  loss_ce_5: 0.6898  loss_mask_5: 0.4082  loss_dice_5: 1.157  loss_ce_6: 0.6296  loss_mask_6: 0.3905  loss_dice_6: 1.177  loss_ce_7: 0.5857  loss_mask_7: 0.3857  loss_dice_7: 1.242  loss_ce_8: 0.5687  loss_mask_8: 0.383  loss_dice_8: 1.303    time: 0.3216  last_time: 0.3231  data_time: 0.0031  last_data_time: 0.0030   lr: 9.2598e-05  max_mem: 14641M
[12/04 23:46:37] d2.utils.events INFO:  eta: 0:48:20  iter: 839  total_loss: 34.47  loss_ce: 0.8411  loss_mask: 0.493  loss_dice: 1.289  loss_contrastive: 0  loss_ce_0: 2.254  loss_mask_0: 0.718  loss_dice_0: 2.406  loss_ce_1: 1.415  loss_mask_1: 0.5291  loss_dice_1: 1.562  loss_ce_2: 1.198  loss_mask_2: 0.535  loss_dice_2: 1.367  loss_ce_3: 1.107  loss_mask_3: 0.5676  loss_dice_3: 1.372  loss_ce_4: 0.9872  loss_mask_4: 0.5201  loss_dice_4: 1.333  loss_ce_5: 0.8713  loss_mask_5: 0.5449  loss_dice_5: 1.436  loss_ce_6: 0.9647  loss_mask_6: 0.5814  loss_dice_6: 1.306  loss_ce_7: 0.8104  loss_mask_7: 0.5232  loss_dice_7: 1.365  loss_ce_8: 0.8078  loss_mask_8: 0.4713  loss_dice_8: 1.315    time: 0.3216  last_time: 0.3224  data_time: 0.0036  last_data_time: 0.0030   lr: 9.2416e-05  max_mem: 14641M
[12/04 23:46:44] d2.utils.events INFO:  eta: 0:48:15  iter: 859  total_loss: 32.22  loss_ce: 0.8507  loss_mask: 0.5291  loss_dice: 1.186  loss_contrastive: 0  loss_ce_0: 1.384  loss_mask_0: 0.5358  loss_dice_0: 1.799  loss_ce_1: 1.212  loss_mask_1: 0.5077  loss_dice_1: 1.28  loss_ce_2: 1.025  loss_mask_2: 0.4539  loss_dice_2: 1.093  loss_ce_3: 0.7946  loss_mask_3: 0.5221  loss_dice_3: 1.043  loss_ce_4: 0.7935  loss_mask_4: 0.549  loss_dice_4: 1.24  loss_ce_5: 0.7073  loss_mask_5: 0.5651  loss_dice_5: 1.209  loss_ce_6: 0.6888  loss_mask_6: 0.5976  loss_dice_6: 1.153  loss_ce_7: 0.5271  loss_mask_7: 0.5822  loss_dice_7: 1.153  loss_ce_8: 0.6775  loss_mask_8: 0.4881  loss_dice_8: 1.163    time: 0.3218  last_time: 0.3064  data_time: 0.0032  last_data_time: 0.0026   lr: 9.2235e-05  max_mem: 14641M
[12/04 23:46:50] d2.utils.events INFO:  eta: 0:48:08  iter: 879  total_loss: 27.17  loss_ce: 0.7206  loss_mask: 0.3489  loss_dice: 1.203  loss_contrastive: 0  loss_ce_0: 1.317  loss_mask_0: 0.4517  loss_dice_0: 1.527  loss_ce_1: 0.9282  loss_mask_1: 0.4173  loss_dice_1: 1.656  loss_ce_2: 0.8041  loss_mask_2: 0.4253  loss_dice_2: 1.288  loss_ce_3: 0.628  loss_mask_3: 0.3644  loss_dice_3: 1.203  loss_ce_4: 0.5726  loss_mask_4: 0.3953  loss_dice_4: 1.316  loss_ce_5: 0.7956  loss_mask_5: 0.3202  loss_dice_5: 1.231  loss_ce_6: 0.6416  loss_mask_6: 0.3393  loss_dice_6: 1.205  loss_ce_7: 0.7159  loss_mask_7: 0.3803  loss_dice_7: 1.208  loss_ce_8: 0.6771  loss_mask_8: 0.3795  loss_dice_8: 1.222    time: 0.3217  last_time: 0.3209  data_time: 0.0032  last_data_time: 0.0029   lr: 9.2053e-05  max_mem: 14641M
[12/04 23:46:57] d2.utils.events INFO:  eta: 0:48:03  iter: 899  total_loss: 21.61  loss_ce: 0.4846  loss_mask: 0.3905  loss_dice: 1.025  loss_contrastive: 0  loss_ce_0: 1.804  loss_mask_0: 0.5571  loss_dice_0: 1.466  loss_ce_1: 0.9626  loss_mask_1: 0.3464  loss_dice_1: 1.181  loss_ce_2: 0.886  loss_mask_2: 0.3353  loss_dice_2: 1.074  loss_ce_3: 0.7209  loss_mask_3: 0.3403  loss_dice_3: 0.9823  loss_ce_4: 0.6616  loss_mask_4: 0.3605  loss_dice_4: 1.031  loss_ce_5: 0.5734  loss_mask_5: 0.3605  loss_dice_5: 0.9416  loss_ce_6: 0.4581  loss_mask_6: 0.4174  loss_dice_6: 0.9786  loss_ce_7: 0.4651  loss_mask_7: 0.4051  loss_dice_7: 0.9305  loss_ce_8: 0.4923  loss_mask_8: 0.3978  loss_dice_8: 0.9292    time: 0.3217  last_time: 0.3421  data_time: 0.0035  last_data_time: 0.0031   lr: 9.1871e-05  max_mem: 14641M
[12/04 23:47:03] d2.utils.events INFO:  eta: 0:47:56  iter: 919  total_loss: 23.36  loss_ce: 0.4897  loss_mask: 0.5231  loss_dice: 1.214  loss_contrastive: 0  loss_ce_0: 1.519  loss_mask_0: 0.5909  loss_dice_0: 1.469  loss_ce_1: 0.9446  loss_mask_1: 0.4978  loss_dice_1: 1.05  loss_ce_2: 0.7092  loss_mask_2: 0.4831  loss_dice_2: 1.121  loss_ce_3: 0.5707  loss_mask_3: 0.4598  loss_dice_3: 1.101  loss_ce_4: 0.5151  loss_mask_4: 0.4374  loss_dice_4: 1.064  loss_ce_5: 0.6266  loss_mask_5: 0.4674  loss_dice_5: 1.007  loss_ce_6: 0.4743  loss_mask_6: 0.4816  loss_dice_6: 1.105  loss_ce_7: 0.49  loss_mask_7: 0.497  loss_dice_7: 1.208  loss_ce_8: 0.4434  loss_mask_8: 0.5241  loss_dice_8: 1.09    time: 0.3217  last_time: 0.3138  data_time: 0.0034  last_data_time: 0.0032   lr: 9.169e-05  max_mem: 14641M
[12/04 23:47:09] d2.utils.events INFO:  eta: 0:47:48  iter: 939  total_loss: 23.78  loss_ce: 0.4854  loss_mask: 0.3656  loss_dice: 1.087  loss_contrastive: 0  loss_ce_0: 1.554  loss_mask_0: 0.4808  loss_dice_0: 1.553  loss_ce_1: 0.8944  loss_mask_1: 0.3877  loss_dice_1: 1.177  loss_ce_2: 0.827  loss_mask_2: 0.3769  loss_dice_2: 1.109  loss_ce_3: 0.7312  loss_mask_3: 0.3803  loss_dice_3: 1.058  loss_ce_4: 0.61  loss_mask_4: 0.3351  loss_dice_4: 1.131  loss_ce_5: 0.5506  loss_mask_5: 0.356  loss_dice_5: 1.074  loss_ce_6: 0.4886  loss_mask_6: 0.3789  loss_dice_6: 0.9984  loss_ce_7: 0.6071  loss_mask_7: 0.3982  loss_dice_7: 1.086  loss_ce_8: 0.5471  loss_mask_8: 0.3593  loss_dice_8: 1.049    time: 0.3216  last_time: 0.3251  data_time: 0.0031  last_data_time: 0.0036   lr: 9.1508e-05  max_mem: 14641M
[12/04 23:47:16] d2.utils.events INFO:  eta: 0:47:42  iter: 959  total_loss: 22.88  loss_ce: 0.4069  loss_mask: 0.4276  loss_dice: 1.054  loss_contrastive: 0  loss_ce_0: 1.342  loss_mask_0: 0.7823  loss_dice_0: 1.601  loss_ce_1: 0.8214  loss_mask_1: 0.5099  loss_dice_1: 1.276  loss_ce_2: 0.6702  loss_mask_2: 0.37  loss_dice_2: 1.199  loss_ce_3: 0.5597  loss_mask_3: 0.3655  loss_dice_3: 1.065  loss_ce_4: 0.4442  loss_mask_4: 0.4726  loss_dice_4: 1.302  loss_ce_5: 0.4104  loss_mask_5: 0.4846  loss_dice_5: 1.087  loss_ce_6: 0.4322  loss_mask_6: 0.3975  loss_dice_6: 1.118  loss_ce_7: 0.4024  loss_mask_7: 0.3934  loss_dice_7: 1.183  loss_ce_8: 0.3909  loss_mask_8: 0.3978  loss_dice_8: 1.076    time: 0.3215  last_time: 0.3043  data_time: 0.0035  last_data_time: 0.0037   lr: 9.1326e-05  max_mem: 14641M
[12/04 23:47:22] d2.utils.events INFO:  eta: 0:47:36  iter: 979  total_loss: 21.96  loss_ce: 0.5338  loss_mask: 0.3851  loss_dice: 0.9682  loss_contrastive: 0  loss_ce_0: 1.926  loss_mask_0: 0.4689  loss_dice_0: 1.61  loss_ce_1: 0.8886  loss_mask_1: 0.3651  loss_dice_1: 1.302  loss_ce_2: 0.8515  loss_mask_2: 0.2996  loss_dice_2: 1.131  loss_ce_3: 0.6927  loss_mask_3: 0.309  loss_dice_3: 1.061  loss_ce_4: 0.542  loss_mask_4: 0.323  loss_dice_4: 1.043  loss_ce_5: 0.487  loss_mask_5: 0.2987  loss_dice_5: 0.9895  loss_ce_6: 0.4867  loss_mask_6: 0.3311  loss_dice_6: 1.051  loss_ce_7: 0.5654  loss_mask_7: 0.3712  loss_dice_7: 1.03  loss_ce_8: 0.5194  loss_mask_8: 0.3833  loss_dice_8: 1.065    time: 0.3215  last_time: 0.3057  data_time: 0.0033  last_data_time: 0.0029   lr: 9.1144e-05  max_mem: 14641M
[12/04 23:47:29] d2.utils.events INFO:  eta: 0:47:29  iter: 999  total_loss: 26.42  loss_ce: 0.4966  loss_mask: 0.4591  loss_dice: 1.515  loss_contrastive: 0  loss_ce_0: 1.414  loss_mask_0: 0.4733  loss_dice_0: 1.826  loss_ce_1: 0.956  loss_mask_1: 0.4167  loss_dice_1: 1.417  loss_ce_2: 0.8697  loss_mask_2: 0.3563  loss_dice_2: 1.355  loss_ce_3: 0.5847  loss_mask_3: 0.443  loss_dice_3: 1.501  loss_ce_4: 0.687  loss_mask_4: 0.3701  loss_dice_4: 1.451  loss_ce_5: 0.5936  loss_mask_5: 0.3872  loss_dice_5: 1.437  loss_ce_6: 0.5685  loss_mask_6: 0.4813  loss_dice_6: 1.466  loss_ce_7: 0.5374  loss_mask_7: 0.4337  loss_dice_7: 1.526  loss_ce_8: 0.6291  loss_mask_8: 0.4506  loss_dice_8: 1.483    time: 0.3215  last_time: 0.3205  data_time: 0.0032  last_data_time: 0.0029   lr: 9.0962e-05  max_mem: 14641M
[12/04 23:47:35] d2.utils.events INFO:  eta: 0:47:24  iter: 1019  total_loss: 28.47  loss_ce: 0.6339  loss_mask: 0.6054  loss_dice: 1.275  loss_contrastive: 0  loss_ce_0: 1.693  loss_mask_0: 0.6959  loss_dice_0: 1.611  loss_ce_1: 0.8433  loss_mask_1: 0.6799  loss_dice_1: 1.331  loss_ce_2: 0.7759  loss_mask_2: 0.6674  loss_dice_2: 1.263  loss_ce_3: 0.7186  loss_mask_3: 0.6253  loss_dice_3: 1.312  loss_ce_4: 0.8218  loss_mask_4: 0.6147  loss_dice_4: 1.188  loss_ce_5: 0.6844  loss_mask_5: 0.6692  loss_dice_5: 1.355  loss_ce_6: 0.6519  loss_mask_6: 0.635  loss_dice_6: 1.211  loss_ce_7: 0.5538  loss_mask_7: 0.6393  loss_dice_7: 1.313  loss_ce_8: 0.563  loss_mask_8: 0.6413  loss_dice_8: 1.338    time: 0.3215  last_time: 0.3146  data_time: 0.0036  last_data_time: 0.0025   lr: 9.078e-05  max_mem: 14641M
[12/04 23:47:41] d2.utils.events INFO:  eta: 0:47:18  iter: 1039  total_loss: 40.52  loss_ce: 0.9657  loss_mask: 0.6604  loss_dice: 1.475  loss_contrastive: 0  loss_ce_0: 2.23  loss_mask_0: 0.9219  loss_dice_0: 2.123  loss_ce_1: 1.483  loss_mask_1: 0.8094  loss_dice_1: 1.617  loss_ce_2: 1.286  loss_mask_2: 0.6876  loss_dice_2: 1.636  loss_ce_3: 1.113  loss_mask_3: 0.6537  loss_dice_3: 1.59  loss_ce_4: 1.167  loss_mask_4: 0.6324  loss_dice_4: 1.834  loss_ce_5: 0.9679  loss_mask_5: 0.619  loss_dice_5: 1.904  loss_ce_6: 0.9504  loss_mask_6: 0.6136  loss_dice_6: 1.618  loss_ce_7: 0.9325  loss_mask_7: 0.7337  loss_dice_7: 1.604  loss_ce_8: 0.9355  loss_mask_8: 0.7185  loss_dice_8: 1.557    time: 0.3214  last_time: 0.3140  data_time: 0.0033  last_data_time: 0.0033   lr: 9.0598e-05  max_mem: 14641M
[12/04 23:47:48] d2.utils.events INFO:  eta: 0:47:14  iter: 1059  total_loss: 33.31  loss_ce: 0.7752  loss_mask: 0.5735  loss_dice: 1.651  loss_contrastive: 0  loss_ce_0: 1.772  loss_mask_0: 0.5982  loss_dice_0: 2.193  loss_ce_1: 1.035  loss_mask_1: 0.4554  loss_dice_1: 1.819  loss_ce_2: 1.031  loss_mask_2: 0.4972  loss_dice_2: 1.587  loss_ce_3: 0.9241  loss_mask_3: 0.5979  loss_dice_3: 1.548  loss_ce_4: 0.798  loss_mask_4: 0.5072  loss_dice_4: 1.683  loss_ce_5: 0.7526  loss_mask_5: 0.5112  loss_dice_5: 1.721  loss_ce_6: 0.8421  loss_mask_6: 0.5461  loss_dice_6: 1.494  loss_ce_7: 0.8248  loss_mask_7: 0.5928  loss_dice_7: 1.759  loss_ce_8: 0.759  loss_mask_8: 0.5928  loss_dice_8: 1.845    time: 0.3214  last_time: 0.3312  data_time: 0.0031  last_data_time: 0.0043   lr: 9.0416e-05  max_mem: 14641M
[12/04 23:47:54] d2.utils.events INFO:  eta: 0:47:06  iter: 1079  total_loss: 32.67  loss_ce: 0.8763  loss_mask: 0.4125  loss_dice: 1.568  loss_contrastive: 0  loss_ce_0: 2.118  loss_mask_0: 0.5216  loss_dice_0: 2.006  loss_ce_1: 1.537  loss_mask_1: 0.4281  loss_dice_1: 1.5  loss_ce_2: 1.396  loss_mask_2: 0.4697  loss_dice_2: 1.561  loss_ce_3: 1.201  loss_mask_3: 0.4316  loss_dice_3: 1.624  loss_ce_4: 0.9969  loss_mask_4: 0.4553  loss_dice_4: 1.543  loss_ce_5: 1.004  loss_mask_5: 0.4392  loss_dice_5: 1.564  loss_ce_6: 0.9844  loss_mask_6: 0.4569  loss_dice_6: 1.56  loss_ce_7: 0.9588  loss_mask_7: 0.4594  loss_dice_7: 1.605  loss_ce_8: 0.8709  loss_mask_8: 0.4878  loss_dice_8: 1.508    time: 0.3215  last_time: 0.3089  data_time: 0.0035  last_data_time: 0.0034   lr: 9.0234e-05  max_mem: 14641M
[12/04 23:48:01] d2.utils.events INFO:  eta: 0:46:59  iter: 1099  total_loss: 28.41  loss_ce: 0.7136  loss_mask: 0.3927  loss_dice: 1.221  loss_contrastive: 0  loss_ce_0: 1.448  loss_mask_0: 0.5102  loss_dice_0: 1.582  loss_ce_1: 0.9048  loss_mask_1: 0.4333  loss_dice_1: 1.279  loss_ce_2: 0.8655  loss_mask_2: 0.4563  loss_dice_2: 1.182  loss_ce_3: 0.8336  loss_mask_3: 0.4304  loss_dice_3: 1.173  loss_ce_4: 0.7128  loss_mask_4: 0.3888  loss_dice_4: 1.198  loss_ce_5: 0.6519  loss_mask_5: 0.4374  loss_dice_5: 1.311  loss_ce_6: 0.5707  loss_mask_6: 0.438  loss_dice_6: 1.291  loss_ce_7: 0.5612  loss_mask_7: 0.4785  loss_dice_7: 1.351  loss_ce_8: 0.659  loss_mask_8: 0.4423  loss_dice_8: 1.271    time: 0.3214  last_time: 0.3209  data_time: 0.0031  last_data_time: 0.0038   lr: 9.0052e-05  max_mem: 14641M
[12/04 23:48:07] d2.utils.events INFO:  eta: 0:46:54  iter: 1119  total_loss: 26.36  loss_ce: 0.7123  loss_mask: 0.4387  loss_dice: 1.2  loss_contrastive: 0  loss_ce_0: 1.751  loss_mask_0: 0.4867  loss_dice_0: 1.901  loss_ce_1: 0.9218  loss_mask_1: 0.4151  loss_dice_1: 1.335  loss_ce_2: 0.8621  loss_mask_2: 0.417  loss_dice_2: 1.293  loss_ce_3: 0.71  loss_mask_3: 0.4113  loss_dice_3: 1.208  loss_ce_4: 0.783  loss_mask_4: 0.4024  loss_dice_4: 1.239  loss_ce_5: 0.8184  loss_mask_5: 0.4326  loss_dice_5: 1.272  loss_ce_6: 0.7606  loss_mask_6: 0.4217  loss_dice_6: 1.236  loss_ce_7: 0.7566  loss_mask_7: 0.4205  loss_dice_7: 1.236  loss_ce_8: 0.7137  loss_mask_8: 0.4107  loss_dice_8: 1.228    time: 0.3214  last_time: 0.3072  data_time: 0.0035  last_data_time: 0.0038   lr: 8.987e-05  max_mem: 14641M
[12/04 23:48:14] d2.utils.events INFO:  eta: 0:46:46  iter: 1139  total_loss: 23.5  loss_ce: 0.4322  loss_mask: 0.5091  loss_dice: 1.163  loss_contrastive: 0  loss_ce_0: 1.876  loss_mask_0: 0.6074  loss_dice_0: 1.663  loss_ce_1: 1.063  loss_mask_1: 0.5027  loss_dice_1: 1.208  loss_ce_2: 0.724  loss_mask_2: 0.4987  loss_dice_2: 1.196  loss_ce_3: 0.6106  loss_mask_3: 0.5213  loss_dice_3: 1.169  loss_ce_4: 0.4505  loss_mask_4: 0.5184  loss_dice_4: 1.132  loss_ce_5: 0.4636  loss_mask_5: 0.4673  loss_dice_5: 1.159  loss_ce_6: 0.4705  loss_mask_6: 0.4869  loss_dice_6: 1.07  loss_ce_7: 0.4628  loss_mask_7: 0.5178  loss_dice_7: 1.136  loss_ce_8: 0.4674  loss_mask_8: 0.4936  loss_dice_8: 1.186    time: 0.3214  last_time: 0.3070  data_time: 0.0036  last_data_time: 0.0037   lr: 8.9688e-05  max_mem: 14641M
[12/04 23:48:20] d2.utils.events INFO:  eta: 0:46:42  iter: 1159  total_loss: 21.38  loss_ce: 0.3588  loss_mask: 0.2926  loss_dice: 0.7421  loss_contrastive: 0  loss_ce_0: 1.282  loss_mask_0: 0.4656  loss_dice_0: 1.433  loss_ce_1: 0.7824  loss_mask_1: 0.4066  loss_dice_1: 0.6619  loss_ce_2: 0.5959  loss_mask_2: 0.3616  loss_dice_2: 0.7287  loss_ce_3: 0.4836  loss_mask_3: 0.3602  loss_dice_3: 0.6868  loss_ce_4: 0.4812  loss_mask_4: 0.3288  loss_dice_4: 0.6778  loss_ce_5: 0.3604  loss_mask_5: 0.3333  loss_dice_5: 0.8686  loss_ce_6: 0.3521  loss_mask_6: 0.3514  loss_dice_6: 0.7543  loss_ce_7: 0.2547  loss_mask_7: 0.3228  loss_dice_7: 0.7742  loss_ce_8: 0.3012  loss_mask_8: 0.3231  loss_dice_8: 0.7485    time: 0.3214  last_time: 0.3008  data_time: 0.0035  last_data_time: 0.0026   lr: 8.9506e-05  max_mem: 14641M
[12/04 23:48:27] d2.utils.events INFO:  eta: 0:46:33  iter: 1179  total_loss: 25.44  loss_ce: 0.5664  loss_mask: 0.3192  loss_dice: 1.219  loss_contrastive: 0  loss_ce_0: 1.886  loss_mask_0: 0.3809  loss_dice_0: 1.591  loss_ce_1: 0.9446  loss_mask_1: 0.3455  loss_dice_1: 1.333  loss_ce_2: 0.8045  loss_mask_2: 0.3061  loss_dice_2: 1.268  loss_ce_3: 0.7342  loss_mask_3: 0.331  loss_dice_3: 1.225  loss_ce_4: 0.618  loss_mask_4: 0.3216  loss_dice_4: 1.286  loss_ce_5: 0.6044  loss_mask_5: 0.3065  loss_dice_5: 1.242  loss_ce_6: 0.6654  loss_mask_6: 0.3587  loss_dice_6: 1.281  loss_ce_7: 0.6395  loss_mask_7: 0.3483  loss_dice_7: 1.249  loss_ce_8: 0.5888  loss_mask_8: 0.355  loss_dice_8: 1.215    time: 0.3214  last_time: 0.3035  data_time: 0.0032  last_data_time: 0.0029   lr: 8.9324e-05  max_mem: 14641M
[12/04 23:48:33] d2.utils.events INFO:  eta: 0:46:28  iter: 1199  total_loss: 23.39  loss_ce: 0.4202  loss_mask: 0.2825  loss_dice: 1.123  loss_contrastive: 0  loss_ce_0: 1.498  loss_mask_0: 0.3005  loss_dice_0: 1.464  loss_ce_1: 0.9598  loss_mask_1: 0.3089  loss_dice_1: 1.252  loss_ce_2: 0.8232  loss_mask_2: 0.2647  loss_dice_2: 1.204  loss_ce_3: 0.5437  loss_mask_3: 0.28  loss_dice_3: 1.145  loss_ce_4: 0.4707  loss_mask_4: 0.3016  loss_dice_4: 1.279  loss_ce_5: 0.4635  loss_mask_5: 0.2857  loss_dice_5: 1.261  loss_ce_6: 0.477  loss_mask_6: 0.2811  loss_dice_6: 1.128  loss_ce_7: 0.3535  loss_mask_7: 0.2844  loss_dice_7: 1.104  loss_ce_8: 0.4347  loss_mask_8: 0.2787  loss_dice_8: 1.173    time: 0.3215  last_time: 0.2960  data_time: 0.0039  last_data_time: 0.0020   lr: 8.9141e-05  max_mem: 14652M
[12/04 23:48:40] d2.utils.events INFO:  eta: 0:46:24  iter: 1219  total_loss: 23.88  loss_ce: 0.3748  loss_mask: 0.2554  loss_dice: 1.089  loss_contrastive: 0  loss_ce_0: 1.672  loss_mask_0: 0.4127  loss_dice_0: 1.697  loss_ce_1: 0.942  loss_mask_1: 0.2791  loss_dice_1: 1.167  loss_ce_2: 0.825  loss_mask_2: 0.264  loss_dice_2: 1.088  loss_ce_3: 0.6984  loss_mask_3: 0.2557  loss_dice_3: 1.09  loss_ce_4: 0.5414  loss_mask_4: 0.256  loss_dice_4: 1.077  loss_ce_5: 0.4566  loss_mask_5: 0.2515  loss_dice_5: 1.094  loss_ce_6: 0.5468  loss_mask_6: 0.249  loss_dice_6: 1.087  loss_ce_7: 0.5085  loss_mask_7: 0.2558  loss_dice_7: 1.139  loss_ce_8: 0.4355  loss_mask_8: 0.2564  loss_dice_8: 1.137    time: 0.3215  last_time: 0.3091  data_time: 0.0039  last_data_time: 0.0029   lr: 8.8959e-05  max_mem: 14652M
[12/04 23:48:46] d2.utils.events INFO:  eta: 0:46:17  iter: 1239  total_loss: 21.12  loss_ce: 0.4178  loss_mask: 0.2747  loss_dice: 1.049  loss_contrastive: 0  loss_ce_0: 1.597  loss_mask_0: 0.3477  loss_dice_0: 1.269  loss_ce_1: 1.012  loss_mask_1: 0.2687  loss_dice_1: 1.147  loss_ce_2: 0.8021  loss_mask_2: 0.2732  loss_dice_2: 1.091  loss_ce_3: 0.598  loss_mask_3: 0.2548  loss_dice_3: 1.14  loss_ce_4: 0.5472  loss_mask_4: 0.2553  loss_dice_4: 1.115  loss_ce_5: 0.5042  loss_mask_5: 0.2513  loss_dice_5: 1.061  loss_ce_6: 0.3804  loss_mask_6: 0.2254  loss_dice_6: 1.075  loss_ce_7: 0.3714  loss_mask_7: 0.2544  loss_dice_7: 1.091  loss_ce_8: 0.4507  loss_mask_8: 0.2872  loss_dice_8: 1.169    time: 0.3215  last_time: 0.3062  data_time: 0.0037  last_data_time: 0.0031   lr: 8.8777e-05  max_mem: 14652M
[12/04 23:48:52] d2.utils.events INFO:  eta: 0:46:09  iter: 1259  total_loss: 21.71  loss_ce: 0.5455  loss_mask: 0.3619  loss_dice: 1.084  loss_contrastive: 0  loss_ce_0: 1.557  loss_mask_0: 0.3753  loss_dice_0: 1.417  loss_ce_1: 0.8291  loss_mask_1: 0.3521  loss_dice_1: 1.131  loss_ce_2: 0.7368  loss_mask_2: 0.3262  loss_dice_2: 1.072  loss_ce_3: 0.5472  loss_mask_3: 0.3267  loss_dice_3: 1.043  loss_ce_4: 0.4907  loss_mask_4: 0.318  loss_dice_4: 1.083  loss_ce_5: 0.5199  loss_mask_5: 0.3039  loss_dice_5: 1.02  loss_ce_6: 0.6135  loss_mask_6: 0.2996  loss_dice_6: 1.065  loss_ce_7: 0.5382  loss_mask_7: 0.3453  loss_dice_7: 1.072  loss_ce_8: 0.4186  loss_mask_8: 0.3417  loss_dice_8: 1.029    time: 0.3214  last_time: 0.3040  data_time: 0.0035  last_data_time: 0.0032   lr: 8.8594e-05  max_mem: 14652M
[12/04 23:48:59] d2.utils.events INFO:  eta: 0:46:03  iter: 1279  total_loss: 21.56  loss_ce: 0.2068  loss_mask: 0.4133  loss_dice: 1.122  loss_contrastive: 0  loss_ce_0: 0.9131  loss_mask_0: 0.4749  loss_dice_0: 1.403  loss_ce_1: 0.3768  loss_mask_1: 0.4378  loss_dice_1: 1.086  loss_ce_2: 0.6586  loss_mask_2: 0.4076  loss_dice_2: 0.987  loss_ce_3: 0.3569  loss_mask_3: 0.3652  loss_dice_3: 0.9783  loss_ce_4: 0.3254  loss_mask_4: 0.3641  loss_dice_4: 0.967  loss_ce_5: 0.2349  loss_mask_5: 0.3766  loss_dice_5: 0.9757  loss_ce_6: 0.2502  loss_mask_6: 0.3709  loss_dice_6: 0.9546  loss_ce_7: 0.1988  loss_mask_7: 0.3863  loss_dice_7: 1.07  loss_ce_8: 0.2317  loss_mask_8: 0.3943  loss_dice_8: 1.01    time: 0.3215  last_time: 0.3500  data_time: 0.0034  last_data_time: 0.0027   lr: 8.8412e-05  max_mem: 14652M
[12/04 23:49:05] d2.utils.events INFO:  eta: 0:45:55  iter: 1299  total_loss: 26.11  loss_ce: 0.4331  loss_mask: 0.5017  loss_dice: 1.107  loss_contrastive: 0  loss_ce_0: 1.511  loss_mask_0: 0.6635  loss_dice_0: 1.728  loss_ce_1: 0.8483  loss_mask_1: 0.5512  loss_dice_1: 1.308  loss_ce_2: 0.7367  loss_mask_2: 0.4989  loss_dice_2: 1.171  loss_ce_3: 0.5661  loss_mask_3: 0.5265  loss_dice_3: 1.13  loss_ce_4: 0.5012  loss_mask_4: 0.4895  loss_dice_4: 1.015  loss_ce_5: 0.4108  loss_mask_5: 0.5279  loss_dice_5: 1.056  loss_ce_6: 0.4113  loss_mask_6: 0.5381  loss_dice_6: 1.137  loss_ce_7: 0.3451  loss_mask_7: 0.556  loss_dice_7: 1.085  loss_ce_8: 0.4394  loss_mask_8: 0.4938  loss_dice_8: 1.078    time: 0.3216  last_time: 0.3325  data_time: 0.0038  last_data_time: 0.0032   lr: 8.8229e-05  max_mem: 14652M
[12/04 23:49:12] d2.utils.events INFO:  eta: 0:45:48  iter: 1319  total_loss: 29.17  loss_ce: 0.647  loss_mask: 0.3551  loss_dice: 1.287  loss_contrastive: 0  loss_ce_0: 1.516  loss_mask_0: 0.426  loss_dice_0: 1.701  loss_ce_1: 0.9304  loss_mask_1: 0.3816  loss_dice_1: 1.618  loss_ce_2: 0.9341  loss_mask_2: 0.3499  loss_dice_2: 1.431  loss_ce_3: 0.9262  loss_mask_3: 0.3606  loss_dice_3: 1.319  loss_ce_4: 0.8708  loss_mask_4: 0.3533  loss_dice_4: 1.369  loss_ce_5: 0.8722  loss_mask_5: 0.3476  loss_dice_5: 1.357  loss_ce_6: 0.7098  loss_mask_6: 0.368  loss_dice_6: 1.205  loss_ce_7: 0.7821  loss_mask_7: 0.3662  loss_dice_7: 1.276  loss_ce_8: 0.6743  loss_mask_8: 0.3689  loss_dice_8: 1.259    time: 0.3216  last_time: 0.2961  data_time: 0.0033  last_data_time: 0.0020   lr: 8.8047e-05  max_mem: 14652M
[12/04 23:49:18] d2.utils.events INFO:  eta: 0:45:43  iter: 1339  total_loss: 26.05  loss_ce: 0.5107  loss_mask: 0.2767  loss_dice: 1.226  loss_contrastive: 0  loss_ce_0: 1.752  loss_mask_0: 0.4273  loss_dice_0: 1.749  loss_ce_1: 0.9878  loss_mask_1: 0.2738  loss_dice_1: 1.429  loss_ce_2: 0.872  loss_mask_2: 0.2586  loss_dice_2: 1.352  loss_ce_3: 0.7617  loss_mask_3: 0.3104  loss_dice_3: 1.247  loss_ce_4: 0.6524  loss_mask_4: 0.2813  loss_dice_4: 1.286  loss_ce_5: 0.6503  loss_mask_5: 0.2954  loss_dice_5: 1.296  loss_ce_6: 0.5443  loss_mask_6: 0.2989  loss_dice_6: 1.183  loss_ce_7: 0.5664  loss_mask_7: 0.3012  loss_dice_7: 1.256  loss_ce_8: 0.4838  loss_mask_8: 0.298  loss_dice_8: 1.355    time: 0.3216  last_time: 0.3294  data_time: 0.0036  last_data_time: 0.0032   lr: 8.7864e-05  max_mem: 14652M
[12/04 23:49:25] d2.utils.events INFO:  eta: 0:45:35  iter: 1359  total_loss: 29.21  loss_ce: 0.6914  loss_mask: 0.4213  loss_dice: 1.223  loss_contrastive: 0  loss_ce_0: 1.383  loss_mask_0: 0.5132  loss_dice_0: 1.702  loss_ce_1: 0.9374  loss_mask_1: 0.3832  loss_dice_1: 1.441  loss_ce_2: 0.889  loss_mask_2: 0.3231  loss_dice_2: 1.243  loss_ce_3: 0.7704  loss_mask_3: 0.3823  loss_dice_3: 1.228  loss_ce_4: 0.8087  loss_mask_4: 0.4851  loss_dice_4: 1.387  loss_ce_5: 0.7511  loss_mask_5: 0.4453  loss_dice_5: 1.43  loss_ce_6: 0.6806  loss_mask_6: 0.4487  loss_dice_6: 1.286  loss_ce_7: 0.5416  loss_mask_7: 0.4527  loss_dice_7: 1.292  loss_ce_8: 0.6544  loss_mask_8: 0.4316  loss_dice_8: 1.204    time: 0.3216  last_time: 0.3417  data_time: 0.0039  last_data_time: 0.0028   lr: 8.7681e-05  max_mem: 14652M
[12/04 23:49:31] d2.utils.events INFO:  eta: 0:45:23  iter: 1379  total_loss: 24.45  loss_ce: 0.6  loss_mask: 0.335  loss_dice: 1.294  loss_contrastive: 0  loss_ce_0: 1.274  loss_mask_0: 0.3788  loss_dice_0: 1.692  loss_ce_1: 0.812  loss_mask_1: 0.3232  loss_dice_1: 1.461  loss_ce_2: 0.5782  loss_mask_2: 0.3383  loss_dice_2: 1.48  loss_ce_3: 0.6226  loss_mask_3: 0.3473  loss_dice_3: 1.329  loss_ce_4: 0.6125  loss_mask_4: 0.3697  loss_dice_4: 1.366  loss_ce_5: 0.4325  loss_mask_5: 0.3787  loss_dice_5: 1.25  loss_ce_6: 0.4703  loss_mask_6: 0.3478  loss_dice_6: 1.297  loss_ce_7: 0.5458  loss_mask_7: 0.3463  loss_dice_7: 1.312  loss_ce_8: 0.4922  loss_mask_8: 0.3418  loss_dice_8: 1.334    time: 0.3216  last_time: 0.3338  data_time: 0.0037  last_data_time: 0.0038   lr: 8.7499e-05  max_mem: 14652M
[12/04 23:49:38] d2.utils.events INFO:  eta: 0:45:18  iter: 1399  total_loss: 24.66  loss_ce: 0.4888  loss_mask: 0.3716  loss_dice: 1.203  loss_contrastive: 0  loss_ce_0: 1.336  loss_mask_0: 0.4434  loss_dice_0: 1.288  loss_ce_1: 0.9997  loss_mask_1: 0.3974  loss_dice_1: 1.397  loss_ce_2: 0.8403  loss_mask_2: 0.393  loss_dice_2: 1.12  loss_ce_3: 0.759  loss_mask_3: 0.3788  loss_dice_3: 1.254  loss_ce_4: 0.5462  loss_mask_4: 0.4085  loss_dice_4: 1.291  loss_ce_5: 0.466  loss_mask_5: 0.3854  loss_dice_5: 1.231  loss_ce_6: 0.452  loss_mask_6: 0.3893  loss_dice_6: 1.253  loss_ce_7: 0.5107  loss_mask_7: 0.3907  loss_dice_7: 1.24  loss_ce_8: 0.5071  loss_mask_8: 0.3921  loss_dice_8: 1.285    time: 0.3216  last_time: 0.3073  data_time: 0.0039  last_data_time: 0.0038   lr: 8.7316e-05  max_mem: 14652M
[12/04 23:49:44] d2.utils.events INFO:  eta: 0:45:13  iter: 1419  total_loss: 24.82  loss_ce: 0.5597  loss_mask: 0.2703  loss_dice: 1.241  loss_contrastive: 0  loss_ce_0: 1.806  loss_mask_0: 0.3579  loss_dice_0: 1.58  loss_ce_1: 1.013  loss_mask_1: 0.3271  loss_dice_1: 1.548  loss_ce_2: 0.9006  loss_mask_2: 0.2784  loss_dice_2: 1.263  loss_ce_3: 0.8461  loss_mask_3: 0.263  loss_dice_3: 1.218  loss_ce_4: 0.6646  loss_mask_4: 0.2775  loss_dice_4: 1.271  loss_ce_5: 0.6186  loss_mask_5: 0.2887  loss_dice_5: 1.207  loss_ce_6: 0.5396  loss_mask_6: 0.2809  loss_dice_6: 1.28  loss_ce_7: 0.5352  loss_mask_7: 0.2752  loss_dice_7: 1.269  loss_ce_8: 0.5733  loss_mask_8: 0.2735  loss_dice_8: 1.247    time: 0.3216  last_time: 0.3275  data_time: 0.0034  last_data_time: 0.0054   lr: 8.7133e-05  max_mem: 14652M
[12/04 23:49:51] d2.utils.events INFO:  eta: 0:45:10  iter: 1439  total_loss: 20.45  loss_ce: 0.511  loss_mask: 0.3889  loss_dice: 1.038  loss_contrastive: 0  loss_ce_0: 1.186  loss_mask_0: 0.5633  loss_dice_0: 1.225  loss_ce_1: 0.6294  loss_mask_1: 0.4303  loss_dice_1: 1.093  loss_ce_2: 0.639  loss_mask_2: 0.4283  loss_dice_2: 1.136  loss_ce_3: 0.5892  loss_mask_3: 0.4446  loss_dice_3: 1.079  loss_ce_4: 0.4496  loss_mask_4: 0.4282  loss_dice_4: 1.088  loss_ce_5: 0.3822  loss_mask_5: 0.4044  loss_dice_5: 0.9745  loss_ce_6: 0.4883  loss_mask_6: 0.402  loss_dice_6: 1.009  loss_ce_7: 0.4628  loss_mask_7: 0.3946  loss_dice_7: 1.063  loss_ce_8: 0.4059  loss_mask_8: 0.4045  loss_dice_8: 1.037    time: 0.3216  last_time: 0.3217  data_time: 0.0034  last_data_time: 0.0048   lr: 8.695e-05  max_mem: 14652M
[12/04 23:49:57] d2.utils.events INFO:  eta: 0:45:03  iter: 1459  total_loss: 22.27  loss_ce: 0.3735  loss_mask: 0.2254  loss_dice: 1.213  loss_contrastive: 0  loss_ce_0: 1.079  loss_mask_0: 0.3134  loss_dice_0: 1.731  loss_ce_1: 0.7944  loss_mask_1: 0.2589  loss_dice_1: 1.363  loss_ce_2: 0.6065  loss_mask_2: 0.2369  loss_dice_2: 1.316  loss_ce_3: 0.4908  loss_mask_3: 0.231  loss_dice_3: 1.263  loss_ce_4: 0.4848  loss_mask_4: 0.2299  loss_dice_4: 1.259  loss_ce_5: 0.5084  loss_mask_5: 0.2845  loss_dice_5: 1.208  loss_ce_6: 0.3933  loss_mask_6: 0.2748  loss_dice_6: 1.133  loss_ce_7: 0.4117  loss_mask_7: 0.2348  loss_dice_7: 1.26  loss_ce_8: 0.3592  loss_mask_8: 0.2373  loss_dice_8: 1.251    time: 0.3215  last_time: 0.3139  data_time: 0.0036  last_data_time: 0.0027   lr: 8.6768e-05  max_mem: 14657M
[12/04 23:50:03] d2.utils.events INFO:  eta: 0:44:59  iter: 1479  total_loss: 20.95  loss_ce: 0.3812  loss_mask: 0.3941  loss_dice: 0.9452  loss_contrastive: 0  loss_ce_0: 1.362  loss_mask_0: 0.3866  loss_dice_0: 1.05  loss_ce_1: 0.6858  loss_mask_1: 0.4044  loss_dice_1: 0.9989  loss_ce_2: 0.5296  loss_mask_2: 0.3272  loss_dice_2: 0.9648  loss_ce_3: 0.4925  loss_mask_3: 0.3151  loss_dice_3: 0.9575  loss_ce_4: 0.4414  loss_mask_4: 0.3456  loss_dice_4: 0.9706  loss_ce_5: 0.487  loss_mask_5: 0.413  loss_dice_5: 1.012  loss_ce_6: 0.4374  loss_mask_6: 0.4111  loss_dice_6: 0.9228  loss_ce_7: 0.4226  loss_mask_7: 0.3845  loss_dice_7: 0.9167  loss_ce_8: 0.3585  loss_mask_8: 0.4109  loss_dice_8: 0.9902    time: 0.3215  last_time: 0.3184  data_time: 0.0038  last_data_time: 0.0033   lr: 8.6585e-05  max_mem: 14657M
[12/04 23:50:10] d2.utils.events INFO:  eta: 0:44:54  iter: 1499  total_loss: 20.36  loss_ce: 0.3267  loss_mask: 0.2768  loss_dice: 0.9779  loss_contrastive: 0  loss_ce_0: 1.165  loss_mask_0: 0.3616  loss_dice_0: 1.361  loss_ce_1: 0.7238  loss_mask_1: 0.2691  loss_dice_1: 1.322  loss_ce_2: 0.5963  loss_mask_2: 0.212  loss_dice_2: 1.143  loss_ce_3: 0.4333  loss_mask_3: 0.2558  loss_dice_3: 1.061  loss_ce_4: 0.3551  loss_mask_4: 0.2548  loss_dice_4: 1.052  loss_ce_5: 0.3165  loss_mask_5: 0.241  loss_dice_5: 0.9556  loss_ce_6: 0.3467  loss_mask_6: 0.2626  loss_dice_6: 0.9525  loss_ce_7: 0.3321  loss_mask_7: 0.2253  loss_dice_7: 1.066  loss_ce_8: 0.4222  loss_mask_8: 0.2374  loss_dice_8: 0.9649    time: 0.3216  last_time: 0.3241  data_time: 0.0037  last_data_time: 0.0065   lr: 8.6402e-05  max_mem: 14657M
[12/04 23:50:16] d2.utils.events INFO:  eta: 0:44:48  iter: 1519  total_loss: 21.95  loss_ce: 0.5269  loss_mask: 0.3754  loss_dice: 0.9085  loss_contrastive: 0  loss_ce_0: 1.783  loss_mask_0: 0.4515  loss_dice_0: 1.416  loss_ce_1: 1.11  loss_mask_1: 0.3291  loss_dice_1: 1.154  loss_ce_2: 0.9443  loss_mask_2: 0.3178  loss_dice_2: 1.05  loss_ce_3: 0.6148  loss_mask_3: 0.3242  loss_dice_3: 1.011  loss_ce_4: 0.5662  loss_mask_4: 0.3147  loss_dice_4: 0.9016  loss_ce_5: 0.5685  loss_mask_5: 0.3351  loss_dice_5: 0.8707  loss_ce_6: 0.5116  loss_mask_6: 0.3917  loss_dice_6: 1.027  loss_ce_7: 0.472  loss_mask_7: 0.3798  loss_dice_7: 1.077  loss_ce_8: 0.4711  loss_mask_8: 0.3286  loss_dice_8: 1.034    time: 0.3216  last_time: 0.3106  data_time: 0.0036  last_data_time: 0.0032   lr: 8.6219e-05  max_mem: 14657M
[12/04 23:50:23] d2.utils.events INFO:  eta: 0:44:42  iter: 1539  total_loss: 26.4  loss_ce: 0.5516  loss_mask: 0.3047  loss_dice: 1.445  loss_contrastive: 0  loss_ce_0: 1.754  loss_mask_0: 0.4531  loss_dice_0: 1.917  loss_ce_1: 1.072  loss_mask_1: 0.2814  loss_dice_1: 1.441  loss_ce_2: 0.9108  loss_mask_2: 0.2792  loss_dice_2: 1.515  loss_ce_3: 0.7824  loss_mask_3: 0.3491  loss_dice_3: 1.401  loss_ce_4: 0.6624  loss_mask_4: 0.3075  loss_dice_4: 1.34  loss_ce_5: 0.6674  loss_mask_5: 0.2339  loss_dice_5: 1.328  loss_ce_6: 0.6573  loss_mask_6: 0.2989  loss_dice_6: 1.334  loss_ce_7: 0.7208  loss_mask_7: 0.2509  loss_dice_7: 1.403  loss_ce_8: 0.6942  loss_mask_8: 0.2586  loss_dice_8: 1.445    time: 0.3217  last_time: 0.3266  data_time: 0.0033  last_data_time: 0.0034   lr: 8.6036e-05  max_mem: 14691M
[12/04 23:50:29] d2.utils.events INFO:  eta: 0:44:36  iter: 1559  total_loss: 25.14  loss_ce: 0.5597  loss_mask: 0.4026  loss_dice: 1.215  loss_contrastive: 0  loss_ce_0: 1.58  loss_mask_0: 0.4619  loss_dice_0: 1.595  loss_ce_1: 0.9923  loss_mask_1: 0.4076  loss_dice_1: 1.371  loss_ce_2: 0.7486  loss_mask_2: 0.437  loss_dice_2: 1.254  loss_ce_3: 0.6167  loss_mask_3: 0.4261  loss_dice_3: 1.185  loss_ce_4: 0.5946  loss_mask_4: 0.3673  loss_dice_4: 1.19  loss_ce_5: 0.6676  loss_mask_5: 0.443  loss_dice_5: 1.176  loss_ce_6: 0.5483  loss_mask_6: 0.3982  loss_dice_6: 1.216  loss_ce_7: 0.6171  loss_mask_7: 0.4345  loss_dice_7: 1.22  loss_ce_8: 0.6165  loss_mask_8: 0.4219  loss_dice_8: 1.11    time: 0.3217  last_time: 0.3363  data_time: 0.0040  last_data_time: 0.0085   lr: 8.5853e-05  max_mem: 14691M
[12/04 23:50:36] d2.utils.events INFO:  eta: 0:44:28  iter: 1579  total_loss: 26.98  loss_ce: 0.564  loss_mask: 0.4946  loss_dice: 1.092  loss_contrastive: 0  loss_ce_0: 1.814  loss_mask_0: 0.5254  loss_dice_0: 1.625  loss_ce_1: 1.145  loss_mask_1: 0.5644  loss_dice_1: 1.246  loss_ce_2: 0.8891  loss_mask_2: 0.5286  loss_dice_2: 1.155  loss_ce_3: 0.7358  loss_mask_3: 0.5289  loss_dice_3: 1.014  loss_ce_4: 0.7619  loss_mask_4: 0.5189  loss_dice_4: 1.038  loss_ce_5: 0.6354  loss_mask_5: 0.5138  loss_dice_5: 1.052  loss_ce_6: 0.6502  loss_mask_6: 0.5116  loss_dice_6: 1.039  loss_ce_7: 0.5642  loss_mask_7: 0.5187  loss_dice_7: 1.034  loss_ce_8: 0.5759  loss_mask_8: 0.4847  loss_dice_8: 1.044    time: 0.3217  last_time: 0.3317  data_time: 0.0034  last_data_time: 0.0031   lr: 8.567e-05  max_mem: 14691M
[12/04 23:50:42] d2.utils.events INFO:  eta: 0:44:21  iter: 1599  total_loss: 25.18  loss_ce: 0.6704  loss_mask: 0.4357  loss_dice: 1.093  loss_contrastive: 0  loss_ce_0: 1.602  loss_mask_0: 0.5224  loss_dice_0: 1.475  loss_ce_1: 0.9982  loss_mask_1: 0.3763  loss_dice_1: 1.18  loss_ce_2: 0.8269  loss_mask_2: 0.4288  loss_dice_2: 1.13  loss_ce_3: 0.7055  loss_mask_3: 0.4229  loss_dice_3: 1.091  loss_ce_4: 0.633  loss_mask_4: 0.4175  loss_dice_4: 1.124  loss_ce_5: 0.6036  loss_mask_5: 0.4667  loss_dice_5: 1.19  loss_ce_6: 0.5645  loss_mask_6: 0.4724  loss_dice_6: 1.092  loss_ce_7: 0.6163  loss_mask_7: 0.4593  loss_dice_7: 1.095  loss_ce_8: 0.7266  loss_mask_8: 0.4335  loss_dice_8: 1.065    time: 0.3217  last_time: 0.3219  data_time: 0.0035  last_data_time: 0.0024   lr: 8.5487e-05  max_mem: 14691M
[12/04 23:50:49] d2.utils.events INFO:  eta: 0:44:13  iter: 1619  total_loss: 21.89  loss_ce: 0.4755  loss_mask: 0.3116  loss_dice: 1.016  loss_contrastive: 0  loss_ce_0: 1.456  loss_mask_0: 0.4702  loss_dice_0: 1.313  loss_ce_1: 0.8188  loss_mask_1: 0.4568  loss_dice_1: 1.049  loss_ce_2: 0.6177  loss_mask_2: 0.3973  loss_dice_2: 0.9721  loss_ce_3: 0.4745  loss_mask_3: 0.3807  loss_dice_3: 1.008  loss_ce_4: 0.4482  loss_mask_4: 0.3957  loss_dice_4: 1.019  loss_ce_5: 0.4172  loss_mask_5: 0.4185  loss_dice_5: 0.979  loss_ce_6: 0.4876  loss_mask_6: 0.3776  loss_dice_6: 0.9995  loss_ce_7: 0.4764  loss_mask_7: 0.3693  loss_dice_7: 1.052  loss_ce_8: 0.4405  loss_mask_8: 0.3254  loss_dice_8: 0.9843    time: 0.3216  last_time: 0.3075  data_time: 0.0033  last_data_time: 0.0032   lr: 8.5303e-05  max_mem: 14691M
[12/04 23:50:55] d2.utils.events INFO:  eta: 0:44:08  iter: 1639  total_loss: 19.76  loss_ce: 0.4633  loss_mask: 0.4542  loss_dice: 0.9813  loss_contrastive: 0  loss_ce_0: 1.163  loss_mask_0: 0.5253  loss_dice_0: 1.199  loss_ce_1: 0.7419  loss_mask_1: 0.5107  loss_dice_1: 1.073  loss_ce_2: 0.5983  loss_mask_2: 0.497  loss_dice_2: 1.097  loss_ce_3: 0.4807  loss_mask_3: 0.4675  loss_dice_3: 0.9882  loss_ce_4: 0.4579  loss_mask_4: 0.4493  loss_dice_4: 1.043  loss_ce_5: 0.4075  loss_mask_5: 0.469  loss_dice_5: 1.035  loss_ce_6: 0.5075  loss_mask_6: 0.4697  loss_dice_6: 0.9815  loss_ce_7: 0.3457  loss_mask_7: 0.466  loss_dice_7: 1.01  loss_ce_8: 0.4494  loss_mask_8: 0.4417  loss_dice_8: 0.9997    time: 0.3216  last_time: 0.3235  data_time: 0.0034  last_data_time: 0.0035   lr: 8.512e-05  max_mem: 14691M
[12/04 23:51:02] d2.utils.events INFO:  eta: 0:44:04  iter: 1659  total_loss: 20.49  loss_ce: 0.4538  loss_mask: 0.357  loss_dice: 0.9315  loss_contrastive: 0  loss_ce_0: 1.174  loss_mask_0: 0.4145  loss_dice_0: 1.308  loss_ce_1: 0.9161  loss_mask_1: 0.3512  loss_dice_1: 1.078  loss_ce_2: 0.7222  loss_mask_2: 0.2656  loss_dice_2: 0.999  loss_ce_3: 0.5195  loss_mask_3: 0.2442  loss_dice_3: 0.9867  loss_ce_4: 0.5177  loss_mask_4: 0.2491  loss_dice_4: 0.9399  loss_ce_5: 0.483  loss_mask_5: 0.3658  loss_dice_5: 0.9207  loss_ce_6: 0.6405  loss_mask_6: 0.3725  loss_dice_6: 1.06  loss_ce_7: 0.5222  loss_mask_7: 0.4163  loss_dice_7: 0.9864  loss_ce_8: 0.5026  loss_mask_8: 0.3547  loss_dice_8: 1.07    time: 0.3216  last_time: 0.3342  data_time: 0.0032  last_data_time: 0.0028   lr: 8.4937e-05  max_mem: 14691M
[12/04 23:51:08] d2.utils.events INFO:  eta: 0:43:57  iter: 1679  total_loss: 23.26  loss_ce: 0.2236  loss_mask: 0.554  loss_dice: 0.9933  loss_contrastive: 0  loss_ce_0: 1.394  loss_mask_0: 0.6576  loss_dice_0: 1.38  loss_ce_1: 0.7285  loss_mask_1: 0.5535  loss_dice_1: 1.193  loss_ce_2: 0.5176  loss_mask_2: 0.5558  loss_dice_2: 1.21  loss_ce_3: 0.4573  loss_mask_3: 0.4893  loss_dice_3: 1.137  loss_ce_4: 0.3934  loss_mask_4: 0.5062  loss_dice_4: 1.188  loss_ce_5: 0.3528  loss_mask_5: 0.5209  loss_dice_5: 1.188  loss_ce_6: 0.2541  loss_mask_6: 0.5496  loss_dice_6: 1.074  loss_ce_7: 0.2465  loss_mask_7: 0.5409  loss_dice_7: 1.132  loss_ce_8: 0.2244  loss_mask_8: 0.5293  loss_dice_8: 1.028    time: 0.3216  last_time: 0.3081  data_time: 0.0032  last_data_time: 0.0037   lr: 8.4754e-05  max_mem: 14691M
[12/04 23:51:15] d2.utils.events INFO:  eta: 0:43:53  iter: 1699  total_loss: 22.08  loss_ce: 0.677  loss_mask: 0.3329  loss_dice: 1.218  loss_contrastive: 0  loss_ce_0: 1.423  loss_mask_0: 0.3371  loss_dice_0: 1.474  loss_ce_1: 0.9551  loss_mask_1: 0.3533  loss_dice_1: 1.321  loss_ce_2: 0.809  loss_mask_2: 0.3204  loss_dice_2: 1.333  loss_ce_3: 0.7642  loss_mask_3: 0.348  loss_dice_3: 1.216  loss_ce_4: 0.6511  loss_mask_4: 0.3049  loss_dice_4: 1.157  loss_ce_5: 0.7658  loss_mask_5: 0.2989  loss_dice_5: 1.241  loss_ce_6: 0.6767  loss_mask_6: 0.313  loss_dice_6: 1.226  loss_ce_7: 0.5949  loss_mask_7: 0.3123  loss_dice_7: 1.189  loss_ce_8: 0.5881  loss_mask_8: 0.3225  loss_dice_8: 1.163    time: 0.3217  last_time: 0.3119  data_time: 0.0035  last_data_time: 0.0029   lr: 8.457e-05  max_mem: 14691M
[12/04 23:51:21] d2.utils.events INFO:  eta: 0:43:48  iter: 1719  total_loss: 25.27  loss_ce: 0.4847  loss_mask: 0.3627  loss_dice: 1.191  loss_contrastive: 0  loss_ce_0: 1.441  loss_mask_0: 0.4122  loss_dice_0: 1.594  loss_ce_1: 0.9184  loss_mask_1: 0.3906  loss_dice_1: 1.315  loss_ce_2: 0.8621  loss_mask_2: 0.3793  loss_dice_2: 1.241  loss_ce_3: 0.6001  loss_mask_3: 0.369  loss_dice_3: 1.111  loss_ce_4: 0.5984  loss_mask_4: 0.443  loss_dice_4: 1.236  loss_ce_5: 0.5961  loss_mask_5: 0.436  loss_dice_5: 1.164  loss_ce_6: 0.5892  loss_mask_6: 0.3956  loss_dice_6: 1.104  loss_ce_7: 0.5596  loss_mask_7: 0.3763  loss_dice_7: 1.124  loss_ce_8: 0.5793  loss_mask_8: 0.3696  loss_dice_8: 1.096    time: 0.3217  last_time: 0.3017  data_time: 0.0034  last_data_time: 0.0026   lr: 8.4387e-05  max_mem: 14691M
[12/04 23:51:27] d2.utils.events INFO:  eta: 0:43:42  iter: 1739  total_loss: 22.94  loss_ce: 0.4877  loss_mask: 0.3173  loss_dice: 1.231  loss_contrastive: 0  loss_ce_0: 1.351  loss_mask_0: 0.3688  loss_dice_0: 1.808  loss_ce_1: 0.731  loss_mask_1: 0.343  loss_dice_1: 1.412  loss_ce_2: 0.5295  loss_mask_2: 0.325  loss_dice_2: 1.364  loss_ce_3: 0.5176  loss_mask_3: 0.3005  loss_dice_3: 1.283  loss_ce_4: 0.5511  loss_mask_4: 0.334  loss_dice_4: 1.268  loss_ce_5: 0.4223  loss_mask_5: 0.3253  loss_dice_5: 1.291  loss_ce_6: 0.4639  loss_mask_6: 0.31  loss_dice_6: 1.235  loss_ce_7: 0.4423  loss_mask_7: 0.3112  loss_dice_7: 1.287  loss_ce_8: 0.3873  loss_mask_8: 0.3225  loss_dice_8: 1.202    time: 0.3216  last_time: 0.3451  data_time: 0.0038  last_data_time: 0.0056   lr: 8.4203e-05  max_mem: 14691M
[12/04 23:51:34] d2.utils.events INFO:  eta: 0:43:35  iter: 1759  total_loss: 23.15  loss_ce: 0.5165  loss_mask: 0.3565  loss_dice: 1.031  loss_contrastive: 0  loss_ce_0: 1.348  loss_mask_0: 0.4429  loss_dice_0: 1.306  loss_ce_1: 0.7277  loss_mask_1: 0.3849  loss_dice_1: 1.099  loss_ce_2: 0.6348  loss_mask_2: 0.3438  loss_dice_2: 1.037  loss_ce_3: 0.5088  loss_mask_3: 0.3495  loss_dice_3: 0.9782  loss_ce_4: 0.5643  loss_mask_4: 0.3392  loss_dice_4: 0.9391  loss_ce_5: 0.5243  loss_mask_5: 0.3372  loss_dice_5: 0.9924  loss_ce_6: 0.4502  loss_mask_6: 0.3618  loss_dice_6: 1.096  loss_ce_7: 0.4109  loss_mask_7: 0.3716  loss_dice_7: 1.005  loss_ce_8: 0.4084  loss_mask_8: 0.3556  loss_dice_8: 0.9633    time: 0.3216  last_time: 0.3233  data_time: 0.0036  last_data_time: 0.0029   lr: 8.402e-05  max_mem: 14691M
[12/04 23:51:40] d2.utils.events INFO:  eta: 0:43:29  iter: 1779  total_loss: 20.85  loss_ce: 0.487  loss_mask: 0.2808  loss_dice: 0.9867  loss_contrastive: 0  loss_ce_0: 1.498  loss_mask_0: 0.3244  loss_dice_0: 1.352  loss_ce_1: 0.9496  loss_mask_1: 0.3099  loss_dice_1: 1.149  loss_ce_2: 0.6377  loss_mask_2: 0.2815  loss_dice_2: 1.195  loss_ce_3: 0.6393  loss_mask_3: 0.2693  loss_dice_3: 1.077  loss_ce_4: 0.604  loss_mask_4: 0.2678  loss_dice_4: 1.073  loss_ce_5: 0.5961  loss_mask_5: 0.2714  loss_dice_5: 1.039  loss_ce_6: 0.5388  loss_mask_6: 0.1999  loss_dice_6: 1.042  loss_ce_7: 0.5865  loss_mask_7: 0.1884  loss_dice_7: 1.039  loss_ce_8: 0.5617  loss_mask_8: 0.2046  loss_dice_8: 1.052    time: 0.3216  last_time: 0.3328  data_time: 0.0034  last_data_time: 0.0056   lr: 8.3836e-05  max_mem: 14691M
[12/04 23:51:47] d2.utils.events INFO:  eta: 0:43:24  iter: 1799  total_loss: 17.1  loss_ce: 0.2517  loss_mask: 0.2464  loss_dice: 1.045  loss_contrastive: 0  loss_ce_0: 1.054  loss_mask_0: 0.3112  loss_dice_0: 1.134  loss_ce_1: 0.6399  loss_mask_1: 0.2453  loss_dice_1: 1.014  loss_ce_2: 0.5667  loss_mask_2: 0.2085  loss_dice_2: 0.9783  loss_ce_3: 0.4518  loss_mask_3: 0.2355  loss_dice_3: 1.036  loss_ce_4: 0.3206  loss_mask_4: 0.2525  loss_dice_4: 0.9434  loss_ce_5: 0.3936  loss_mask_5: 0.2724  loss_dice_5: 0.8349  loss_ce_6: 0.2893  loss_mask_6: 0.2547  loss_dice_6: 0.9908  loss_ce_7: 0.2662  loss_mask_7: 0.2526  loss_dice_7: 0.878  loss_ce_8: 0.2832  loss_mask_8: 0.2419  loss_dice_8: 1.037    time: 0.3216  last_time: 0.3115  data_time: 0.0031  last_data_time: 0.0030   lr: 8.3653e-05  max_mem: 14691M
[12/04 23:51:53] d2.utils.events INFO:  eta: 0:43:18  iter: 1819  total_loss: 22.65  loss_ce: 0.5857  loss_mask: 0.3316  loss_dice: 1.185  loss_contrastive: 0  loss_ce_0: 1.444  loss_mask_0: 0.3986  loss_dice_0: 1.679  loss_ce_1: 0.8638  loss_mask_1: 0.3174  loss_dice_1: 1.298  loss_ce_2: 0.7233  loss_mask_2: 0.3534  loss_dice_2: 1.319  loss_ce_3: 0.6302  loss_mask_3: 0.3136  loss_dice_3: 1.109  loss_ce_4: 0.5549  loss_mask_4: 0.3631  loss_dice_4: 1.17  loss_ce_5: 0.5206  loss_mask_5: 0.3454  loss_dice_5: 1.164  loss_ce_6: 0.6182  loss_mask_6: 0.3484  loss_dice_6: 1.156  loss_ce_7: 0.563  loss_mask_7: 0.3184  loss_dice_7: 1.191  loss_ce_8: 0.5909  loss_mask_8: 0.3178  loss_dice_8: 1.22    time: 0.3216  last_time: 0.3342  data_time: 0.0032  last_data_time: 0.0031   lr: 8.3469e-05  max_mem: 14691M
[12/04 23:52:00] d2.utils.events INFO:  eta: 0:43:13  iter: 1839  total_loss: 21.99  loss_ce: 0.543  loss_mask: 0.3112  loss_dice: 1.018  loss_contrastive: 0  loss_ce_0: 1.4  loss_mask_0: 0.5693  loss_dice_0: 1.208  loss_ce_1: 0.9393  loss_mask_1: 0.3398  loss_dice_1: 1.019  loss_ce_2: 0.6198  loss_mask_2: 0.4088  loss_dice_2: 0.9835  loss_ce_3: 0.6665  loss_mask_3: 0.326  loss_dice_3: 0.9868  loss_ce_4: 0.6238  loss_mask_4: 0.3411  loss_dice_4: 1.007  loss_ce_5: 0.5469  loss_mask_5: 0.3328  loss_dice_5: 1.015  loss_ce_6: 0.5893  loss_mask_6: 0.2941  loss_dice_6: 1.061  loss_ce_7: 0.5484  loss_mask_7: 0.3004  loss_dice_7: 0.9764  loss_ce_8: 0.5063  loss_mask_8: 0.3339  loss_dice_8: 1.11    time: 0.3216  last_time: 0.3576  data_time: 0.0035  last_data_time: 0.0069   lr: 8.3285e-05  max_mem: 14691M
[12/04 23:52:06] d2.utils.events INFO:  eta: 0:43:06  iter: 1859  total_loss: 23.3  loss_ce: 0.3553  loss_mask: 0.3986  loss_dice: 1.013  loss_contrastive: 0  loss_ce_0: 1.162  loss_mask_0: 0.342  loss_dice_0: 1.363  loss_ce_1: 0.6976  loss_mask_1: 0.2957  loss_dice_1: 1.123  loss_ce_2: 0.5904  loss_mask_2: 0.3997  loss_dice_2: 1.133  loss_ce_3: 0.5238  loss_mask_3: 0.3154  loss_dice_3: 1.11  loss_ce_4: 0.4504  loss_mask_4: 0.4026  loss_dice_4: 1.127  loss_ce_5: 0.4053  loss_mask_5: 0.3876  loss_dice_5: 1.079  loss_ce_6: 0.4434  loss_mask_6: 0.3644  loss_dice_6: 1.061  loss_ce_7: 0.4344  loss_mask_7: 0.3883  loss_dice_7: 1.019  loss_ce_8: 0.4249  loss_mask_8: 0.3638  loss_dice_8: 1.008    time: 0.3217  last_time: 0.3098  data_time: 0.0034  last_data_time: 0.0026   lr: 8.3102e-05  max_mem: 14757M
[12/04 23:52:13] d2.utils.events INFO:  eta: 0:43:00  iter: 1879  total_loss: 24.34  loss_ce: 0.5224  loss_mask: 0.4346  loss_dice: 1.15  loss_contrastive: 0  loss_ce_0: 1.263  loss_mask_0: 0.4643  loss_dice_0: 1.473  loss_ce_1: 0.8848  loss_mask_1: 0.5102  loss_dice_1: 1.045  loss_ce_2: 0.6853  loss_mask_2: 0.4285  loss_dice_2: 1.178  loss_ce_3: 0.857  loss_mask_3: 0.4221  loss_dice_3: 1.114  loss_ce_4: 0.6736  loss_mask_4: 0.4361  loss_dice_4: 1.097  loss_ce_5: 0.5188  loss_mask_5: 0.4904  loss_dice_5: 1.092  loss_ce_6: 0.586  loss_mask_6: 0.448  loss_dice_6: 1.067  loss_ce_7: 0.5929  loss_mask_7: 0.4361  loss_dice_7: 1.104  loss_ce_8: 0.5518  loss_mask_8: 0.4464  loss_dice_8: 1.182    time: 0.3218  last_time: 0.3105  data_time: 0.0034  last_data_time: 0.0035   lr: 8.2918e-05  max_mem: 14757M
[12/04 23:52:19] d2.utils.events INFO:  eta: 0:42:54  iter: 1899  total_loss: 22.65  loss_ce: 0.4925  loss_mask: 0.2675  loss_dice: 1.08  loss_contrastive: 0  loss_ce_0: 1.504  loss_mask_0: 0.477  loss_dice_0: 1.479  loss_ce_1: 0.9153  loss_mask_1: 0.3575  loss_dice_1: 1.043  loss_ce_2: 0.7588  loss_mask_2: 0.267  loss_dice_2: 1.163  loss_ce_3: 0.5966  loss_mask_3: 0.2902  loss_dice_3: 1.089  loss_ce_4: 0.5643  loss_mask_4: 0.2698  loss_dice_4: 1.157  loss_ce_5: 0.572  loss_mask_5: 0.2667  loss_dice_5: 1.105  loss_ce_6: 0.44  loss_mask_6: 0.3214  loss_dice_6: 1.092  loss_ce_7: 0.4112  loss_mask_7: 0.2697  loss_dice_7: 1.137  loss_ce_8: 0.4826  loss_mask_8: 0.2588  loss_dice_8: 0.9831    time: 0.3218  last_time: 0.3320  data_time: 0.0037  last_data_time: 0.0028   lr: 8.2734e-05  max_mem: 14757M
[12/04 23:52:26] d2.utils.events INFO:  eta: 0:42:47  iter: 1919  total_loss: 18.06  loss_ce: 0.278  loss_mask: 0.2872  loss_dice: 0.8413  loss_contrastive: 0  loss_ce_0: 1.194  loss_mask_0: 0.3061  loss_dice_0: 1.041  loss_ce_1: 0.5628  loss_mask_1: 0.3044  loss_dice_1: 0.9171  loss_ce_2: 0.456  loss_mask_2: 0.2994  loss_dice_2: 0.9729  loss_ce_3: 0.3948  loss_mask_3: 0.2765  loss_dice_3: 1.102  loss_ce_4: 0.456  loss_mask_4: 0.2798  loss_dice_4: 0.9318  loss_ce_5: 0.3732  loss_mask_5: 0.2733  loss_dice_5: 0.9553  loss_ce_6: 0.3371  loss_mask_6: 0.2918  loss_dice_6: 0.8471  loss_ce_7: 0.351  loss_mask_7: 0.2952  loss_dice_7: 0.835  loss_ce_8: 0.284  loss_mask_8: 0.2976  loss_dice_8: 1.01    time: 0.3218  last_time: 0.3095  data_time: 0.0036  last_data_time: 0.0022   lr: 8.255e-05  max_mem: 14757M
[12/04 23:52:32] d2.utils.events INFO:  eta: 0:42:43  iter: 1939  total_loss: 23.3  loss_ce: 0.4446  loss_mask: 0.4545  loss_dice: 0.9504  loss_contrastive: 0  loss_ce_0: 1.357  loss_mask_0: 0.4626  loss_dice_0: 1.096  loss_ce_1: 0.867  loss_mask_1: 0.4463  loss_dice_1: 1.091  loss_ce_2: 0.7545  loss_mask_2: 0.3898  loss_dice_2: 1.046  loss_ce_3: 0.5733  loss_mask_3: 0.3969  loss_dice_3: 0.9708  loss_ce_4: 0.5229  loss_mask_4: 0.3721  loss_dice_4: 1.016  loss_ce_5: 0.4166  loss_mask_5: 0.3828  loss_dice_5: 0.9901  loss_ce_6: 0.3415  loss_mask_6: 0.462  loss_dice_6: 1.014  loss_ce_7: 0.4483  loss_mask_7: 0.4325  loss_dice_7: 0.9981  loss_ce_8: 0.4887  loss_mask_8: 0.4379  loss_dice_8: 0.9974    time: 0.3218  last_time: 0.3251  data_time: 0.0035  last_data_time: 0.0027   lr: 8.2366e-05  max_mem: 14757M
[12/04 23:52:39] d2.utils.events INFO:  eta: 0:42:40  iter: 1959  total_loss: 22.84  loss_ce: 0.428  loss_mask: 0.3917  loss_dice: 1.062  loss_contrastive: 0  loss_ce_0: 1.15  loss_mask_0: 0.453  loss_dice_0: 1.322  loss_ce_1: 0.5857  loss_mask_1: 0.411  loss_dice_1: 1.188  loss_ce_2: 0.7255  loss_mask_2: 0.3966  loss_dice_2: 1.105  loss_ce_3: 0.4744  loss_mask_3: 0.3765  loss_dice_3: 0.9801  loss_ce_4: 0.4398  loss_mask_4: 0.3781  loss_dice_4: 1.01  loss_ce_5: 0.3604  loss_mask_5: 0.3892  loss_dice_5: 1.079  loss_ce_6: 0.426  loss_mask_6: 0.3935  loss_dice_6: 1.084  loss_ce_7: 0.3995  loss_mask_7: 0.4746  loss_dice_7: 1.141  loss_ce_8: 0.4031  loss_mask_8: 0.4497  loss_dice_8: 1.179    time: 0.3219  last_time: 0.3260  data_time: 0.0034  last_data_time: 0.0060   lr: 8.2182e-05  max_mem: 14757M
[12/04 23:52:45] d2.utils.events INFO:  eta: 0:42:33  iter: 1979  total_loss: 21.2  loss_ce: 0.3811  loss_mask: 0.2612  loss_dice: 0.9788  loss_contrastive: 0  loss_ce_0: 1.417  loss_mask_0: 0.2721  loss_dice_0: 1.437  loss_ce_1: 0.896  loss_mask_1: 0.2923  loss_dice_1: 1.156  loss_ce_2: 0.5907  loss_mask_2: 0.2851  loss_dice_2: 1.1  loss_ce_3: 0.4871  loss_mask_3: 0.273  loss_dice_3: 0.9879  loss_ce_4: 0.503  loss_mask_4: 0.2674  loss_dice_4: 1.115  loss_ce_5: 0.5018  loss_mask_5: 0.2673  loss_dice_5: 0.9687  loss_ce_6: 0.3619  loss_mask_6: 0.2739  loss_dice_6: 0.9853  loss_ce_7: 0.2705  loss_mask_7: 0.2727  loss_dice_7: 1.055  loss_ce_8: 0.2985  loss_mask_8: 0.2671  loss_dice_8: 1.029    time: 0.3218  last_time: 0.3190  data_time: 0.0040  last_data_time: 0.0031   lr: 8.1998e-05  max_mem: 14757M
[12/04 23:52:52] d2.utils.events INFO:  eta: 0:42:32  iter: 1999  total_loss: 27.74  loss_ce: 0.6083  loss_mask: 0.4959  loss_dice: 1.242  loss_contrastive: 0  loss_ce_0: 1.312  loss_mask_0: 0.5816  loss_dice_0: 1.375  loss_ce_1: 1.083  loss_mask_1: 0.5186  loss_dice_1: 1.358  loss_ce_2: 1.033  loss_mask_2: 0.4961  loss_dice_2: 1.319  loss_ce_3: 0.9085  loss_mask_3: 0.486  loss_dice_3: 1.307  loss_ce_4: 0.6893  loss_mask_4: 0.4945  loss_dice_4: 1.352  loss_ce_5: 0.7725  loss_mask_5: 0.4742  loss_dice_5: 1.274  loss_ce_6: 0.6188  loss_mask_6: 0.5219  loss_dice_6: 1.293  loss_ce_7: 0.6636  loss_mask_7: 0.5136  loss_dice_7: 1.293  loss_ce_8: 0.5882  loss_mask_8: 0.4922  loss_dice_8: 1.267    time: 0.3218  last_time: 0.3339  data_time: 0.0035  last_data_time: 0.0027   lr: 8.1814e-05  max_mem: 14757M
[12/04 23:52:58] d2.utils.events INFO:  eta: 0:42:25  iter: 2019  total_loss: 23.46  loss_ce: 0.483  loss_mask: 0.3887  loss_dice: 0.8058  loss_contrastive: 0  loss_ce_0: 1.287  loss_mask_0: 0.4369  loss_dice_0: 0.982  loss_ce_1: 0.7665  loss_mask_1: 0.4129  loss_dice_1: 1.033  loss_ce_2: 0.6223  loss_mask_2: 0.3872  loss_dice_2: 0.8305  loss_ce_3: 0.5081  loss_mask_3: 0.3936  loss_dice_3: 0.9128  loss_ce_4: 0.5371  loss_mask_4: 0.4015  loss_dice_4: 0.9091  loss_ce_5: 0.5081  loss_mask_5: 0.384  loss_dice_5: 0.8548  loss_ce_6: 0.5327  loss_mask_6: 0.3973  loss_dice_6: 0.8019  loss_ce_7: 0.5012  loss_mask_7: 0.3876  loss_dice_7: 0.7535  loss_ce_8: 0.54  loss_mask_8: 0.3929  loss_dice_8: 0.8089    time: 0.3218  last_time: 0.3100  data_time: 0.0037  last_data_time: 0.0046   lr: 8.163e-05  max_mem: 14757M
[12/04 23:53:04] d2.utils.events INFO:  eta: 0:42:19  iter: 2039  total_loss: 21.32  loss_ce: 0.3098  loss_mask: 0.2523  loss_dice: 0.8987  loss_contrastive: 0  loss_ce_0: 0.8857  loss_mask_0: 0.3539  loss_dice_0: 0.9898  loss_ce_1: 0.5657  loss_mask_1: 0.2692  loss_dice_1: 0.908  loss_ce_2: 0.457  loss_mask_2: 0.2753  loss_dice_2: 0.9834  loss_ce_3: 0.3127  loss_mask_3: 0.2642  loss_dice_3: 0.9551  loss_ce_4: 0.3089  loss_mask_4: 0.2581  loss_dice_4: 1.031  loss_ce_5: 0.3457  loss_mask_5: 0.2634  loss_dice_5: 0.9738  loss_ce_6: 0.3306  loss_mask_6: 0.264  loss_dice_6: 0.9752  loss_ce_7: 0.3245  loss_mask_7: 0.2658  loss_dice_7: 0.8807  loss_ce_8: 0.291  loss_mask_8: 0.2648  loss_dice_8: 0.9823    time: 0.3218  last_time: 0.3215  data_time: 0.0030  last_data_time: 0.0029   lr: 8.1446e-05  max_mem: 14757M
[12/04 23:53:11] d2.utils.events INFO:  eta: 0:42:10  iter: 2059  total_loss: 20.03  loss_ce: 0.3589  loss_mask: 0.2696  loss_dice: 1.061  loss_contrastive: 0  loss_ce_0: 1.119  loss_mask_0: 0.3022  loss_dice_0: 1.492  loss_ce_1: 0.8117  loss_mask_1: 0.2772  loss_dice_1: 1.302  loss_ce_2: 0.7379  loss_mask_2: 0.2714  loss_dice_2: 1.009  loss_ce_3: 0.5559  loss_mask_3: 0.2672  loss_dice_3: 1.113  loss_ce_4: 0.5013  loss_mask_4: 0.2771  loss_dice_4: 0.9835  loss_ce_5: 0.4669  loss_mask_5: 0.2775  loss_dice_5: 1.018  loss_ce_6: 0.3948  loss_mask_6: 0.2741  loss_dice_6: 0.9817  loss_ce_7: 0.383  loss_mask_7: 0.2775  loss_dice_7: 1.002  loss_ce_8: 0.3824  loss_mask_8: 0.2795  loss_dice_8: 1.059    time: 0.3217  last_time: 0.3111  data_time: 0.0034  last_data_time: 0.0034   lr: 8.1262e-05  max_mem: 14757M
[12/04 23:53:17] d2.utils.events INFO:  eta: 0:42:03  iter: 2079  total_loss: 20.68  loss_ce: 0.3594  loss_mask: 0.327  loss_dice: 1.015  loss_contrastive: 0  loss_ce_0: 1.293  loss_mask_0: 0.3252  loss_dice_0: 1.208  loss_ce_1: 0.6158  loss_mask_1: 0.2987  loss_dice_1: 1.081  loss_ce_2: 0.602  loss_mask_2: 0.3134  loss_dice_2: 1.089  loss_ce_3: 0.488  loss_mask_3: 0.2867  loss_dice_3: 1.109  loss_ce_4: 0.406  loss_mask_4: 0.2801  loss_dice_4: 1.079  loss_ce_5: 0.2943  loss_mask_5: 0.3136  loss_dice_5: 1.069  loss_ce_6: 0.3452  loss_mask_6: 0.2895  loss_dice_6: 1.06  loss_ce_7: 0.353  loss_mask_7: 0.3208  loss_dice_7: 1.091  loss_ce_8: 0.5056  loss_mask_8: 0.3269  loss_dice_8: 1.077    time: 0.3218  last_time: 0.3120  data_time: 0.0034  last_data_time: 0.0030   lr: 8.1078e-05  max_mem: 14757M
[12/04 23:53:24] d2.utils.events INFO:  eta: 0:41:57  iter: 2099  total_loss: 20.4  loss_ce: 0.3847  loss_mask: 0.3444  loss_dice: 1.069  loss_contrastive: 0  loss_ce_0: 1.342  loss_mask_0: 0.3525  loss_dice_0: 1.253  loss_ce_1: 0.9426  loss_mask_1: 0.3464  loss_dice_1: 1.183  loss_ce_2: 0.7371  loss_mask_2: 0.3368  loss_dice_2: 1.191  loss_ce_3: 0.6415  loss_mask_3: 0.334  loss_dice_3: 1.007  loss_ce_4: 0.568  loss_mask_4: 0.3411  loss_dice_4: 0.8966  loss_ce_5: 0.5426  loss_mask_5: 0.3336  loss_dice_5: 0.9274  loss_ce_6: 0.5119  loss_mask_6: 0.3317  loss_dice_6: 1.057  loss_ce_7: 0.4329  loss_mask_7: 0.338  loss_dice_7: 0.9526  loss_ce_8: 0.4183  loss_mask_8: 0.3538  loss_dice_8: 1.033    time: 0.3218  last_time: 0.3277  data_time: 0.0036  last_data_time: 0.0033   lr: 8.0894e-05  max_mem: 14757M
[12/04 23:53:30] d2.utils.events INFO:  eta: 0:41:54  iter: 2119  total_loss: 18.79  loss_ce: 0.5451  loss_mask: 0.339  loss_dice: 0.8896  loss_contrastive: 0  loss_ce_0: 1.28  loss_mask_0: 0.457  loss_dice_0: 1.126  loss_ce_1: 0.7338  loss_mask_1: 0.3282  loss_dice_1: 0.9055  loss_ce_2: 0.5244  loss_mask_2: 0.3267  loss_dice_2: 0.9308  loss_ce_3: 0.5759  loss_mask_3: 0.3434  loss_dice_3: 0.8844  loss_ce_4: 0.4505  loss_mask_4: 0.3448  loss_dice_4: 0.8732  loss_ce_5: 0.4942  loss_mask_5: 0.3528  loss_dice_5: 0.9129  loss_ce_6: 0.4501  loss_mask_6: 0.3291  loss_dice_6: 0.87  loss_ce_7: 0.4915  loss_mask_7: 0.3465  loss_dice_7: 0.8358  loss_ce_8: 0.5408  loss_mask_8: 0.3385  loss_dice_8: 0.869    time: 0.3218  last_time: 0.3152  data_time: 0.0036  last_data_time: 0.0060   lr: 8.0709e-05  max_mem: 14757M
[12/04 23:53:37] d2.utils.events INFO:  eta: 0:41:48  iter: 2139  total_loss: 21.33  loss_ce: 0.5752  loss_mask: 0.3138  loss_dice: 1.028  loss_contrastive: 0  loss_ce_0: 1.332  loss_mask_0: 0.3769  loss_dice_0: 1.346  loss_ce_1: 0.8436  loss_mask_1: 0.3175  loss_dice_1: 1.098  loss_ce_2: 0.6948  loss_mask_2: 0.3221  loss_dice_2: 1.091  loss_ce_3: 0.6288  loss_mask_3: 0.3088  loss_dice_3: 1.101  loss_ce_4: 0.5961  loss_mask_4: 0.3232  loss_dice_4: 1.129  loss_ce_5: 0.5491  loss_mask_5: 0.3258  loss_dice_5: 1.044  loss_ce_6: 0.5409  loss_mask_6: 0.3229  loss_dice_6: 1.025  loss_ce_7: 0.5695  loss_mask_7: 0.2973  loss_dice_7: 1.035  loss_ce_8: 0.5568  loss_mask_8: 0.3048  loss_dice_8: 1.092    time: 0.3218  last_time: 0.3158  data_time: 0.0034  last_data_time: 0.0028   lr: 8.0525e-05  max_mem: 14757M
[12/04 23:53:43] d2.utils.events INFO:  eta: 0:41:39  iter: 2159  total_loss: 20.14  loss_ce: 0.5696  loss_mask: 0.207  loss_dice: 1.098  loss_contrastive: 0  loss_ce_0: 1.381  loss_mask_0: 0.3209  loss_dice_0: 1.402  loss_ce_1: 0.92  loss_mask_1: 0.2334  loss_dice_1: 1.112  loss_ce_2: 0.7036  loss_mask_2: 0.2723  loss_dice_2: 1.018  loss_ce_3: 0.6144  loss_mask_3: 0.2383  loss_dice_3: 1.117  loss_ce_4: 0.6364  loss_mask_4: 0.2245  loss_dice_4: 1.015  loss_ce_5: 0.5105  loss_mask_5: 0.233  loss_dice_5: 1.065  loss_ce_6: 0.3744  loss_mask_6: 0.2094  loss_dice_6: 1.155  loss_ce_7: 0.5015  loss_mask_7: 0.1895  loss_dice_7: 1.107  loss_ce_8: 0.4223  loss_mask_8: 0.1996  loss_dice_8: 1.077    time: 0.3218  last_time: 0.3042  data_time: 0.0032  last_data_time: 0.0032   lr: 8.034e-05  max_mem: 14757M
[12/04 23:53:50] d2.utils.events INFO:  eta: 0:41:34  iter: 2179  total_loss: 25.72  loss_ce: 0.4738  loss_mask: 0.2705  loss_dice: 1.175  loss_contrastive: 0  loss_ce_0: 1.288  loss_mask_0: 0.3355  loss_dice_0: 1.485  loss_ce_1: 0.8751  loss_mask_1: 0.3768  loss_dice_1: 1.264  loss_ce_2: 0.7808  loss_mask_2: 0.4672  loss_dice_2: 1.248  loss_ce_3: 0.7177  loss_mask_3: 0.3267  loss_dice_3: 1.192  loss_ce_4: 0.5374  loss_mask_4: 0.2925  loss_dice_4: 1.214  loss_ce_5: 0.625  loss_mask_5: 0.3092  loss_dice_5: 1.2  loss_ce_6: 0.5115  loss_mask_6: 0.3172  loss_dice_6: 1.21  loss_ce_7: 0.5112  loss_mask_7: 0.3103  loss_dice_7: 1.229  loss_ce_8: 0.4959  loss_mask_8: 0.2833  loss_dice_8: 1.248    time: 0.3218  last_time: 0.3072  data_time: 0.0033  last_data_time: 0.0026   lr: 8.0156e-05  max_mem: 14757M
[12/04 23:53:56] d2.utils.events INFO:  eta: 0:41:26  iter: 2199  total_loss: 21.49  loss_ce: 0.3839  loss_mask: 0.262  loss_dice: 1.214  loss_contrastive: 0  loss_ce_0: 1.4  loss_mask_0: 0.2847  loss_dice_0: 1.328  loss_ce_1: 0.8186  loss_mask_1: 0.3588  loss_dice_1: 1.443  loss_ce_2: 0.712  loss_mask_2: 0.3117  loss_dice_2: 1.226  loss_ce_3: 0.5669  loss_mask_3: 0.2547  loss_dice_3: 1.103  loss_ce_4: 0.6008  loss_mask_4: 0.2566  loss_dice_4: 1.133  loss_ce_5: 0.5243  loss_mask_5: 0.2548  loss_dice_5: 1.113  loss_ce_6: 0.4162  loss_mask_6: 0.2425  loss_dice_6: 1.046  loss_ce_7: 0.5132  loss_mask_7: 0.2715  loss_dice_7: 1.169  loss_ce_8: 0.4286  loss_mask_8: 0.2975  loss_dice_8: 1.082    time: 0.3218  last_time: 0.3264  data_time: 0.0037  last_data_time: 0.0032   lr: 7.9972e-05  max_mem: 14757M
[12/04 23:54:03] d2.utils.events INFO:  eta: 0:41:19  iter: 2219  total_loss: 24.18  loss_ce: 0.6339  loss_mask: 0.2035  loss_dice: 1.098  loss_contrastive: 0  loss_ce_0: 1.217  loss_mask_0: 0.3074  loss_dice_0: 1.394  loss_ce_1: 0.9332  loss_mask_1: 0.2555  loss_dice_1: 1.293  loss_ce_2: 0.9787  loss_mask_2: 0.2207  loss_dice_2: 1.176  loss_ce_3: 0.7611  loss_mask_3: 0.2332  loss_dice_3: 1.278  loss_ce_4: 0.689  loss_mask_4: 0.2031  loss_dice_4: 1.139  loss_ce_5: 0.5873  loss_mask_5: 0.228  loss_dice_5: 1.168  loss_ce_6: 0.6645  loss_mask_6: 0.2027  loss_dice_6: 1.201  loss_ce_7: 0.607  loss_mask_7: 0.2207  loss_dice_7: 1.181  loss_ce_8: 0.6933  loss_mask_8: 0.2132  loss_dice_8: 1.143    time: 0.3218  last_time: 0.3067  data_time: 0.0034  last_data_time: 0.0025   lr: 7.9787e-05  max_mem: 14757M
[12/04 23:54:09] d2.utils.events INFO:  eta: 0:41:13  iter: 2239  total_loss: 23.64  loss_ce: 0.5637  loss_mask: 0.2203  loss_dice: 1.157  loss_contrastive: 0  loss_ce_0: 1.364  loss_mask_0: 0.274  loss_dice_0: 1.537  loss_ce_1: 0.928  loss_mask_1: 0.2527  loss_dice_1: 1.328  loss_ce_2: 0.7679  loss_mask_2: 0.2479  loss_dice_2: 1.231  loss_ce_3: 0.7912  loss_mask_3: 0.2433  loss_dice_3: 1.12  loss_ce_4: 0.6757  loss_mask_4: 0.2101  loss_dice_4: 1.163  loss_ce_5: 0.6778  loss_mask_5: 0.2146  loss_dice_5: 1.158  loss_ce_6: 0.5931  loss_mask_6: 0.206  loss_dice_6: 1.188  loss_ce_7: 0.4938  loss_mask_7: 0.2238  loss_dice_7: 1.166  loss_ce_8: 0.5096  loss_mask_8: 0.209  loss_dice_8: 1.186    time: 0.3218  last_time: 0.3210  data_time: 0.0036  last_data_time: 0.0026   lr: 7.9602e-05  max_mem: 14757M
[12/04 23:54:15] d2.utils.events INFO:  eta: 0:41:06  iter: 2259  total_loss: 17.9  loss_ce: 0.2445  loss_mask: 0.3211  loss_dice: 0.8847  loss_contrastive: 0  loss_ce_0: 0.95  loss_mask_0: 0.3943  loss_dice_0: 1.19  loss_ce_1: 0.5044  loss_mask_1: 0.3014  loss_dice_1: 0.8708  loss_ce_2: 0.4524  loss_mask_2: 0.3762  loss_dice_2: 0.8519  loss_ce_3: 0.4497  loss_mask_3: 0.3246  loss_dice_3: 0.8084  loss_ce_4: 0.4537  loss_mask_4: 0.2869  loss_dice_4: 0.9279  loss_ce_5: 0.3393  loss_mask_5: 0.3049  loss_dice_5: 0.692  loss_ce_6: 0.221  loss_mask_6: 0.3161  loss_dice_6: 0.8828  loss_ce_7: 0.2618  loss_mask_7: 0.3391  loss_dice_7: 0.8573  loss_ce_8: 0.2402  loss_mask_8: 0.3155  loss_dice_8: 0.8118    time: 0.3218  last_time: 0.3048  data_time: 0.0032  last_data_time: 0.0025   lr: 7.9418e-05  max_mem: 14757M
[12/04 23:54:22] d2.utils.events INFO:  eta: 0:41:00  iter: 2279  total_loss: 20.06  loss_ce: 0.3368  loss_mask: 0.3347  loss_dice: 1.039  loss_contrastive: 0  loss_ce_0: 1.107  loss_mask_0: 0.4067  loss_dice_0: 1.311  loss_ce_1: 0.787  loss_mask_1: 0.3586  loss_dice_1: 1.127  loss_ce_2: 0.6222  loss_mask_2: 0.3837  loss_dice_2: 1.026  loss_ce_3: 0.4495  loss_mask_3: 0.3367  loss_dice_3: 1.055  loss_ce_4: 0.4511  loss_mask_4: 0.3064  loss_dice_4: 1.024  loss_ce_5: 0.4002  loss_mask_5: 0.3361  loss_dice_5: 1.108  loss_ce_6: 0.3904  loss_mask_6: 0.3241  loss_dice_6: 1.031  loss_ce_7: 0.3283  loss_mask_7: 0.3208  loss_dice_7: 1.056  loss_ce_8: 0.364  loss_mask_8: 0.332  loss_dice_8: 1.042    time: 0.3218  last_time: 0.3104  data_time: 0.0034  last_data_time: 0.0034   lr: 7.9233e-05  max_mem: 14757M
[12/04 23:54:28] d2.utils.events INFO:  eta: 0:40:52  iter: 2299  total_loss: 20.31  loss_ce: 0.5043  loss_mask: 0.3726  loss_dice: 0.9612  loss_contrastive: 0  loss_ce_0: 1.316  loss_mask_0: 0.4029  loss_dice_0: 1.369  loss_ce_1: 0.736  loss_mask_1: 0.3639  loss_dice_1: 1.113  loss_ce_2: 0.6781  loss_mask_2: 0.3908  loss_dice_2: 1.051  loss_ce_3: 0.6395  loss_mask_3: 0.3673  loss_dice_3: 0.9574  loss_ce_4: 0.6338  loss_mask_4: 0.3746  loss_dice_4: 0.9929  loss_ce_5: 0.5477  loss_mask_5: 0.3632  loss_dice_5: 0.9348  loss_ce_6: 0.4347  loss_mask_6: 0.372  loss_dice_6: 1.016  loss_ce_7: 0.4639  loss_mask_7: 0.3674  loss_dice_7: 1.046  loss_ce_8: 0.5045  loss_mask_8: 0.356  loss_dice_8: 0.9775    time: 0.3217  last_time: 0.3268  data_time: 0.0043  last_data_time: 0.0036   lr: 7.9048e-05  max_mem: 14757M
[12/04 23:54:35] d2.utils.events INFO:  eta: 0:40:45  iter: 2319  total_loss: 24.61  loss_ce: 0.4826  loss_mask: 0.3576  loss_dice: 1.171  loss_contrastive: 0  loss_ce_0: 1.37  loss_mask_0: 0.4193  loss_dice_0: 1.188  loss_ce_1: 0.8453  loss_mask_1: 0.3761  loss_dice_1: 0.973  loss_ce_2: 0.8311  loss_mask_2: 0.3737  loss_dice_2: 0.9524  loss_ce_3: 0.5966  loss_mask_3: 0.3613  loss_dice_3: 0.993  loss_ce_4: 0.4879  loss_mask_4: 0.3659  loss_dice_4: 0.9673  loss_ce_5: 0.4979  loss_mask_5: 0.3428  loss_dice_5: 1.089  loss_ce_6: 0.5918  loss_mask_6: 0.3405  loss_dice_6: 1.095  loss_ce_7: 0.5601  loss_mask_7: 0.3516  loss_dice_7: 0.9783  loss_ce_8: 0.474  loss_mask_8: 0.3564  loss_dice_8: 0.9705    time: 0.3217  last_time: 0.3060  data_time: 0.0036  last_data_time: 0.0030   lr: 7.8863e-05  max_mem: 14757M
[12/04 23:54:41] d2.utils.events INFO:  eta: 0:40:39  iter: 2339  total_loss: 20.78  loss_ce: 0.3741  loss_mask: 0.3162  loss_dice: 0.9785  loss_contrastive: 0  loss_ce_0: 1.347  loss_mask_0: 0.39  loss_dice_0: 1.238  loss_ce_1: 0.7402  loss_mask_1: 0.3058  loss_dice_1: 0.987  loss_ce_2: 0.7134  loss_mask_2: 0.3135  loss_dice_2: 1.013  loss_ce_3: 0.5225  loss_mask_3: 0.2777  loss_dice_3: 0.942  loss_ce_4: 0.4733  loss_mask_4: 0.271  loss_dice_4: 0.9407  loss_ce_5: 0.4432  loss_mask_5: 0.2944  loss_dice_5: 0.957  loss_ce_6: 0.4273  loss_mask_6: 0.3255  loss_dice_6: 0.8339  loss_ce_7: 0.4089  loss_mask_7: 0.3251  loss_dice_7: 0.8341  loss_ce_8: 0.4194  loss_mask_8: 0.299  loss_dice_8: 0.9084    time: 0.3218  last_time: 0.3177  data_time: 0.0039  last_data_time: 0.0042   lr: 7.8679e-05  max_mem: 14757M
[12/04 23:54:48] d2.utils.events INFO:  eta: 0:40:34  iter: 2359  total_loss: 20.61  loss_ce: 0.3867  loss_mask: 0.3822  loss_dice: 1.078  loss_contrastive: 0  loss_ce_0: 1.374  loss_mask_0: 0.4616  loss_dice_0: 1.445  loss_ce_1: 0.9249  loss_mask_1: 0.4  loss_dice_1: 1.363  loss_ce_2: 0.6736  loss_mask_2: 0.4002  loss_dice_2: 1.178  loss_ce_3: 0.5977  loss_mask_3: 0.3717  loss_dice_3: 1.088  loss_ce_4: 0.5011  loss_mask_4: 0.3859  loss_dice_4: 1.137  loss_ce_5: 0.4909  loss_mask_5: 0.3807  loss_dice_5: 1.09  loss_ce_6: 0.4757  loss_mask_6: 0.4268  loss_dice_6: 1.074  loss_ce_7: 0.3949  loss_mask_7: 0.3871  loss_dice_7: 1.032  loss_ce_8: 0.3996  loss_mask_8: 0.3654  loss_dice_8: 1.137    time: 0.3218  last_time: 0.3194  data_time: 0.0037  last_data_time: 0.0032   lr: 7.8494e-05  max_mem: 14757M
[12/04 23:54:54] d2.utils.events INFO:  eta: 0:40:28  iter: 2379  total_loss: 17.96  loss_ce: 0.3567  loss_mask: 0.5019  loss_dice: 0.8741  loss_contrastive: 0  loss_ce_0: 1.04  loss_mask_0: 0.546  loss_dice_0: 1.108  loss_ce_1: 0.5636  loss_mask_1: 0.491  loss_dice_1: 0.9012  loss_ce_2: 0.5448  loss_mask_2: 0.4963  loss_dice_2: 0.8808  loss_ce_3: 0.4925  loss_mask_3: 0.4656  loss_dice_3: 0.9662  loss_ce_4: 0.4429  loss_mask_4: 0.4965  loss_dice_4: 1.001  loss_ce_5: 0.3543  loss_mask_5: 0.4906  loss_dice_5: 0.868  loss_ce_6: 0.4173  loss_mask_6: 0.5039  loss_dice_6: 0.9248  loss_ce_7: 0.3815  loss_mask_7: 0.4888  loss_dice_7: 0.9051  loss_ce_8: 0.39  loss_mask_8: 0.4992  loss_dice_8: 0.9445    time: 0.3218  last_time: 0.3078  data_time: 0.0031  last_data_time: 0.0029   lr: 7.8309e-05  max_mem: 14757M
[12/04 23:55:01] d2.utils.events INFO:  eta: 0:40:21  iter: 2399  total_loss: 18.05  loss_ce: 0.1704  loss_mask: 0.312  loss_dice: 0.8469  loss_contrastive: 0  loss_ce_0: 1.207  loss_mask_0: 0.3797  loss_dice_0: 1.284  loss_ce_1: 0.7676  loss_mask_1: 0.2773  loss_dice_1: 1.045  loss_ce_2: 0.4189  loss_mask_2: 0.3011  loss_dice_2: 0.8863  loss_ce_3: 0.3719  loss_mask_3: 0.3214  loss_dice_3: 0.8797  loss_ce_4: 0.2515  loss_mask_4: 0.3143  loss_dice_4: 0.8844  loss_ce_5: 0.2277  loss_mask_5: 0.3052  loss_dice_5: 0.8619  loss_ce_6: 0.2016  loss_mask_6: 0.3161  loss_dice_6: 0.8749  loss_ce_7: 0.2694  loss_mask_7: 0.3107  loss_dice_7: 0.8673  loss_ce_8: 0.19  loss_mask_8: 0.3018  loss_dice_8: 0.8475    time: 0.3218  last_time: 0.3087  data_time: 0.0031  last_data_time: 0.0039   lr: 7.8124e-05  max_mem: 14757M
[12/04 23:55:07] d2.utils.events INFO:  eta: 0:40:15  iter: 2419  total_loss: 18.32  loss_ce: 0.1731  loss_mask: 0.2776  loss_dice: 0.7731  loss_contrastive: 0  loss_ce_0: 0.8587  loss_mask_0: 0.3507  loss_dice_0: 1.136  loss_ce_1: 0.5843  loss_mask_1: 0.2847  loss_dice_1: 1.144  loss_ce_2: 0.3591  loss_mask_2: 0.3055  loss_dice_2: 0.8623  loss_ce_3: 0.2847  loss_mask_3: 0.3092  loss_dice_3: 0.9357  loss_ce_4: 0.2263  loss_mask_4: 0.3097  loss_dice_4: 0.8977  loss_ce_5: 0.1921  loss_mask_5: 0.3138  loss_dice_5: 0.944  loss_ce_6: 0.1823  loss_mask_6: 0.2932  loss_dice_6: 0.9144  loss_ce_7: 0.1672  loss_mask_7: 0.2943  loss_dice_7: 0.8338  loss_ce_8: 0.1728  loss_mask_8: 0.284  loss_dice_8: 0.8803    time: 0.3219  last_time: 0.3083  data_time: 0.0031  last_data_time: 0.0037   lr: 7.7939e-05  max_mem: 14757M
[12/04 23:55:14] d2.utils.events INFO:  eta: 0:40:11  iter: 2439  total_loss: 23.68  loss_ce: 0.5117  loss_mask: 0.3578  loss_dice: 1.243  loss_contrastive: 0  loss_ce_0: 1.384  loss_mask_0: 0.394  loss_dice_0: 1.552  loss_ce_1: 0.9271  loss_mask_1: 0.2773  loss_dice_1: 1.181  loss_ce_2: 0.6917  loss_mask_2: 0.2747  loss_dice_2: 1.259  loss_ce_3: 0.6773  loss_mask_3: 0.276  loss_dice_3: 1.147  loss_ce_4: 0.6462  loss_mask_4: 0.288  loss_dice_4: 1.206  loss_ce_5: 0.5507  loss_mask_5: 0.2816  loss_dice_5: 1.132  loss_ce_6: 0.6011  loss_mask_6: 0.2679  loss_dice_6: 1.113  loss_ce_7: 0.5136  loss_mask_7: 0.2743  loss_dice_7: 1.094  loss_ce_8: 0.5281  loss_mask_8: 0.2688  loss_dice_8: 1.061    time: 0.3219  last_time: 0.3340  data_time: 0.0034  last_data_time: 0.0032   lr: 7.7754e-05  max_mem: 14757M
[12/04 23:55:20] d2.utils.events INFO:  eta: 0:40:06  iter: 2459  total_loss: 20.62  loss_ce: 0.3284  loss_mask: 0.3307  loss_dice: 1.023  loss_contrastive: 0  loss_ce_0: 1.047  loss_mask_0: 0.3746  loss_dice_0: 1.447  loss_ce_1: 0.6658  loss_mask_1: 0.3657  loss_dice_1: 0.976  loss_ce_2: 0.4867  loss_mask_2: 0.3217  loss_dice_2: 0.9823  loss_ce_3: 0.4856  loss_mask_3: 0.2662  loss_dice_3: 0.9243  loss_ce_4: 0.353  loss_mask_4: 0.2962  loss_dice_4: 0.9414  loss_ce_5: 0.3088  loss_mask_5: 0.2968  loss_dice_5: 0.9929  loss_ce_6: 0.3472  loss_mask_6: 0.3119  loss_dice_6: 0.9305  loss_ce_7: 0.3351  loss_mask_7: 0.3015  loss_dice_7: 0.9728  loss_ce_8: 0.329  loss_mask_8: 0.2616  loss_dice_8: 0.976    time: 0.3219  last_time: 0.3078  data_time: 0.0033  last_data_time: 0.0030   lr: 7.7569e-05  max_mem: 14757M
[12/04 23:55:27] d2.utils.events INFO:  eta: 0:40:01  iter: 2479  total_loss: 23.15  loss_ce: 0.4893  loss_mask: 0.3031  loss_dice: 1.051  loss_contrastive: 0  loss_ce_0: 1.577  loss_mask_0: 0.4256  loss_dice_0: 1.418  loss_ce_1: 0.9121  loss_mask_1: 0.3248  loss_dice_1: 1.175  loss_ce_2: 0.7229  loss_mask_2: 0.3055  loss_dice_2: 1.16  loss_ce_3: 0.668  loss_mask_3: 0.3329  loss_dice_3: 1.05  loss_ce_4: 0.4926  loss_mask_4: 0.3489  loss_dice_4: 0.9844  loss_ce_5: 0.4263  loss_mask_5: 0.3372  loss_dice_5: 1.178  loss_ce_6: 0.4911  loss_mask_6: 0.306  loss_dice_6: 1.078  loss_ce_7: 0.5305  loss_mask_7: 0.3025  loss_dice_7: 0.9834  loss_ce_8: 0.4815  loss_mask_8: 0.3072  loss_dice_8: 1.142    time: 0.3219  last_time: 0.3256  data_time: 0.0032  last_data_time: 0.0033   lr: 7.7383e-05  max_mem: 14757M
[12/04 23:55:33] d2.utils.events INFO:  eta: 0:39:54  iter: 2499  total_loss: 21.15  loss_ce: 0.5243  loss_mask: 0.3705  loss_dice: 1.125  loss_contrastive: 0  loss_ce_0: 1.422  loss_mask_0: 0.3882  loss_dice_0: 1.454  loss_ce_1: 0.877  loss_mask_1: 0.364  loss_dice_1: 1.18  loss_ce_2: 0.7125  loss_mask_2: 0.3703  loss_dice_2: 1.1  loss_ce_3: 0.6714  loss_mask_3: 0.3762  loss_dice_3: 1.101  loss_ce_4: 0.4952  loss_mask_4: 0.3906  loss_dice_4: 1.152  loss_ce_5: 0.5553  loss_mask_5: 0.3513  loss_dice_5: 1.138  loss_ce_6: 0.5812  loss_mask_6: 0.3568  loss_dice_6: 1.118  loss_ce_7: 0.5823  loss_mask_7: 0.3495  loss_dice_7: 1.047  loss_ce_8: 0.4551  loss_mask_8: 0.3642  loss_dice_8: 1.084    time: 0.3219  last_time: 0.3191  data_time: 0.0035  last_data_time: 0.0035   lr: 7.7198e-05  max_mem: 14757M
[12/04 23:55:40] d2.utils.events INFO:  eta: 0:39:47  iter: 2519  total_loss: 22.94  loss_ce: 0.4176  loss_mask: 0.395  loss_dice: 1.17  loss_contrastive: 0  loss_ce_0: 1.223  loss_mask_0: 0.414  loss_dice_0: 1.511  loss_ce_1: 0.6827  loss_mask_1: 0.3942  loss_dice_1: 1.266  loss_ce_2: 0.6784  loss_mask_2: 0.382  loss_dice_2: 1.109  loss_ce_3: 0.4822  loss_mask_3: 0.3909  loss_dice_3: 1.103  loss_ce_4: 0.3752  loss_mask_4: 0.3753  loss_dice_4: 1.151  loss_ce_5: 0.4799  loss_mask_5: 0.388  loss_dice_5: 1.163  loss_ce_6: 0.4413  loss_mask_6: 0.3776  loss_dice_6: 1.18  loss_ce_7: 0.4252  loss_mask_7: 0.3745  loss_dice_7: 1.112  loss_ce_8: 0.4281  loss_mask_8: 0.3588  loss_dice_8: 1.255    time: 0.3219  last_time: 0.3185  data_time: 0.0036  last_data_time: 0.0030   lr: 7.7013e-05  max_mem: 14757M
[12/04 23:55:46] d2.utils.events INFO:  eta: 0:39:40  iter: 2539  total_loss: 22.92  loss_ce: 0.4354  loss_mask: 0.2025  loss_dice: 1.165  loss_contrastive: 0  loss_ce_0: 1.674  loss_mask_0: 0.2548  loss_dice_0: 1.369  loss_ce_1: 0.7811  loss_mask_1: 0.2017  loss_dice_1: 1.17  loss_ce_2: 0.5956  loss_mask_2: 0.2614  loss_dice_2: 1.079  loss_ce_3: 0.4889  loss_mask_3: 0.2649  loss_dice_3: 1.07  loss_ce_4: 0.497  loss_mask_4: 0.2576  loss_dice_4: 1.108  loss_ce_5: 0.5139  loss_mask_5: 0.2844  loss_dice_5: 1.168  loss_ce_6: 0.4829  loss_mask_6: 0.2126  loss_dice_6: 1.144  loss_ce_7: 0.3798  loss_mask_7: 0.2026  loss_dice_7: 1.173  loss_ce_8: 0.3971  loss_mask_8: 0.2046  loss_dice_8: 1.114    time: 0.3218  last_time: 0.3077  data_time: 0.0034  last_data_time: 0.0027   lr: 7.6828e-05  max_mem: 14757M
[12/04 23:55:52] d2.utils.events INFO:  eta: 0:39:33  iter: 2559  total_loss: 19.16  loss_ce: 0.3348  loss_mask: 0.2356  loss_dice: 0.9444  loss_contrastive: 0  loss_ce_0: 1.07  loss_mask_0: 0.3596  loss_dice_0: 1.162  loss_ce_1: 0.5721  loss_mask_1: 0.2465  loss_dice_1: 1.053  loss_ce_2: 0.3787  loss_mask_2: 0.2425  loss_dice_2: 1.059  loss_ce_3: 0.3117  loss_mask_3: 0.2333  loss_dice_3: 0.9176  loss_ce_4: 0.332  loss_mask_4: 0.237  loss_dice_4: 0.8806  loss_ce_5: 0.3163  loss_mask_5: 0.2204  loss_dice_5: 0.9926  loss_ce_6: 0.3514  loss_mask_6: 0.2286  loss_dice_6: 0.9206  loss_ce_7: 0.3303  loss_mask_7: 0.2208  loss_dice_7: 0.9405  loss_ce_8: 0.25  loss_mask_8: 0.2303  loss_dice_8: 1.003    time: 0.3218  last_time: 0.3363  data_time: 0.0032  last_data_time: 0.0038   lr: 7.6642e-05  max_mem: 14757M
[12/04 23:55:59] d2.utils.events INFO:  eta: 0:39:27  iter: 2579  total_loss: 24.13  loss_ce: 0.5617  loss_mask: 0.3839  loss_dice: 1.27  loss_contrastive: 0  loss_ce_0: 1.236  loss_mask_0: 0.4692  loss_dice_0: 1.493  loss_ce_1: 0.908  loss_mask_1: 0.387  loss_dice_1: 1.331  loss_ce_2: 0.7742  loss_mask_2: 0.3726  loss_dice_2: 1.238  loss_ce_3: 0.691  loss_mask_3: 0.3944  loss_dice_3: 1.278  loss_ce_4: 0.4808  loss_mask_4: 0.395  loss_dice_4: 1.249  loss_ce_5: 0.593  loss_mask_5: 0.3723  loss_dice_5: 1.215  loss_ce_6: 0.5767  loss_mask_6: 0.3827  loss_dice_6: 1.233  loss_ce_7: 0.5433  loss_mask_7: 0.3908  loss_dice_7: 1.27  loss_ce_8: 0.585  loss_mask_8: 0.3966  loss_dice_8: 1.237    time: 0.3219  last_time: 0.3080  data_time: 0.0036  last_data_time: 0.0022   lr: 7.6457e-05  max_mem: 14757M
[12/04 23:56:05] d2.utils.events INFO:  eta: 0:39:20  iter: 2599  total_loss: 21.05  loss_ce: 0.364  loss_mask: 0.4699  loss_dice: 1.002  loss_contrastive: 0  loss_ce_0: 1.426  loss_mask_0: 0.5405  loss_dice_0: 1.365  loss_ce_1: 0.6419  loss_mask_1: 0.4384  loss_dice_1: 1.126  loss_ce_2: 0.7084  loss_mask_2: 0.4518  loss_dice_2: 1.046  loss_ce_3: 0.5224  loss_mask_3: 0.4383  loss_dice_3: 1.014  loss_ce_4: 0.4624  loss_mask_4: 0.4298  loss_dice_4: 0.9703  loss_ce_5: 0.3773  loss_mask_5: 0.4462  loss_dice_5: 1.017  loss_ce_6: 0.3682  loss_mask_6: 0.4365  loss_dice_6: 0.9263  loss_ce_7: 0.3562  loss_mask_7: 0.4488  loss_dice_7: 0.9809  loss_ce_8: 0.3579  loss_mask_8: 0.455  loss_dice_8: 1.009    time: 0.3219  last_time: 0.3096  data_time: 0.0036  last_data_time: 0.0031   lr: 7.6271e-05  max_mem: 14757M
[12/04 23:56:12] d2.utils.events INFO:  eta: 0:39:13  iter: 2619  total_loss: 25.98  loss_ce: 0.5246  loss_mask: 0.4089  loss_dice: 1.133  loss_contrastive: 0  loss_ce_0: 1.178  loss_mask_0: 0.4649  loss_dice_0: 1.338  loss_ce_1: 0.7326  loss_mask_1: 0.4549  loss_dice_1: 1.248  loss_ce_2: 0.6415  loss_mask_2: 0.39  loss_dice_2: 1.105  loss_ce_3: 0.6372  loss_mask_3: 0.3829  loss_dice_3: 1.006  loss_ce_4: 0.5172  loss_mask_4: 0.3883  loss_dice_4: 1.079  loss_ce_5: 0.5264  loss_mask_5: 0.3644  loss_dice_5: 1.037  loss_ce_6: 0.5706  loss_mask_6: 0.3949  loss_dice_6: 0.9503  loss_ce_7: 0.585  loss_mask_7: 0.4469  loss_dice_7: 1.13  loss_ce_8: 0.5992  loss_mask_8: 0.4005  loss_dice_8: 1.174    time: 0.3219  last_time: 0.3105  data_time: 0.0033  last_data_time: 0.0029   lr: 7.6086e-05  max_mem: 14757M
[12/04 23:56:18] d2.utils.events INFO:  eta: 0:39:08  iter: 2639  total_loss: 20.22  loss_ce: 0.3961  loss_mask: 0.3366  loss_dice: 1.042  loss_contrastive: 0  loss_ce_0: 0.9828  loss_mask_0: 0.374  loss_dice_0: 1.208  loss_ce_1: 0.6336  loss_mask_1: 0.3638  loss_dice_1: 1.085  loss_ce_2: 0.479  loss_mask_2: 0.3474  loss_dice_2: 1.083  loss_ce_3: 0.3251  loss_mask_3: 0.3319  loss_dice_3: 0.9217  loss_ce_4: 0.3065  loss_mask_4: 0.2977  loss_dice_4: 1.074  loss_ce_5: 0.3643  loss_mask_5: 0.2986  loss_dice_5: 1.035  loss_ce_6: 0.3662  loss_mask_6: 0.3018  loss_dice_6: 0.922  loss_ce_7: 0.3867  loss_mask_7: 0.3078  loss_dice_7: 0.8702  loss_ce_8: 0.3008  loss_mask_8: 0.3197  loss_dice_8: 0.9571    time: 0.3219  last_time: 0.3089  data_time: 0.0039  last_data_time: 0.0038   lr: 7.59e-05  max_mem: 14757M
[12/04 23:56:25] d2.utils.events INFO:  eta: 0:38:59  iter: 2659  total_loss: 22.31  loss_ce: 0.445  loss_mask: 0.302  loss_dice: 1.155  loss_contrastive: 0  loss_ce_0: 1.361  loss_mask_0: 0.3684  loss_dice_0: 1.444  loss_ce_1: 0.7498  loss_mask_1: 0.2962  loss_dice_1: 1.338  loss_ce_2: 0.6971  loss_mask_2: 0.3426  loss_dice_2: 1.216  loss_ce_3: 0.5729  loss_mask_3: 0.3029  loss_dice_3: 1.111  loss_ce_4: 0.5667  loss_mask_4: 0.298  loss_dice_4: 1.091  loss_ce_5: 0.5019  loss_mask_5: 0.3038  loss_dice_5: 1.112  loss_ce_6: 0.4977  loss_mask_6: 0.3517  loss_dice_6: 1.183  loss_ce_7: 0.5247  loss_mask_7: 0.319  loss_dice_7: 1.133  loss_ce_8: 0.449  loss_mask_8: 0.3114  loss_dice_8: 1.126    time: 0.3219  last_time: 0.3281  data_time: 0.0034  last_data_time: 0.0034   lr: 7.5715e-05  max_mem: 14757M
[12/04 23:56:31] d2.utils.events INFO:  eta: 0:38:55  iter: 2679  total_loss: 26.89  loss_ce: 0.5795  loss_mask: 0.5056  loss_dice: 0.9986  loss_contrastive: 0  loss_ce_0: 1.459  loss_mask_0: 0.7382  loss_dice_0: 1.432  loss_ce_1: 1.051  loss_mask_1: 0.6034  loss_dice_1: 1.178  loss_ce_2: 0.8789  loss_mask_2: 0.4995  loss_dice_2: 1.091  loss_ce_3: 0.6303  loss_mask_3: 0.521  loss_dice_3: 1.035  loss_ce_4: 0.5068  loss_mask_4: 0.4886  loss_dice_4: 1.043  loss_ce_5: 0.5719  loss_mask_5: 0.5011  loss_dice_5: 1.075  loss_ce_6: 0.5165  loss_mask_6: 0.4839  loss_dice_6: 0.9669  loss_ce_7: 0.4692  loss_mask_7: 0.4918  loss_dice_7: 1.037  loss_ce_8: 0.5472  loss_mask_8: 0.4903  loss_dice_8: 1.044    time: 0.3219  last_time: 0.3096  data_time: 0.0039  last_data_time: 0.0028   lr: 7.5529e-05  max_mem: 14757M
[12/04 23:56:38] d2.utils.events INFO:  eta: 0:38:45  iter: 2699  total_loss: 23.97  loss_ce: 0.4172  loss_mask: 0.3917  loss_dice: 1.051  loss_contrastive: 0  loss_ce_0: 1.457  loss_mask_0: 0.5501  loss_dice_0: 1.444  loss_ce_1: 0.8356  loss_mask_1: 0.4282  loss_dice_1: 1.135  loss_ce_2: 0.7076  loss_mask_2: 0.3823  loss_dice_2: 1.161  loss_ce_3: 0.5891  loss_mask_3: 0.3542  loss_dice_3: 1.102  loss_ce_4: 0.5263  loss_mask_4: 0.3791  loss_dice_4: 1.056  loss_ce_5: 0.5351  loss_mask_5: 0.3648  loss_dice_5: 1.14  loss_ce_6: 0.4834  loss_mask_6: 0.3673  loss_dice_6: 1.084  loss_ce_7: 0.4288  loss_mask_7: 0.3922  loss_dice_7: 1.114  loss_ce_8: 0.4986  loss_mask_8: 0.3523  loss_dice_8: 1.1    time: 0.3219  last_time: 0.2989  data_time: 0.0036  last_data_time: 0.0022   lr: 7.5343e-05  max_mem: 14757M
[12/04 23:56:44] d2.utils.events INFO:  eta: 0:38:38  iter: 2719  total_loss: 18.34  loss_ce: 0.2827  loss_mask: 0.3794  loss_dice: 0.8458  loss_contrastive: 0  loss_ce_0: 0.959  loss_mask_0: 0.4547  loss_dice_0: 1.137  loss_ce_1: 0.6026  loss_mask_1: 0.292  loss_dice_1: 0.9163  loss_ce_2: 0.4302  loss_mask_2: 0.3456  loss_dice_2: 1.027  loss_ce_3: 0.3077  loss_mask_3: 0.3414  loss_dice_3: 0.8683  loss_ce_4: 0.3125  loss_mask_4: 0.3439  loss_dice_4: 0.8272  loss_ce_5: 0.231  loss_mask_5: 0.3309  loss_dice_5: 0.8938  loss_ce_6: 0.2882  loss_mask_6: 0.3135  loss_dice_6: 0.8725  loss_ce_7: 0.2858  loss_mask_7: 0.3251  loss_dice_7: 0.9215  loss_ce_8: 0.2792  loss_mask_8: 0.3824  loss_dice_8: 0.7899    time: 0.3219  last_time: 0.3199  data_time: 0.0039  last_data_time: 0.0031   lr: 7.5157e-05  max_mem: 14757M
[12/04 23:56:51] d2.utils.events INFO:  eta: 0:38:33  iter: 2739  total_loss: 22.94  loss_ce: 0.3362  loss_mask: 0.3397  loss_dice: 1.152  loss_contrastive: 0  loss_ce_0: 1.451  loss_mask_0: 0.4434  loss_dice_0: 1.524  loss_ce_1: 0.6885  loss_mask_1: 0.3577  loss_dice_1: 1.257  loss_ce_2: 0.5345  loss_mask_2: 0.3168  loss_dice_2: 1.214  loss_ce_3: 0.6111  loss_mask_3: 0.3297  loss_dice_3: 1.071  loss_ce_4: 0.605  loss_mask_4: 0.3486  loss_dice_4: 1.169  loss_ce_5: 0.488  loss_mask_5: 0.3356  loss_dice_5: 1.224  loss_ce_6: 0.4443  loss_mask_6: 0.3356  loss_dice_6: 1.206  loss_ce_7: 0.3906  loss_mask_7: 0.3424  loss_dice_7: 1.24  loss_ce_8: 0.4004  loss_mask_8: 0.3714  loss_dice_8: 1.15    time: 0.3219  last_time: 0.3256  data_time: 0.0037  last_data_time: 0.0026   lr: 7.4972e-05  max_mem: 14757M
[12/04 23:56:57] d2.utils.events INFO:  eta: 0:38:24  iter: 2759  total_loss: 21.24  loss_ce: 0.5728  loss_mask: 0.3885  loss_dice: 1.086  loss_contrastive: 0  loss_ce_0: 1.531  loss_mask_0: 0.5024  loss_dice_0: 1.443  loss_ce_1: 0.7623  loss_mask_1: 0.4182  loss_dice_1: 1.282  loss_ce_2: 0.6541  loss_mask_2: 0.3537  loss_dice_2: 1.125  loss_ce_3: 0.6619  loss_mask_3: 0.3518  loss_dice_3: 1.083  loss_ce_4: 0.5588  loss_mask_4: 0.3816  loss_dice_4: 1.103  loss_ce_5: 0.5729  loss_mask_5: 0.3641  loss_dice_5: 0.9771  loss_ce_6: 0.5117  loss_mask_6: 0.3913  loss_dice_6: 1.041  loss_ce_7: 0.5805  loss_mask_7: 0.4104  loss_dice_7: 1.029  loss_ce_8: 0.5644  loss_mask_8: 0.3889  loss_dice_8: 1.025    time: 0.3219  last_time: 0.3229  data_time: 0.0031  last_data_time: 0.0031   lr: 7.4786e-05  max_mem: 14757M
[12/04 23:57:04] d2.utils.events INFO:  eta: 0:38:17  iter: 2779  total_loss: 20.68  loss_ce: 0.3823  loss_mask: 0.2588  loss_dice: 1.044  loss_contrastive: 0  loss_ce_0: 1.205  loss_mask_0: 0.4452  loss_dice_0: 1.626  loss_ce_1: 0.7035  loss_mask_1: 0.3239  loss_dice_1: 1.387  loss_ce_2: 0.6058  loss_mask_2: 0.309  loss_dice_2: 1.222  loss_ce_3: 0.5955  loss_mask_3: 0.268  loss_dice_3: 1.115  loss_ce_4: 0.5024  loss_mask_4: 0.2685  loss_dice_4: 1.085  loss_ce_5: 0.4737  loss_mask_5: 0.2554  loss_dice_5: 1.032  loss_ce_6: 0.3855  loss_mask_6: 0.2595  loss_dice_6: 1.108  loss_ce_7: 0.3044  loss_mask_7: 0.2554  loss_dice_7: 1.096  loss_ce_8: 0.375  loss_mask_8: 0.25  loss_dice_8: 1.047    time: 0.3219  last_time: 0.3410  data_time: 0.0036  last_data_time: 0.0066   lr: 7.46e-05  max_mem: 14757M
[12/04 23:57:10] d2.utils.events INFO:  eta: 0:38:11  iter: 2799  total_loss: 24.94  loss_ce: 0.4938  loss_mask: 0.4854  loss_dice: 1.173  loss_contrastive: 0  loss_ce_0: 1.233  loss_mask_0: 0.5085  loss_dice_0: 1.381  loss_ce_1: 0.7522  loss_mask_1: 0.4926  loss_dice_1: 1.198  loss_ce_2: 0.6552  loss_mask_2: 0.4735  loss_dice_2: 1.26  loss_ce_3: 0.6216  loss_mask_3: 0.5063  loss_dice_3: 1.122  loss_ce_4: 0.5964  loss_mask_4: 0.4913  loss_dice_4: 1.121  loss_ce_5: 0.5321  loss_mask_5: 0.4867  loss_dice_5: 1.185  loss_ce_6: 0.5794  loss_mask_6: 0.4383  loss_dice_6: 1.172  loss_ce_7: 0.5573  loss_mask_7: 0.4795  loss_dice_7: 1.11  loss_ce_8: 0.4958  loss_mask_8: 0.4867  loss_dice_8: 1.092    time: 0.3219  last_time: 0.3157  data_time: 0.0034  last_data_time: 0.0037   lr: 7.4414e-05  max_mem: 14757M
[12/04 23:57:16] d2.utils.events INFO:  eta: 0:38:04  iter: 2819  total_loss: 25.03  loss_ce: 0.6492  loss_mask: 0.2893  loss_dice: 1.086  loss_contrastive: 0  loss_ce_0: 1.48  loss_mask_0: 0.3758  loss_dice_0: 1.422  loss_ce_1: 1.028  loss_mask_1: 0.3789  loss_dice_1: 1.312  loss_ce_2: 0.9167  loss_mask_2: 0.3004  loss_dice_2: 1.167  loss_ce_3: 0.7352  loss_mask_3: 0.2834  loss_dice_3: 1.013  loss_ce_4: 0.7115  loss_mask_4: 0.2893  loss_dice_4: 1.209  loss_ce_5: 0.6585  loss_mask_5: 0.2839  loss_dice_5: 0.9672  loss_ce_6: 0.5971  loss_mask_6: 0.3001  loss_dice_6: 1.124  loss_ce_7: 0.6512  loss_mask_7: 0.3066  loss_dice_7: 1.107  loss_ce_8: 0.6717  loss_mask_8: 0.2928  loss_dice_8: 1.061    time: 0.3219  last_time: 0.3267  data_time: 0.0038  last_data_time: 0.0060   lr: 7.4228e-05  max_mem: 14757M
[12/04 23:57:23] d2.utils.events INFO:  eta: 0:37:58  iter: 2839  total_loss: 23.4  loss_ce: 0.5602  loss_mask: 0.1306  loss_dice: 1.135  loss_contrastive: 0  loss_ce_0: 1.555  loss_mask_0: 0.2721  loss_dice_0: 1.678  loss_ce_1: 1.055  loss_mask_1: 0.1666  loss_dice_1: 1.385  loss_ce_2: 0.7478  loss_mask_2: 0.1697  loss_dice_2: 1.317  loss_ce_3: 0.7949  loss_mask_3: 0.1534  loss_dice_3: 1.155  loss_ce_4: 0.583  loss_mask_4: 0.1438  loss_dice_4: 1.189  loss_ce_5: 0.6655  loss_mask_5: 0.1349  loss_dice_5: 1.197  loss_ce_6: 0.653  loss_mask_6: 0.1425  loss_dice_6: 1.237  loss_ce_7: 0.5632  loss_mask_7: 0.136  loss_dice_7: 1.249  loss_ce_8: 0.6655  loss_mask_8: 0.1313  loss_dice_8: 1.177    time: 0.3220  last_time: 0.3072  data_time: 0.0039  last_data_time: 0.0030   lr: 7.4042e-05  max_mem: 14757M
[12/04 23:57:30] d2.utils.events INFO:  eta: 0:37:52  iter: 2859  total_loss: 25.64  loss_ce: 0.4803  loss_mask: 0.3627  loss_dice: 1.143  loss_contrastive: 0  loss_ce_0: 1.566  loss_mask_0: 0.4687  loss_dice_0: 1.599  loss_ce_1: 0.8493  loss_mask_1: 0.4327  loss_dice_1: 1.374  loss_ce_2: 0.7461  loss_mask_2: 0.3745  loss_dice_2: 1.286  loss_ce_3: 0.6063  loss_mask_3: 0.3784  loss_dice_3: 1.187  loss_ce_4: 0.5029  loss_mask_4: 0.3921  loss_dice_4: 1.223  loss_ce_5: 0.4785  loss_mask_5: 0.3506  loss_dice_5: 1.136  loss_ce_6: 0.49  loss_mask_6: 0.3469  loss_dice_6: 1.158  loss_ce_7: 0.5535  loss_mask_7: 0.3635  loss_dice_7: 1.169  loss_ce_8: 0.4736  loss_mask_8: 0.3283  loss_dice_8: 1.103    time: 0.3220  last_time: 0.3115  data_time: 0.0039  last_data_time: 0.0043   lr: 7.3856e-05  max_mem: 14757M
[12/04 23:57:36] d2.utils.events INFO:  eta: 0:37:45  iter: 2879  total_loss: 19.84  loss_ce: 0.404  loss_mask: 0.3978  loss_dice: 0.8786  loss_contrastive: 0  loss_ce_0: 0.9944  loss_mask_0: 0.4968  loss_dice_0: 1.217  loss_ce_1: 0.6737  loss_mask_1: 0.4398  loss_dice_1: 0.899  loss_ce_2: 0.4597  loss_mask_2: 0.3903  loss_dice_2: 0.9282  loss_ce_3: 0.4335  loss_mask_3: 0.4049  loss_dice_3: 0.88  loss_ce_4: 0.3679  loss_mask_4: 0.4085  loss_dice_4: 0.825  loss_ce_5: 0.3957  loss_mask_5: 0.3874  loss_dice_5: 0.9518  loss_ce_6: 0.3204  loss_mask_6: 0.3734  loss_dice_6: 0.8985  loss_ce_7: 0.4056  loss_mask_7: 0.3717  loss_dice_7: 0.8862  loss_ce_8: 0.324  loss_mask_8: 0.38  loss_dice_8: 0.8075    time: 0.3220  last_time: 0.3140  data_time: 0.0033  last_data_time: 0.0037   lr: 7.3669e-05  max_mem: 14757M
[12/04 23:57:43] d2.utils.events INFO:  eta: 0:37:39  iter: 2899  total_loss: 25.28  loss_ce: 0.3934  loss_mask: 0.3093  loss_dice: 1.27  loss_contrastive: 0  loss_ce_0: 1.134  loss_mask_0: 0.3411  loss_dice_0: 1.856  loss_ce_1: 0.7545  loss_mask_1: 0.3459  loss_dice_1: 1.538  loss_ce_2: 0.6104  loss_mask_2: 0.3566  loss_dice_2: 1.379  loss_ce_3: 0.5434  loss_mask_3: 0.3755  loss_dice_3: 1.481  loss_ce_4: 0.4868  loss_mask_4: 0.3086  loss_dice_4: 1.5  loss_ce_5: 0.4755  loss_mask_5: 0.3551  loss_dice_5: 1.255  loss_ce_6: 0.4402  loss_mask_6: 0.3475  loss_dice_6: 1.359  loss_ce_7: 0.4041  loss_mask_7: 0.3531  loss_dice_7: 1.298  loss_ce_8: 0.3401  loss_mask_8: 0.3213  loss_dice_8: 1.336    time: 0.3220  last_time: 0.3337  data_time: 0.0035  last_data_time: 0.0025   lr: 7.3483e-05  max_mem: 14757M
[12/04 23:57:49] d2.utils.events INFO:  eta: 0:37:34  iter: 2919  total_loss: 20.46  loss_ce: 0.4096  loss_mask: 0.2446  loss_dice: 1.072  loss_contrastive: 0  loss_ce_0: 1.43  loss_mask_0: 0.3404  loss_dice_0: 1.547  loss_ce_1: 0.9144  loss_mask_1: 0.2858  loss_dice_1: 1.301  loss_ce_2: 0.7104  loss_mask_2: 0.2489  loss_dice_2: 1.145  loss_ce_3: 0.6505  loss_mask_3: 0.2565  loss_dice_3: 1.08  loss_ce_4: 0.512  loss_mask_4: 0.2437  loss_dice_4: 1.187  loss_ce_5: 0.4266  loss_mask_5: 0.2527  loss_dice_5: 1.067  loss_ce_6: 0.4704  loss_mask_6: 0.2616  loss_dice_6: 1.025  loss_ce_7: 0.4143  loss_mask_7: 0.2519  loss_dice_7: 1.057  loss_ce_8: 0.3757  loss_mask_8: 0.2463  loss_dice_8: 1.117    time: 0.3220  last_time: 0.3304  data_time: 0.0032  last_data_time: 0.0026   lr: 7.3297e-05  max_mem: 14757M
[12/04 23:57:55] d2.utils.events INFO:  eta: 0:37:27  iter: 2939  total_loss: 21.8  loss_ce: 0.3856  loss_mask: 0.3526  loss_dice: 0.9717  loss_contrastive: 0  loss_ce_0: 0.9535  loss_mask_0: 0.4587  loss_dice_0: 1.382  loss_ce_1: 0.4509  loss_mask_1: 0.3557  loss_dice_1: 1.247  loss_ce_2: 0.3998  loss_mask_2: 0.4698  loss_dice_2: 1.039  loss_ce_3: 0.3965  loss_mask_3: 0.3858  loss_dice_3: 0.9291  loss_ce_4: 0.3738  loss_mask_4: 0.3776  loss_dice_4: 1.059  loss_ce_5: 0.3206  loss_mask_5: 0.3591  loss_dice_5: 1.027  loss_ce_6: 0.3251  loss_mask_6: 0.3646  loss_dice_6: 0.8843  loss_ce_7: 0.4031  loss_mask_7: 0.3379  loss_dice_7: 1.035  loss_ce_8: 0.3586  loss_mask_8: 0.3451  loss_dice_8: 1.038    time: 0.3220  last_time: 0.3088  data_time: 0.0036  last_data_time: 0.0036   lr: 7.311e-05  max_mem: 14757M
[12/04 23:58:02] d2.utils.events INFO:  eta: 0:37:20  iter: 2959  total_loss: 25.2  loss_ce: 0.5939  loss_mask: 0.4609  loss_dice: 1.321  loss_contrastive: 0  loss_ce_0: 1.355  loss_mask_0: 0.5251  loss_dice_0: 1.374  loss_ce_1: 0.8974  loss_mask_1: 0.4999  loss_dice_1: 1.401  loss_ce_2: 0.8539  loss_mask_2: 0.4545  loss_dice_2: 1.419  loss_ce_3: 0.7431  loss_mask_3: 0.466  loss_dice_3: 1.233  loss_ce_4: 0.6632  loss_mask_4: 0.5053  loss_dice_4: 1.268  loss_ce_5: 0.6628  loss_mask_5: 0.4766  loss_dice_5: 1.252  loss_ce_6: 0.4983  loss_mask_6: 0.4525  loss_dice_6: 1.236  loss_ce_7: 0.5774  loss_mask_7: 0.4504  loss_dice_7: 1.208  loss_ce_8: 0.4478  loss_mask_8: 0.4214  loss_dice_8: 1.323    time: 0.3220  last_time: 0.3185  data_time: 0.0035  last_data_time: 0.0033   lr: 7.2924e-05  max_mem: 14757M
[12/04 23:58:08] d2.utils.events INFO:  eta: 0:37:13  iter: 2979  total_loss: 19.16  loss_ce: 0.5133  loss_mask: 0.4102  loss_dice: 0.9345  loss_contrastive: 0  loss_ce_0: 0.9082  loss_mask_0: 0.4517  loss_dice_0: 1.316  loss_ce_1: 0.7337  loss_mask_1: 0.4677  loss_dice_1: 1.063  loss_ce_2: 0.6281  loss_mask_2: 0.4157  loss_dice_2: 1.016  loss_ce_3: 0.5662  loss_mask_3: 0.4232  loss_dice_3: 0.9509  loss_ce_4: 0.5076  loss_mask_4: 0.4337  loss_dice_4: 0.9856  loss_ce_5: 0.5651  loss_mask_5: 0.4296  loss_dice_5: 0.9964  loss_ce_6: 0.4613  loss_mask_6: 0.4354  loss_dice_6: 0.9053  loss_ce_7: 0.5426  loss_mask_7: 0.4194  loss_dice_7: 0.9682  loss_ce_8: 0.5265  loss_mask_8: 0.4208  loss_dice_8: 0.8865    time: 0.3220  last_time: 0.3059  data_time: 0.0031  last_data_time: 0.0028   lr: 7.2738e-05  max_mem: 14757M
[12/04 23:58:15] d2.utils.events INFO:  eta: 0:37:05  iter: 2999  total_loss: 27.86  loss_ce: 0.4762  loss_mask: 0.5631  loss_dice: 1.174  loss_contrastive: 0  loss_ce_0: 1.084  loss_mask_0: 0.5845  loss_dice_0: 1.484  loss_ce_1: 0.6991  loss_mask_1: 0.5517  loss_dice_1: 1.283  loss_ce_2: 0.7477  loss_mask_2: 0.5215  loss_dice_2: 1.239  loss_ce_3: 0.5763  loss_mask_3: 0.5059  loss_dice_3: 1.217  loss_ce_4: 0.6314  loss_mask_4: 0.5469  loss_dice_4: 1.169  loss_ce_5: 0.452  loss_mask_5: 0.5425  loss_dice_5: 1.196  loss_ce_6: 0.5644  loss_mask_6: 0.5271  loss_dice_6: 1.175  loss_ce_7: 0.4671  loss_mask_7: 0.5073  loss_dice_7: 1.215  loss_ce_8: 0.5353  loss_mask_8: 0.5096  loss_dice_8: 1.224    time: 0.3220  last_time: 0.3128  data_time: 0.0037  last_data_time: 0.0062   lr: 7.2551e-05  max_mem: 14757M
[12/04 23:58:21] d2.utils.events INFO:  eta: 0:36:59  iter: 3019  total_loss: 21.91  loss_ce: 0.4712  loss_mask: 0.3221  loss_dice: 0.963  loss_contrastive: 0  loss_ce_0: 1.363  loss_mask_0: 0.4495  loss_dice_0: 1.603  loss_ce_1: 0.7771  loss_mask_1: 0.3678  loss_dice_1: 1.061  loss_ce_2: 0.7403  loss_mask_2: 0.3229  loss_dice_2: 1.057  loss_ce_3: 0.5963  loss_mask_3: 0.3176  loss_dice_3: 1.042  loss_ce_4: 0.5545  loss_mask_4: 0.3454  loss_dice_4: 1.047  loss_ce_5: 0.5329  loss_mask_5: 0.3504  loss_dice_5: 1.052  loss_ce_6: 0.4145  loss_mask_6: 0.3344  loss_dice_6: 1.048  loss_ce_7: 0.4892  loss_mask_7: 0.3368  loss_dice_7: 1.098  loss_ce_8: 0.5355  loss_mask_8: 0.3355  loss_dice_8: 1.031    time: 0.3219  last_time: 0.3180  data_time: 0.0038  last_data_time: 0.0059   lr: 7.2365e-05  max_mem: 14757M
[12/04 23:58:28] d2.utils.events INFO:  eta: 0:36:53  iter: 3039  total_loss: 16.12  loss_ce: 0.3309  loss_mask: 0.3582  loss_dice: 0.7415  loss_contrastive: 0  loss_ce_0: 0.8741  loss_mask_0: 0.4074  loss_dice_0: 0.9263  loss_ce_1: 0.5951  loss_mask_1: 0.3685  loss_dice_1: 0.8918  loss_ce_2: 0.4702  loss_mask_2: 0.3594  loss_dice_2: 0.7833  loss_ce_3: 0.4764  loss_mask_3: 0.3535  loss_dice_3: 0.7968  loss_ce_4: 0.4829  loss_mask_4: 0.3427  loss_dice_4: 0.8764  loss_ce_5: 0.3287  loss_mask_5: 0.3473  loss_dice_5: 0.7755  loss_ce_6: 0.3202  loss_mask_6: 0.3423  loss_dice_6: 0.7719  loss_ce_7: 0.3033  loss_mask_7: 0.3482  loss_dice_7: 0.7967  loss_ce_8: 0.3038  loss_mask_8: 0.3449  loss_dice_8: 0.839    time: 0.3219  last_time: 0.3128  data_time: 0.0030  last_data_time: 0.0026   lr: 7.2178e-05  max_mem: 14757M
[12/04 23:58:34] d2.utils.events INFO:  eta: 0:36:47  iter: 3059  total_loss: 22.61  loss_ce: 0.5265  loss_mask: 0.428  loss_dice: 1.052  loss_contrastive: 0  loss_ce_0: 1.262  loss_mask_0: 0.4593  loss_dice_0: 1.363  loss_ce_1: 1.039  loss_mask_1: 0.4487  loss_dice_1: 1.225  loss_ce_2: 0.8046  loss_mask_2: 0.4179  loss_dice_2: 1.125  loss_ce_3: 0.5962  loss_mask_3: 0.3754  loss_dice_3: 1.162  loss_ce_4: 0.6336  loss_mask_4: 0.3826  loss_dice_4: 1.083  loss_ce_5: 0.5418  loss_mask_5: 0.3901  loss_dice_5: 1.058  loss_ce_6: 0.4753  loss_mask_6: 0.3818  loss_dice_6: 1.05  loss_ce_7: 0.591  loss_mask_7: 0.4036  loss_dice_7: 1.088  loss_ce_8: 0.4972  loss_mask_8: 0.4417  loss_dice_8: 1.044    time: 0.3220  last_time: 0.3150  data_time: 0.0036  last_data_time: 0.0026   lr: 7.1991e-05  max_mem: 14757M
[12/04 23:58:41] d2.utils.events INFO:  eta: 0:36:41  iter: 3079  total_loss: 20.85  loss_ce: 0.4196  loss_mask: 0.3475  loss_dice: 0.8367  loss_contrastive: 0  loss_ce_0: 1.223  loss_mask_0: 0.4012  loss_dice_0: 1.48  loss_ce_1: 0.8525  loss_mask_1: 0.3651  loss_dice_1: 0.9766  loss_ce_2: 0.5865  loss_mask_2: 0.3436  loss_dice_2: 1.107  loss_ce_3: 0.5248  loss_mask_3: 0.349  loss_dice_3: 0.9342  loss_ce_4: 0.471  loss_mask_4: 0.3402  loss_dice_4: 0.8626  loss_ce_5: 0.5136  loss_mask_5: 0.3315  loss_dice_5: 0.8305  loss_ce_6: 0.4822  loss_mask_6: 0.3346  loss_dice_6: 0.8273  loss_ce_7: 0.4691  loss_mask_7: 0.3328  loss_dice_7: 0.8548  loss_ce_8: 0.4166  loss_mask_8: 0.3301  loss_dice_8: 0.8521    time: 0.3220  last_time: 0.3238  data_time: 0.0037  last_data_time: 0.0027   lr: 7.1805e-05  max_mem: 14757M
[12/04 23:58:47] d2.utils.events INFO:  eta: 0:36:35  iter: 3099  total_loss: 19.07  loss_ce: 0.2494  loss_mask: 0.4383  loss_dice: 0.9048  loss_contrastive: 0  loss_ce_0: 0.9039  loss_mask_0: 0.5183  loss_dice_0: 1.127  loss_ce_1: 0.6331  loss_mask_1: 0.4306  loss_dice_1: 0.8889  loss_ce_2: 0.4347  loss_mask_2: 0.4305  loss_dice_2: 0.9693  loss_ce_3: 0.4512  loss_mask_3: 0.4533  loss_dice_3: 0.9036  loss_ce_4: 0.3255  loss_mask_4: 0.4215  loss_dice_4: 0.9447  loss_ce_5: 0.3161  loss_mask_5: 0.4123  loss_dice_5: 0.9708  loss_ce_6: 0.2685  loss_mask_6: 0.4505  loss_dice_6: 0.8604  loss_ce_7: 0.244  loss_mask_7: 0.4498  loss_dice_7: 0.909  loss_ce_8: 0.2486  loss_mask_8: 0.4501  loss_dice_8: 0.8923    time: 0.3220  last_time: 0.3174  data_time: 0.0032  last_data_time: 0.0045   lr: 7.1618e-05  max_mem: 14757M
[12/04 23:58:54] d2.utils.events INFO:  eta: 0:36:30  iter: 3119  total_loss: 20.72  loss_ce: 0.3583  loss_mask: 0.2323  loss_dice: 1.028  loss_contrastive: 0  loss_ce_0: 1.395  loss_mask_0: 0.3499  loss_dice_0: 1.432  loss_ce_1: 0.8882  loss_mask_1: 0.2655  loss_dice_1: 1.25  loss_ce_2: 0.7482  loss_mask_2: 0.2416  loss_dice_2: 1.2  loss_ce_3: 0.5829  loss_mask_3: 0.2359  loss_dice_3: 1.029  loss_ce_4: 0.4224  loss_mask_4: 0.2433  loss_dice_4: 1.111  loss_ce_5: 0.4753  loss_mask_5: 0.2676  loss_dice_5: 1.129  loss_ce_6: 0.3586  loss_mask_6: 0.247  loss_dice_6: 1.102  loss_ce_7: 0.3372  loss_mask_7: 0.238  loss_dice_7: 1.102  loss_ce_8: 0.3929  loss_mask_8: 0.2457  loss_dice_8: 1.082    time: 0.3220  last_time: 0.3172  data_time: 0.0033  last_data_time: 0.0027   lr: 7.1431e-05  max_mem: 14757M
[12/04 23:59:00] d2.utils.events INFO:  eta: 0:36:23  iter: 3139  total_loss: 21.97  loss_ce: 0.4226  loss_mask: 0.4087  loss_dice: 1.118  loss_contrastive: 0  loss_ce_0: 1.233  loss_mask_0: 0.49  loss_dice_0: 1.266  loss_ce_1: 0.8115  loss_mask_1: 0.4638  loss_dice_1: 1.126  loss_ce_2: 0.7836  loss_mask_2: 0.4462  loss_dice_2: 1.103  loss_ce_3: 0.6384  loss_mask_3: 0.4253  loss_dice_3: 1.104  loss_ce_4: 0.603  loss_mask_4: 0.4392  loss_dice_4: 1.137  loss_ce_5: 0.4671  loss_mask_5: 0.426  loss_dice_5: 1.108  loss_ce_6: 0.391  loss_mask_6: 0.4307  loss_dice_6: 1.073  loss_ce_7: 0.4159  loss_mask_7: 0.436  loss_dice_7: 1.068  loss_ce_8: 0.4068  loss_mask_8: 0.4336  loss_dice_8: 1.086    time: 0.3220  last_time: 0.3055  data_time: 0.0032  last_data_time: 0.0028   lr: 7.1244e-05  max_mem: 14757M
[12/04 23:59:06] d2.utils.events INFO:  eta: 0:36:18  iter: 3159  total_loss: 19.52  loss_ce: 0.2729  loss_mask: 0.2446  loss_dice: 0.9226  loss_contrastive: 0  loss_ce_0: 1.22  loss_mask_0: 0.3517  loss_dice_0: 1.179  loss_ce_1: 0.9451  loss_mask_1: 0.285  loss_dice_1: 1.174  loss_ce_2: 0.7537  loss_mask_2: 0.2748  loss_dice_2: 1.033  loss_ce_3: 0.5153  loss_mask_3: 0.2564  loss_dice_3: 0.9106  loss_ce_4: 0.4118  loss_mask_4: 0.2518  loss_dice_4: 1.002  loss_ce_5: 0.3978  loss_mask_5: 0.2615  loss_dice_5: 1.04  loss_ce_6: 0.2911  loss_mask_6: 0.2491  loss_dice_6: 1.061  loss_ce_7: 0.3875  loss_mask_7: 0.2533  loss_dice_7: 0.8991  loss_ce_8: 0.408  loss_mask_8: 0.2582  loss_dice_8: 0.9605    time: 0.3220  last_time: 0.3239  data_time: 0.0036  last_data_time: 0.0032   lr: 7.1057e-05  max_mem: 14757M
[12/04 23:59:13] d2.utils.events INFO:  eta: 0:36:12  iter: 3179  total_loss: 20.97  loss_ce: 0.3672  loss_mask: 0.4223  loss_dice: 1.049  loss_contrastive: 0  loss_ce_0: 1.153  loss_mask_0: 0.5081  loss_dice_0: 1.253  loss_ce_1: 0.6076  loss_mask_1: 0.4232  loss_dice_1: 1.045  loss_ce_2: 0.5009  loss_mask_2: 0.4289  loss_dice_2: 1.054  loss_ce_3: 0.6022  loss_mask_3: 0.4168  loss_dice_3: 0.9038  loss_ce_4: 0.4078  loss_mask_4: 0.4183  loss_dice_4: 1.081  loss_ce_5: 0.4827  loss_mask_5: 0.4239  loss_dice_5: 0.9515  loss_ce_6: 0.4428  loss_mask_6: 0.413  loss_dice_6: 0.946  loss_ce_7: 0.3454  loss_mask_7: 0.4104  loss_dice_7: 0.9667  loss_ce_8: 0.431  loss_mask_8: 0.4309  loss_dice_8: 1.033    time: 0.3220  last_time: 0.3159  data_time: 0.0038  last_data_time: 0.0029   lr: 7.087e-05  max_mem: 14757M
[12/04 23:59:19] d2.utils.events INFO:  eta: 0:36:05  iter: 3199  total_loss: 22.43  loss_ce: 0.5322  loss_mask: 0.3593  loss_dice: 0.9863  loss_contrastive: 0  loss_ce_0: 1.183  loss_mask_0: 0.49  loss_dice_0: 1.281  loss_ce_1: 0.8491  loss_mask_1: 0.4263  loss_dice_1: 1.15  loss_ce_2: 0.6725  loss_mask_2: 0.3841  loss_dice_2: 1.068  loss_ce_3: 0.7202  loss_mask_3: 0.3828  loss_dice_3: 0.9916  loss_ce_4: 0.6565  loss_mask_4: 0.3748  loss_dice_4: 1.083  loss_ce_5: 0.5178  loss_mask_5: 0.4165  loss_dice_5: 0.9988  loss_ce_6: 0.507  loss_mask_6: 0.3839  loss_dice_6: 1.076  loss_ce_7: 0.5006  loss_mask_7: 0.3506  loss_dice_7: 1.018  loss_ce_8: 0.3909  loss_mask_8: 0.3914  loss_dice_8: 1.018    time: 0.3220  last_time: 0.3145  data_time: 0.0035  last_data_time: 0.0025   lr: 7.0683e-05  max_mem: 14757M
[12/04 23:59:26] d2.utils.events INFO:  eta: 0:35:58  iter: 3219  total_loss: 27.9  loss_ce: 0.5444  loss_mask: 0.3184  loss_dice: 1.495  loss_contrastive: 0  loss_ce_0: 1.511  loss_mask_0: 0.4301  loss_dice_0: 1.843  loss_ce_1: 1.042  loss_mask_1: 0.3532  loss_dice_1: 1.594  loss_ce_2: 0.9392  loss_mask_2: 0.3502  loss_dice_2: 1.516  loss_ce_3: 0.8861  loss_mask_3: 0.3605  loss_dice_3: 1.402  loss_ce_4: 0.716  loss_mask_4: 0.3593  loss_dice_4: 1.44  loss_ce_5: 0.582  loss_mask_5: 0.3336  loss_dice_5: 1.442  loss_ce_6: 0.5834  loss_mask_6: 0.3356  loss_dice_6: 1.469  loss_ce_7: 0.6096  loss_mask_7: 0.3332  loss_dice_7: 1.45  loss_ce_8: 0.6149  loss_mask_8: 0.3364  loss_dice_8: 1.465    time: 0.3220  last_time: 0.3082  data_time: 0.0040  last_data_time: 0.0039   lr: 7.0496e-05  max_mem: 14757M
[12/04 23:59:32] d2.utils.events INFO:  eta: 0:35:52  iter: 3239  total_loss: 23.08  loss_ce: 0.5188  loss_mask: 0.3606  loss_dice: 0.8884  loss_contrastive: 0  loss_ce_0: 1.234  loss_mask_0: 0.4363  loss_dice_0: 1.437  loss_ce_1: 0.7456  loss_mask_1: 0.3682  loss_dice_1: 1.13  loss_ce_2: 0.86  loss_mask_2: 0.3759  loss_dice_2: 1.087  loss_ce_3: 0.6467  loss_mask_3: 0.3419  loss_dice_3: 1.054  loss_ce_4: 0.6703  loss_mask_4: 0.3536  loss_dice_4: 1.143  loss_ce_5: 0.532  loss_mask_5: 0.3415  loss_dice_5: 1.064  loss_ce_6: 0.4867  loss_mask_6: 0.3482  loss_dice_6: 0.9235  loss_ce_7: 0.5366  loss_mask_7: 0.3468  loss_dice_7: 0.9774  loss_ce_8: 0.5507  loss_mask_8: 0.3631  loss_dice_8: 1.031    time: 0.3220  last_time: 0.3184  data_time: 0.0033  last_data_time: 0.0037   lr: 7.0309e-05  max_mem: 14757M
[12/04 23:59:39] d2.utils.events INFO:  eta: 0:35:45  iter: 3259  total_loss: 22.41  loss_ce: 0.3961  loss_mask: 0.2829  loss_dice: 1.153  loss_contrastive: 0  loss_ce_0: 1.161  loss_mask_0: 0.3389  loss_dice_0: 1.367  loss_ce_1: 0.7534  loss_mask_1: 0.2952  loss_dice_1: 1.278  loss_ce_2: 0.6569  loss_mask_2: 0.3056  loss_dice_2: 1.168  loss_ce_3: 0.4906  loss_mask_3: 0.2977  loss_dice_3: 1.057  loss_ce_4: 0.4736  loss_mask_4: 0.3042  loss_dice_4: 1.122  loss_ce_5: 0.5206  loss_mask_5: 0.3126  loss_dice_5: 1.141  loss_ce_6: 0.4927  loss_mask_6: 0.2978  loss_dice_6: 1.142  loss_ce_7: 0.448  loss_mask_7: 0.3066  loss_dice_7: 1.162  loss_ce_8: 0.3938  loss_mask_8: 0.3079  loss_dice_8: 1.106    time: 0.3220  last_time: 0.4513  data_time: 0.0037  last_data_time: 0.0062   lr: 7.0122e-05  max_mem: 14757M
[12/04 23:59:45] d2.utils.events INFO:  eta: 0:35:39  iter: 3279  total_loss: 22.96  loss_ce: 0.3724  loss_mask: 0.3105  loss_dice: 1.008  loss_contrastive: 0  loss_ce_0: 0.938  loss_mask_0: 0.3572  loss_dice_0: 1.062  loss_ce_1: 0.5953  loss_mask_1: 0.3336  loss_dice_1: 1.149  loss_ce_2: 0.5197  loss_mask_2: 0.3091  loss_dice_2: 1.05  loss_ce_3: 0.4767  loss_mask_3: 0.2962  loss_dice_3: 0.991  loss_ce_4: 0.3901  loss_mask_4: 0.2923  loss_dice_4: 1.005  loss_ce_5: 0.3914  loss_mask_5: 0.3082  loss_dice_5: 1.032  loss_ce_6: 0.4216  loss_mask_6: 0.2977  loss_dice_6: 0.9412  loss_ce_7: 0.4047  loss_mask_7: 0.2929  loss_dice_7: 0.9964  loss_ce_8: 0.4003  loss_mask_8: 0.3181  loss_dice_8: 1.049    time: 0.3220  last_time: 0.3412  data_time: 0.0034  last_data_time: 0.0043   lr: 6.9934e-05  max_mem: 14757M
[12/04 23:59:52] d2.utils.events INFO:  eta: 0:35:34  iter: 3299  total_loss: 20.2  loss_ce: 0.174  loss_mask: 0.1879  loss_dice: 1.03  loss_contrastive: 0  loss_ce_0: 1.053  loss_mask_0: 0.2145  loss_dice_0: 1.194  loss_ce_1: 0.8127  loss_mask_1: 0.233  loss_dice_1: 1.054  loss_ce_2: 0.5318  loss_mask_2: 0.2009  loss_dice_2: 1.01  loss_ce_3: 0.502  loss_mask_3: 0.2069  loss_dice_3: 0.872  loss_ce_4: 0.4006  loss_mask_4: 0.184  loss_dice_4: 0.949  loss_ce_5: 0.3194  loss_mask_5: 0.1641  loss_dice_5: 0.9043  loss_ce_6: 0.35  loss_mask_6: 0.1768  loss_dice_6: 0.9849  loss_ce_7: 0.3448  loss_mask_7: 0.2169  loss_dice_7: 1.02  loss_ce_8: 0.3089  loss_mask_8: 0.1641  loss_dice_8: 1.001    time: 0.3220  last_time: 0.3380  data_time: 0.0035  last_data_time: 0.0026   lr: 6.9747e-05  max_mem: 14757M
[12/04 23:59:58] d2.utils.events INFO:  eta: 0:35:28  iter: 3319  total_loss: 22.92  loss_ce: 0.6229  loss_mask: 0.3854  loss_dice: 1.088  loss_contrastive: 0  loss_ce_0: 1.242  loss_mask_0: 0.4897  loss_dice_0: 1.507  loss_ce_1: 0.8989  loss_mask_1: 0.3971  loss_dice_1: 1.238  loss_ce_2: 0.7114  loss_mask_2: 0.357  loss_dice_2: 1.091  loss_ce_3: 0.5856  loss_mask_3: 0.3777  loss_dice_3: 1.049  loss_ce_4: 0.5792  loss_mask_4: 0.3688  loss_dice_4: 1.038  loss_ce_5: 0.4898  loss_mask_5: 0.3724  loss_dice_5: 1.081  loss_ce_6: 0.47  loss_mask_6: 0.3678  loss_dice_6: 1.052  loss_ce_7: 0.5069  loss_mask_7: 0.3851  loss_dice_7: 1.051  loss_ce_8: 0.4739  loss_mask_8: 0.3919  loss_dice_8: 1.148    time: 0.3220  last_time: 0.3076  data_time: 0.0039  last_data_time: 0.0032   lr: 6.956e-05  max_mem: 14757M
[12/05 00:00:05] d2.utils.events INFO:  eta: 0:35:23  iter: 3339  total_loss: 21.83  loss_ce: 0.3596  loss_mask: 0.2659  loss_dice: 0.9413  loss_contrastive: 0  loss_ce_0: 0.9303  loss_mask_0: 0.3596  loss_dice_0: 1.307  loss_ce_1: 0.5405  loss_mask_1: 0.3164  loss_dice_1: 1.053  loss_ce_2: 0.5833  loss_mask_2: 0.2898  loss_dice_2: 0.7836  loss_ce_3: 0.4585  loss_mask_3: 0.2926  loss_dice_3: 0.9554  loss_ce_4: 0.4015  loss_mask_4: 0.3133  loss_dice_4: 0.8892  loss_ce_5: 0.3488  loss_mask_5: 0.3282  loss_dice_5: 0.8525  loss_ce_6: 0.2844  loss_mask_6: 0.2969  loss_dice_6: 0.9477  loss_ce_7: 0.3435  loss_mask_7: 0.2894  loss_dice_7: 0.9928  loss_ce_8: 0.3445  loss_mask_8: 0.2915  loss_dice_8: 0.996    time: 0.3221  last_time: 0.3559  data_time: 0.0030  last_data_time: 0.0036   lr: 6.9372e-05  max_mem: 14757M
[12/05 00:00:11] d2.utils.events INFO:  eta: 0:35:15  iter: 3359  total_loss: 18.3  loss_ce: 0.3917  loss_mask: 0.2325  loss_dice: 0.8854  loss_contrastive: 0  loss_ce_0: 1.252  loss_mask_0: 0.3618  loss_dice_0: 1.199  loss_ce_1: 0.8404  loss_mask_1: 0.2978  loss_dice_1: 1.133  loss_ce_2: 0.5839  loss_mask_2: 0.2862  loss_dice_2: 0.968  loss_ce_3: 0.5056  loss_mask_3: 0.2609  loss_dice_3: 0.9581  loss_ce_4: 0.4576  loss_mask_4: 0.2611  loss_dice_4: 1.024  loss_ce_5: 0.4233  loss_mask_5: 0.2734  loss_dice_5: 0.8792  loss_ce_6: 0.431  loss_mask_6: 0.2435  loss_dice_6: 0.928  loss_ce_7: 0.3834  loss_mask_7: 0.2371  loss_dice_7: 0.9065  loss_ce_8: 0.3781  loss_mask_8: 0.2339  loss_dice_8: 0.9588    time: 0.3221  last_time: 0.3267  data_time: 0.0034  last_data_time: 0.0032   lr: 6.9185e-05  max_mem: 14757M
[12/05 00:00:18] d2.utils.events INFO:  eta: 0:35:08  iter: 3379  total_loss: 18.21  loss_ce: 0.3509  loss_mask: 0.2807  loss_dice: 0.8596  loss_contrastive: 0  loss_ce_0: 1.084  loss_mask_0: 0.3836  loss_dice_0: 0.9159  loss_ce_1: 0.5946  loss_mask_1: 0.341  loss_dice_1: 0.9565  loss_ce_2: 0.5454  loss_mask_2: 0.3263  loss_dice_2: 0.9127  loss_ce_3: 0.4733  loss_mask_3: 0.2641  loss_dice_3: 0.8496  loss_ce_4: 0.3725  loss_mask_4: 0.2879  loss_dice_4: 0.8451  loss_ce_5: 0.3628  loss_mask_5: 0.2866  loss_dice_5: 0.8479  loss_ce_6: 0.3668  loss_mask_6: 0.2689  loss_dice_6: 0.7538  loss_ce_7: 0.3446  loss_mask_7: 0.277  loss_dice_7: 0.849  loss_ce_8: 0.3829  loss_mask_8: 0.2883  loss_dice_8: 0.8512    time: 0.3220  last_time: 0.3234  data_time: 0.0033  last_data_time: 0.0030   lr: 6.8997e-05  max_mem: 14757M
[12/05 00:00:24] d2.utils.events INFO:  eta: 0:35:01  iter: 3399  total_loss: 20.08  loss_ce: 0.36  loss_mask: 0.25  loss_dice: 0.9409  loss_contrastive: 0  loss_ce_0: 1.559  loss_mask_0: 0.391  loss_dice_0: 1.128  loss_ce_1: 0.9607  loss_mask_1: 0.3239  loss_dice_1: 1.092  loss_ce_2: 0.7621  loss_mask_2: 0.2887  loss_dice_2: 1.047  loss_ce_3: 0.5483  loss_mask_3: 0.2805  loss_dice_3: 0.9604  loss_ce_4: 0.4029  loss_mask_4: 0.2743  loss_dice_4: 0.9718  loss_ce_5: 0.3756  loss_mask_5: 0.2795  loss_dice_5: 1.014  loss_ce_6: 0.437  loss_mask_6: 0.2521  loss_dice_6: 0.9816  loss_ce_7: 0.3721  loss_mask_7: 0.2512  loss_dice_7: 1.046  loss_ce_8: 0.4579  loss_mask_8: 0.2778  loss_dice_8: 0.9047    time: 0.3220  last_time: 0.3110  data_time: 0.0033  last_data_time: 0.0038   lr: 6.881e-05  max_mem: 14757M
[12/05 00:00:30] d2.utils.events INFO:  eta: 0:34:54  iter: 3419  total_loss: 24.34  loss_ce: 0.5553  loss_mask: 0.5274  loss_dice: 1.037  loss_contrastive: 0  loss_ce_0: 1.556  loss_mask_0: 0.6629  loss_dice_0: 1.394  loss_ce_1: 1.103  loss_mask_1: 0.4922  loss_dice_1: 1.061  loss_ce_2: 0.8701  loss_mask_2: 0.4835  loss_dice_2: 1.092  loss_ce_3: 0.8037  loss_mask_3: 0.49  loss_dice_3: 1.089  loss_ce_4: 0.6724  loss_mask_4: 0.49  loss_dice_4: 1.051  loss_ce_5: 0.6681  loss_mask_5: 0.4933  loss_dice_5: 1.031  loss_ce_6: 0.6778  loss_mask_6: 0.4939  loss_dice_6: 1.007  loss_ce_7: 0.6277  loss_mask_7: 0.4796  loss_dice_7: 0.9862  loss_ce_8: 0.6018  loss_mask_8: 0.5494  loss_dice_8: 1.032    time: 0.3220  last_time: 0.3245  data_time: 0.0033  last_data_time: 0.0027   lr: 6.8622e-05  max_mem: 14757M
[12/05 00:00:37] d2.utils.events INFO:  eta: 0:34:48  iter: 3439  total_loss: 20.33  loss_ce: 0.4109  loss_mask: 0.3257  loss_dice: 1.178  loss_contrastive: 0  loss_ce_0: 1.219  loss_mask_0: 0.304  loss_dice_0: 1.373  loss_ce_1: 0.7985  loss_mask_1: 0.3326  loss_dice_1: 1.191  loss_ce_2: 0.6378  loss_mask_2: 0.2812  loss_dice_2: 1.055  loss_ce_3: 0.4625  loss_mask_3: 0.3279  loss_dice_3: 1.054  loss_ce_4: 0.4133  loss_mask_4: 0.3215  loss_dice_4: 1.063  loss_ce_5: 0.4171  loss_mask_5: 0.3094  loss_dice_5: 1.186  loss_ce_6: 0.3447  loss_mask_6: 0.2872  loss_dice_6: 1.159  loss_ce_7: 0.4105  loss_mask_7: 0.296  loss_dice_7: 1.152  loss_ce_8: 0.3531  loss_mask_8: 0.2969  loss_dice_8: 1.178    time: 0.3220  last_time: 0.3052  data_time: 0.0033  last_data_time: 0.0030   lr: 6.8434e-05  max_mem: 14757M
[12/05 00:00:43] d2.utils.events INFO:  eta: 0:34:42  iter: 3459  total_loss: 19.86  loss_ce: 0.4082  loss_mask: 0.3464  loss_dice: 0.9765  loss_contrastive: 0  loss_ce_0: 1.123  loss_mask_0: 0.3874  loss_dice_0: 1.069  loss_ce_1: 0.8497  loss_mask_1: 0.3582  loss_dice_1: 1.011  loss_ce_2: 0.7698  loss_mask_2: 0.361  loss_dice_2: 0.9543  loss_ce_3: 0.4912  loss_mask_3: 0.3756  loss_dice_3: 0.9487  loss_ce_4: 0.4281  loss_mask_4: 0.3694  loss_dice_4: 0.9823  loss_ce_5: 0.4291  loss_mask_5: 0.3599  loss_dice_5: 0.9511  loss_ce_6: 0.4223  loss_mask_6: 0.3571  loss_dice_6: 0.9748  loss_ce_7: 0.3779  loss_mask_7: 0.353  loss_dice_7: 0.9817  loss_ce_8: 0.3788  loss_mask_8: 0.3545  loss_dice_8: 1.007    time: 0.3220  last_time: 0.3019  data_time: 0.0030  last_data_time: 0.0031   lr: 6.8246e-05  max_mem: 14757M
[12/05 00:00:50] d2.utils.events INFO:  eta: 0:34:36  iter: 3479  total_loss: 17.73  loss_ce: 0.3038  loss_mask: 0.4464  loss_dice: 0.9345  loss_contrastive: 0  loss_ce_0: 1.051  loss_mask_0: 0.4375  loss_dice_0: 1.082  loss_ce_1: 0.6495  loss_mask_1: 0.4387  loss_dice_1: 0.8686  loss_ce_2: 0.3543  loss_mask_2: 0.3914  loss_dice_2: 0.8582  loss_ce_3: 0.3701  loss_mask_3: 0.4134  loss_dice_3: 0.842  loss_ce_4: 0.3178  loss_mask_4: 0.4428  loss_dice_4: 0.8554  loss_ce_5: 0.3077  loss_mask_5: 0.4537  loss_dice_5: 0.8562  loss_ce_6: 0.3233  loss_mask_6: 0.393  loss_dice_6: 0.8377  loss_ce_7: 0.3353  loss_mask_7: 0.4005  loss_dice_7: 0.8655  loss_ce_8: 0.3391  loss_mask_8: 0.4042  loss_dice_8: 0.8651    time: 0.3220  last_time: 0.3263  data_time: 0.0034  last_data_time: 0.0028   lr: 6.8059e-05  max_mem: 14757M
[12/05 00:00:56] d2.utils.events INFO:  eta: 0:34:29  iter: 3499  total_loss: 20.3  loss_ce: 0.3758  loss_mask: 0.378  loss_dice: 1.014  loss_contrastive: 0  loss_ce_0: 1.227  loss_mask_0: 0.4023  loss_dice_0: 1.296  loss_ce_1: 0.6089  loss_mask_1: 0.3653  loss_dice_1: 0.9974  loss_ce_2: 0.5447  loss_mask_2: 0.3548  loss_dice_2: 0.9343  loss_ce_3: 0.3977  loss_mask_3: 0.3541  loss_dice_3: 0.9293  loss_ce_4: 0.4142  loss_mask_4: 0.3779  loss_dice_4: 0.9085  loss_ce_5: 0.3165  loss_mask_5: 0.3792  loss_dice_5: 0.9691  loss_ce_6: 0.374  loss_mask_6: 0.3839  loss_dice_6: 1.002  loss_ce_7: 0.2371  loss_mask_7: 0.386  loss_dice_7: 0.9882  loss_ce_8: 0.3456  loss_mask_8: 0.3951  loss_dice_8: 1.033    time: 0.3220  last_time: 0.2949  data_time: 0.0037  last_data_time: 0.0019   lr: 6.7871e-05  max_mem: 14757M
[12/05 00:01:03] d2.utils.events INFO:  eta: 0:34:23  iter: 3519  total_loss: 16.4  loss_ce: 0.4519  loss_mask: 0.2692  loss_dice: 0.8083  loss_contrastive: 0  loss_ce_0: 0.8923  loss_mask_0: 0.391  loss_dice_0: 0.9124  loss_ce_1: 0.5709  loss_mask_1: 0.3072  loss_dice_1: 0.8411  loss_ce_2: 0.5163  loss_mask_2: 0.2505  loss_dice_2: 0.8692  loss_ce_3: 0.3661  loss_mask_3: 0.2755  loss_dice_3: 0.858  loss_ce_4: 0.3822  loss_mask_4: 0.2496  loss_dice_4: 0.7771  loss_ce_5: 0.3382  loss_mask_5: 0.2806  loss_dice_5: 0.7679  loss_ce_6: 0.3537  loss_mask_6: 0.2653  loss_dice_6: 0.7574  loss_ce_7: 0.3508  loss_mask_7: 0.2777  loss_dice_7: 0.8513  loss_ce_8: 0.4706  loss_mask_8: 0.2591  loss_dice_8: 0.7879    time: 0.3220  last_time: 0.3216  data_time: 0.0029  last_data_time: 0.0041   lr: 6.7683e-05  max_mem: 14757M
[12/05 00:01:09] d2.utils.events INFO:  eta: 0:34:17  iter: 3539  total_loss: 19.71  loss_ce: 0.1818  loss_mask: 0.4025  loss_dice: 1.023  loss_contrastive: 0  loss_ce_0: 0.9777  loss_mask_0: 0.4473  loss_dice_0: 1.368  loss_ce_1: 0.554  loss_mask_1: 0.397  loss_dice_1: 1.224  loss_ce_2: 0.388  loss_mask_2: 0.3987  loss_dice_2: 1.163  loss_ce_3: 0.3859  loss_mask_3: 0.3877  loss_dice_3: 1.121  loss_ce_4: 0.3123  loss_mask_4: 0.3828  loss_dice_4: 1.078  loss_ce_5: 0.2655  loss_mask_5: 0.378  loss_dice_5: 1.078  loss_ce_6: 0.2614  loss_mask_6: 0.3768  loss_dice_6: 1.091  loss_ce_7: 0.2422  loss_mask_7: 0.3947  loss_dice_7: 1.065  loss_ce_8: 0.2921  loss_mask_8: 0.3984  loss_dice_8: 1.061    time: 0.3220  last_time: 0.3196  data_time: 0.0036  last_data_time: 0.0028   lr: 6.7495e-05  max_mem: 14757M
[12/05 00:01:16] d2.utils.events INFO:  eta: 0:34:11  iter: 3559  total_loss: 22.77  loss_ce: 0.4611  loss_mask: 0.5482  loss_dice: 1.017  loss_contrastive: 0  loss_ce_0: 1.519  loss_mask_0: 0.7022  loss_dice_0: 1.383  loss_ce_1: 1.032  loss_mask_1: 0.5521  loss_dice_1: 1.157  loss_ce_2: 0.7517  loss_mask_2: 0.5433  loss_dice_2: 1.019  loss_ce_3: 0.7319  loss_mask_3: 0.4935  loss_dice_3: 1.018  loss_ce_4: 0.4409  loss_mask_4: 0.5168  loss_dice_4: 1.069  loss_ce_5: 0.4642  loss_mask_5: 0.5454  loss_dice_5: 0.9939  loss_ce_6: 0.5387  loss_mask_6: 0.5432  loss_dice_6: 0.9724  loss_ce_7: 0.5826  loss_mask_7: 0.5341  loss_dice_7: 0.9827  loss_ce_8: 0.5084  loss_mask_8: 0.5197  loss_dice_8: 1.003    time: 0.3220  last_time: 0.3283  data_time: 0.0033  last_data_time: 0.0050   lr: 6.7307e-05  max_mem: 14757M
[12/05 00:01:22] d2.utils.events INFO:  eta: 0:34:05  iter: 3579  total_loss: 22.7  loss_ce: 0.3873  loss_mask: 0.4145  loss_dice: 1.076  loss_contrastive: 0  loss_ce_0: 1.506  loss_mask_0: 0.5336  loss_dice_0: 1.335  loss_ce_1: 0.742  loss_mask_1: 0.4075  loss_dice_1: 1.187  loss_ce_2: 0.6526  loss_mask_2: 0.4377  loss_dice_2: 1.111  loss_ce_3: 0.5683  loss_mask_3: 0.4572  loss_dice_3: 1.087  loss_ce_4: 0.5127  loss_mask_4: 0.4069  loss_dice_4: 1.112  loss_ce_5: 0.4683  loss_mask_5: 0.4372  loss_dice_5: 1.086  loss_ce_6: 0.4713  loss_mask_6: 0.4022  loss_dice_6: 1.133  loss_ce_7: 0.4692  loss_mask_7: 0.3921  loss_dice_7: 1.101  loss_ce_8: 0.3529  loss_mask_8: 0.429  loss_dice_8: 1.081    time: 0.3220  last_time: 0.3315  data_time: 0.0034  last_data_time: 0.0031   lr: 6.7119e-05  max_mem: 14757M
[12/05 00:01:29] d2.utils.events INFO:  eta: 0:34:00  iter: 3599  total_loss: 22.1  loss_ce: 0.3551  loss_mask: 0.3917  loss_dice: 0.9524  loss_contrastive: 0  loss_ce_0: 0.9478  loss_mask_0: 0.4472  loss_dice_0: 1.345  loss_ce_1: 0.6312  loss_mask_1: 0.4181  loss_dice_1: 1.232  loss_ce_2: 0.4583  loss_mask_2: 0.3989  loss_dice_2: 1.107  loss_ce_3: 0.4253  loss_mask_3: 0.4934  loss_dice_3: 1.091  loss_ce_4: 0.4118  loss_mask_4: 0.4679  loss_dice_4: 0.9808  loss_ce_5: 0.3804  loss_mask_5: 0.4539  loss_dice_5: 1.096  loss_ce_6: 0.3924  loss_mask_6: 0.4477  loss_dice_6: 1.019  loss_ce_7: 0.2953  loss_mask_7: 0.4104  loss_dice_7: 0.9591  loss_ce_8: 0.3186  loss_mask_8: 0.4116  loss_dice_8: 0.9886    time: 0.3220  last_time: 0.3216  data_time: 0.0035  last_data_time: 0.0031   lr: 6.693e-05  max_mem: 14757M
[12/05 00:01:35] d2.utils.events INFO:  eta: 0:33:54  iter: 3619  total_loss: 18.78  loss_ce: 0.1909  loss_mask: 0.2816  loss_dice: 0.7807  loss_contrastive: 0  loss_ce_0: 0.8749  loss_mask_0: 0.3552  loss_dice_0: 1.196  loss_ce_1: 0.4367  loss_mask_1: 0.3443  loss_dice_1: 0.9805  loss_ce_2: 0.3726  loss_mask_2: 0.3032  loss_dice_2: 0.8488  loss_ce_3: 0.3873  loss_mask_3: 0.3075  loss_dice_3: 0.8178  loss_ce_4: 0.2897  loss_mask_4: 0.347  loss_dice_4: 0.7711  loss_ce_5: 0.2475  loss_mask_5: 0.388  loss_dice_5: 0.7813  loss_ce_6: 0.2392  loss_mask_6: 0.3733  loss_dice_6: 0.7735  loss_ce_7: 0.1868  loss_mask_7: 0.3726  loss_dice_7: 0.7716  loss_ce_8: 0.2133  loss_mask_8: 0.339  loss_dice_8: 0.7591    time: 0.3220  last_time: 0.3275  data_time: 0.0033  last_data_time: 0.0035   lr: 6.6742e-05  max_mem: 14757M
[12/05 00:01:41] d2.utils.events INFO:  eta: 0:33:47  iter: 3639  total_loss: 23.75  loss_ce: 0.4641  loss_mask: 0.5515  loss_dice: 1.002  loss_contrastive: 0  loss_ce_0: 1.114  loss_mask_0: 0.5992  loss_dice_0: 1.457  loss_ce_1: 0.7401  loss_mask_1: 0.5385  loss_dice_1: 1.113  loss_ce_2: 0.5936  loss_mask_2: 0.5385  loss_dice_2: 1.178  loss_ce_3: 0.4735  loss_mask_3: 0.4581  loss_dice_3: 1.083  loss_ce_4: 0.5294  loss_mask_4: 0.4499  loss_dice_4: 0.9773  loss_ce_5: 0.4229  loss_mask_5: 0.5293  loss_dice_5: 1.046  loss_ce_6: 0.4766  loss_mask_6: 0.513  loss_dice_6: 0.9624  loss_ce_7: 0.415  loss_mask_7: 0.501  loss_dice_7: 1.039  loss_ce_8: 0.4259  loss_mask_8: 0.5527  loss_dice_8: 1.019    time: 0.3220  last_time: 0.3031  data_time: 0.0033  last_data_time: 0.0032   lr: 6.6554e-05  max_mem: 14757M
[12/05 00:01:48] d2.utils.events INFO:  eta: 0:33:40  iter: 3659  total_loss: 20.05  loss_ce: 0.3963  loss_mask: 0.3926  loss_dice: 0.9633  loss_contrastive: 0  loss_ce_0: 1.147  loss_mask_0: 0.4638  loss_dice_0: 1.34  loss_ce_1: 0.6121  loss_mask_1: 0.3785  loss_dice_1: 1.056  loss_ce_2: 0.4993  loss_mask_2: 0.3833  loss_dice_2: 1.04  loss_ce_3: 0.3106  loss_mask_3: 0.3763  loss_dice_3: 1.054  loss_ce_4: 0.3768  loss_mask_4: 0.3554  loss_dice_4: 1.003  loss_ce_5: 0.3041  loss_mask_5: 0.3411  loss_dice_5: 1.004  loss_ce_6: 0.349  loss_mask_6: 0.346  loss_dice_6: 0.9412  loss_ce_7: 0.3662  loss_mask_7: 0.3497  loss_dice_7: 0.9693  loss_ce_8: 0.3348  loss_mask_8: 0.3543  loss_dice_8: 0.9829    time: 0.3220  last_time: 0.3164  data_time: 0.0038  last_data_time: 0.0028   lr: 6.6365e-05  max_mem: 14757M
[12/05 00:01:54] d2.utils.events INFO:  eta: 0:33:33  iter: 3679  total_loss: 22.55  loss_ce: 0.4994  loss_mask: 0.3677  loss_dice: 1.359  loss_contrastive: 0  loss_ce_0: 1.394  loss_mask_0: 0.4832  loss_dice_0: 1.568  loss_ce_1: 0.8768  loss_mask_1: 0.393  loss_dice_1: 1.329  loss_ce_2: 0.5927  loss_mask_2: 0.3789  loss_dice_2: 1.301  loss_ce_3: 0.5398  loss_mask_3: 0.3418  loss_dice_3: 1.191  loss_ce_4: 0.4158  loss_mask_4: 0.3267  loss_dice_4: 1.154  loss_ce_5: 0.349  loss_mask_5: 0.3403  loss_dice_5: 1.207  loss_ce_6: 0.4001  loss_mask_6: 0.3472  loss_dice_6: 1.156  loss_ce_7: 0.4728  loss_mask_7: 0.319  loss_dice_7: 1.184  loss_ce_8: 0.4787  loss_mask_8: 0.3641  loss_dice_8: 1.184    time: 0.3220  last_time: 0.3479  data_time: 0.0034  last_data_time: 0.0042   lr: 6.6177e-05  max_mem: 14757M
[12/05 00:02:01] d2.utils.events INFO:  eta: 0:33:27  iter: 3699  total_loss: 19.78  loss_ce: 0.368  loss_mask: 0.4381  loss_dice: 0.912  loss_contrastive: 0  loss_ce_0: 1.024  loss_mask_0: 0.5188  loss_dice_0: 1.147  loss_ce_1: 0.6827  loss_mask_1: 0.3836  loss_dice_1: 0.93  loss_ce_2: 0.5405  loss_mask_2: 0.4317  loss_dice_2: 0.9639  loss_ce_3: 0.3338  loss_mask_3: 0.4604  loss_dice_3: 0.9541  loss_ce_4: 0.3651  loss_mask_4: 0.3957  loss_dice_4: 0.9875  loss_ce_5: 0.3807  loss_mask_5: 0.4559  loss_dice_5: 0.9462  loss_ce_6: 0.3171  loss_mask_6: 0.4016  loss_dice_6: 0.9394  loss_ce_7: 0.2447  loss_mask_7: 0.4115  loss_dice_7: 0.8917  loss_ce_8: 0.3191  loss_mask_8: 0.4232  loss_dice_8: 0.8567    time: 0.3220  last_time: 0.3122  data_time: 0.0031  last_data_time: 0.0028   lr: 6.5989e-05  max_mem: 14757M
[12/05 00:02:07] d2.utils.events INFO:  eta: 0:33:21  iter: 3719  total_loss: 21.77  loss_ce: 0.1676  loss_mask: 0.3034  loss_dice: 0.9468  loss_contrastive: 0  loss_ce_0: 1.319  loss_mask_0: 0.3901  loss_dice_0: 1.177  loss_ce_1: 0.738  loss_mask_1: 0.3068  loss_dice_1: 1.082  loss_ce_2: 0.353  loss_mask_2: 0.3348  loss_dice_2: 1.083  loss_ce_3: 0.2083  loss_mask_3: 0.3151  loss_dice_3: 1.04  loss_ce_4: 0.2432  loss_mask_4: 0.3051  loss_dice_4: 0.9966  loss_ce_5: 0.159  loss_mask_5: 0.2925  loss_dice_5: 1.007  loss_ce_6: 0.1829  loss_mask_6: 0.3058  loss_dice_6: 1.013  loss_ce_7: 0.1747  loss_mask_7: 0.3253  loss_dice_7: 1.02  loss_ce_8: 0.1746  loss_mask_8: 0.3091  loss_dice_8: 1.028    time: 0.3220  last_time: 0.3120  data_time: 0.0032  last_data_time: 0.0031   lr: 6.58e-05  max_mem: 14757M
[12/05 00:02:14] d2.utils.events INFO:  eta: 0:33:15  iter: 3739  total_loss: 16.8  loss_ce: 0.254  loss_mask: 0.3235  loss_dice: 0.8016  loss_contrastive: 0  loss_ce_0: 0.6471  loss_mask_0: 0.419  loss_dice_0: 1.101  loss_ce_1: 0.3952  loss_mask_1: 0.34  loss_dice_1: 1.013  loss_ce_2: 0.3158  loss_mask_2: 0.3077  loss_dice_2: 0.8507  loss_ce_3: 0.189  loss_mask_3: 0.2825  loss_dice_3: 0.9355  loss_ce_4: 0.1722  loss_mask_4: 0.3025  loss_dice_4: 1.01  loss_ce_5: 0.2187  loss_mask_5: 0.2785  loss_dice_5: 0.8365  loss_ce_6: 0.2637  loss_mask_6: 0.3261  loss_dice_6: 0.8345  loss_ce_7: 0.2481  loss_mask_7: 0.3224  loss_dice_7: 0.8122  loss_ce_8: 0.2451  loss_mask_8: 0.2984  loss_dice_8: 0.9195    time: 0.3220  last_time: 0.2982  data_time: 0.0032  last_data_time: 0.0025   lr: 6.5611e-05  max_mem: 14757M
[12/05 00:02:20] d2.utils.events INFO:  eta: 0:33:10  iter: 3759  total_loss: 19.54  loss_ce: 0.2603  loss_mask: 0.3791  loss_dice: 0.9792  loss_contrastive: 0  loss_ce_0: 1.018  loss_mask_0: 0.3956  loss_dice_0: 1.102  loss_ce_1: 0.4224  loss_mask_1: 0.4321  loss_dice_1: 1.025  loss_ce_2: 0.4406  loss_mask_2: 0.4736  loss_dice_2: 0.9618  loss_ce_3: 0.2361  loss_mask_3: 0.474  loss_dice_3: 0.9537  loss_ce_4: 0.306  loss_mask_4: 0.3168  loss_dice_4: 0.9635  loss_ce_5: 0.1977  loss_mask_5: 0.4131  loss_dice_5: 0.9866  loss_ce_6: 0.2176  loss_mask_6: 0.4162  loss_dice_6: 1.018  loss_ce_7: 0.3674  loss_mask_7: 0.3804  loss_dice_7: 0.9888  loss_ce_8: 0.2209  loss_mask_8: 0.3942  loss_dice_8: 0.9686    time: 0.3219  last_time: 0.3069  data_time: 0.0031  last_data_time: 0.0035   lr: 6.5423e-05  max_mem: 14757M
[12/05 00:02:26] d2.utils.events INFO:  eta: 0:33:03  iter: 3779  total_loss: 19.73  loss_ce: 0.2457  loss_mask: 0.3402  loss_dice: 0.9734  loss_contrastive: 0  loss_ce_0: 0.8549  loss_mask_0: 0.3446  loss_dice_0: 1.141  loss_ce_1: 0.6004  loss_mask_1: 0.3642  loss_dice_1: 1.044  loss_ce_2: 0.4019  loss_mask_2: 0.356  loss_dice_2: 1.075  loss_ce_3: 0.2488  loss_mask_3: 0.364  loss_dice_3: 1.001  loss_ce_4: 0.2516  loss_mask_4: 0.3755  loss_dice_4: 0.9898  loss_ce_5: 0.2228  loss_mask_5: 0.373  loss_dice_5: 1.031  loss_ce_6: 0.2298  loss_mask_6: 0.3431  loss_dice_6: 0.9501  loss_ce_7: 0.2257  loss_mask_7: 0.3456  loss_dice_7: 1.084  loss_ce_8: 0.1882  loss_mask_8: 0.3398  loss_dice_8: 1.043    time: 0.3219  last_time: 0.3238  data_time: 0.0031  last_data_time: 0.0028   lr: 6.5234e-05  max_mem: 14757M
[12/05 00:02:33] d2.utils.events INFO:  eta: 0:32:59  iter: 3799  total_loss: 19.31  loss_ce: 0.4113  loss_mask: 0.2002  loss_dice: 1.009  loss_contrastive: 0  loss_ce_0: 1.405  loss_mask_0: 0.3113  loss_dice_0: 1.388  loss_ce_1: 0.8875  loss_mask_1: 0.2292  loss_dice_1: 1.219  loss_ce_2: 0.681  loss_mask_2: 0.2251  loss_dice_2: 1.081  loss_ce_3: 0.5119  loss_mask_3: 0.2126  loss_dice_3: 0.9445  loss_ce_4: 0.4539  loss_mask_4: 0.1867  loss_dice_4: 1.07  loss_ce_5: 0.405  loss_mask_5: 0.1958  loss_dice_5: 1.091  loss_ce_6: 0.3069  loss_mask_6: 0.2006  loss_dice_6: 1.011  loss_ce_7: 0.3516  loss_mask_7: 0.1976  loss_dice_7: 0.9893  loss_ce_8: 0.3902  loss_mask_8: 0.1932  loss_dice_8: 1.142    time: 0.3219  last_time: 0.3187  data_time: 0.0039  last_data_time: 0.0032   lr: 6.5045e-05  max_mem: 14757M
[12/05 00:02:39] d2.utils.events INFO:  eta: 0:32:53  iter: 3819  total_loss: 17.17  loss_ce: 0.2621  loss_mask: 0.2551  loss_dice: 0.9196  loss_contrastive: 0  loss_ce_0: 0.8651  loss_mask_0: 0.3079  loss_dice_0: 1.049  loss_ce_1: 0.4295  loss_mask_1: 0.2401  loss_dice_1: 0.9122  loss_ce_2: 0.4064  loss_mask_2: 0.2233  loss_dice_2: 0.9816  loss_ce_3: 0.3768  loss_mask_3: 0.2611  loss_dice_3: 0.9334  loss_ce_4: 0.3858  loss_mask_4: 0.2572  loss_dice_4: 0.9759  loss_ce_5: 0.3433  loss_mask_5: 0.2282  loss_dice_5: 0.9514  loss_ce_6: 0.3269  loss_mask_6: 0.2527  loss_dice_6: 0.8896  loss_ce_7: 0.3255  loss_mask_7: 0.2496  loss_dice_7: 0.9103  loss_ce_8: 0.321  loss_mask_8: 0.2398  loss_dice_8: 0.8943    time: 0.3219  last_time: 0.3322  data_time: 0.0037  last_data_time: 0.0032   lr: 6.4856e-05  max_mem: 14757M
[12/05 00:02:46] d2.utils.events INFO:  eta: 0:32:44  iter: 3839  total_loss: 20.04  loss_ce: 0.5464  loss_mask: 0.2415  loss_dice: 1.086  loss_contrastive: 0  loss_ce_0: 1.34  loss_mask_0: 0.3499  loss_dice_0: 1.483  loss_ce_1: 0.9863  loss_mask_1: 0.2604  loss_dice_1: 1.144  loss_ce_2: 0.8401  loss_mask_2: 0.2752  loss_dice_2: 1.269  loss_ce_3: 0.6341  loss_mask_3: 0.276  loss_dice_3: 1.028  loss_ce_4: 0.5688  loss_mask_4: 0.2829  loss_dice_4: 1.141  loss_ce_5: 0.5009  loss_mask_5: 0.2673  loss_dice_5: 1.139  loss_ce_6: 0.5826  loss_mask_6: 0.2931  loss_dice_6: 1.01  loss_ce_7: 0.5767  loss_mask_7: 0.2802  loss_dice_7: 1.115  loss_ce_8: 0.5195  loss_mask_8: 0.2298  loss_dice_8: 0.936    time: 0.3219  last_time: 0.3488  data_time: 0.0039  last_data_time: 0.0029   lr: 6.4668e-05  max_mem: 14757M
[12/05 00:02:52] d2.utils.events INFO:  eta: 0:32:39  iter: 3859  total_loss: 14.74  loss_ce: 0.2675  loss_mask: 0.3081  loss_dice: 0.735  loss_contrastive: 0  loss_ce_0: 0.9044  loss_mask_0: 0.3944  loss_dice_0: 0.8592  loss_ce_1: 0.4386  loss_mask_1: 0.32  loss_dice_1: 0.8255  loss_ce_2: 0.3002  loss_mask_2: 0.3216  loss_dice_2: 0.7439  loss_ce_3: 0.2956  loss_mask_3: 0.3025  loss_dice_3: 0.6819  loss_ce_4: 0.266  loss_mask_4: 0.3118  loss_dice_4: 0.7042  loss_ce_5: 0.2167  loss_mask_5: 0.3088  loss_dice_5: 0.7007  loss_ce_6: 0.2463  loss_mask_6: 0.3285  loss_dice_6: 0.7235  loss_ce_7: 0.2747  loss_mask_7: 0.3189  loss_dice_7: 0.7144  loss_ce_8: 0.276  loss_mask_8: 0.3255  loss_dice_8: 0.7328    time: 0.3219  last_time: 0.3208  data_time: 0.0034  last_data_time: 0.0060   lr: 6.4479e-05  max_mem: 14757M
[12/05 00:02:59] d2.utils.events INFO:  eta: 0:32:34  iter: 3879  total_loss: 24.86  loss_ce: 0.6101  loss_mask: 0.3592  loss_dice: 1.091  loss_contrastive: 0  loss_ce_0: 1.436  loss_mask_0: 0.4898  loss_dice_0: 1.543  loss_ce_1: 0.8182  loss_mask_1: 0.3974  loss_dice_1: 1.249  loss_ce_2: 0.6422  loss_mask_2: 0.3688  loss_dice_2: 1.211  loss_ce_3: 0.573  loss_mask_3: 0.3634  loss_dice_3: 1.056  loss_ce_4: 0.5447  loss_mask_4: 0.3633  loss_dice_4: 1.135  loss_ce_5: 0.502  loss_mask_5: 0.3604  loss_dice_5: 1.183  loss_ce_6: 0.5797  loss_mask_6: 0.3722  loss_dice_6: 1.186  loss_ce_7: 0.6056  loss_mask_7: 0.3664  loss_dice_7: 1.181  loss_ce_8: 0.5734  loss_mask_8: 0.3668  loss_dice_8: 1.117    time: 0.3219  last_time: 0.3316  data_time: 0.0047  last_data_time: 0.0037   lr: 6.429e-05  max_mem: 14757M
[12/05 00:03:05] d2.utils.events INFO:  eta: 0:32:25  iter: 3899  total_loss: 19.43  loss_ce: 0.3585  loss_mask: 0.3076  loss_dice: 0.9977  loss_contrastive: 0  loss_ce_0: 1.069  loss_mask_0: 0.4008  loss_dice_0: 1.2  loss_ce_1: 0.688  loss_mask_1: 0.369  loss_dice_1: 1.158  loss_ce_2: 0.6126  loss_mask_2: 0.3074  loss_dice_2: 1.049  loss_ce_3: 0.4225  loss_mask_3: 0.3388  loss_dice_3: 1.087  loss_ce_4: 0.3538  loss_mask_4: 0.3337  loss_dice_4: 1.046  loss_ce_5: 0.4283  loss_mask_5: 0.2792  loss_dice_5: 1.068  loss_ce_6: 0.3821  loss_mask_6: 0.3079  loss_dice_6: 1.041  loss_ce_7: 0.2949  loss_mask_7: 0.2684  loss_dice_7: 1.034  loss_ce_8: 0.2801  loss_mask_8: 0.2887  loss_dice_8: 1.031    time: 0.3219  last_time: 0.3248  data_time: 0.0036  last_data_time: 0.0031   lr: 6.41e-05  max_mem: 14757M
[12/05 00:03:12] d2.utils.events INFO:  eta: 0:32:18  iter: 3919  total_loss: 21.69  loss_ce: 0.3279  loss_mask: 0.1942  loss_dice: 1.146  loss_contrastive: 0  loss_ce_0: 1.139  loss_mask_0: 0.3158  loss_dice_0: 1.489  loss_ce_1: 0.7595  loss_mask_1: 0.2713  loss_dice_1: 1.334  loss_ce_2: 0.6688  loss_mask_2: 0.2153  loss_dice_2: 1.083  loss_ce_3: 0.4937  loss_mask_3: 0.195  loss_dice_3: 1.023  loss_ce_4: 0.3891  loss_mask_4: 0.1983  loss_dice_4: 1.088  loss_ce_5: 0.408  loss_mask_5: 0.1852  loss_dice_5: 1.103  loss_ce_6: 0.3794  loss_mask_6: 0.1869  loss_dice_6: 1.109  loss_ce_7: 0.3605  loss_mask_7: 0.1938  loss_dice_7: 1.088  loss_ce_8: 0.339  loss_mask_8: 0.1912  loss_dice_8: 1.119    time: 0.3219  last_time: 0.3107  data_time: 0.0035  last_data_time: 0.0025   lr: 6.3911e-05  max_mem: 14757M
[12/05 00:03:18] d2.utils.events INFO:  eta: 0:32:11  iter: 3939  total_loss: 17.99  loss_ce: 0.2495  loss_mask: 0.2976  loss_dice: 0.7919  loss_contrastive: 0  loss_ce_0: 0.9954  loss_mask_0: 0.4012  loss_dice_0: 1.111  loss_ce_1: 0.5331  loss_mask_1: 0.3131  loss_dice_1: 0.9922  loss_ce_2: 0.4681  loss_mask_2: 0.295  loss_dice_2: 1.095  loss_ce_3: 0.2651  loss_mask_3: 0.2897  loss_dice_3: 0.9861  loss_ce_4: 0.2739  loss_mask_4: 0.295  loss_dice_4: 1.033  loss_ce_5: 0.2164  loss_mask_5: 0.2785  loss_dice_5: 0.9252  loss_ce_6: 0.3034  loss_mask_6: 0.2878  loss_dice_6: 0.9265  loss_ce_7: 0.24  loss_mask_7: 0.2829  loss_dice_7: 0.8129  loss_ce_8: 0.2183  loss_mask_8: 0.3021  loss_dice_8: 0.95    time: 0.3219  last_time: 0.3108  data_time: 0.0036  last_data_time: 0.0033   lr: 6.3722e-05  max_mem: 14757M
[12/05 00:03:24] d2.utils.events INFO:  eta: 0:32:04  iter: 3959  total_loss: 21.07  loss_ce: 0.2519  loss_mask: 0.3436  loss_dice: 1.06  loss_contrastive: 0  loss_ce_0: 0.9451  loss_mask_0: 0.4212  loss_dice_0: 1.376  loss_ce_1: 0.5931  loss_mask_1: 0.3694  loss_dice_1: 1.168  loss_ce_2: 0.5439  loss_mask_2: 0.3359  loss_dice_2: 1.163  loss_ce_3: 0.4034  loss_mask_3: 0.3318  loss_dice_3: 1.066  loss_ce_4: 0.373  loss_mask_4: 0.33  loss_dice_4: 1.151  loss_ce_5: 0.3351  loss_mask_5: 0.3618  loss_dice_5: 1.109  loss_ce_6: 0.3289  loss_mask_6: 0.3541  loss_dice_6: 1.104  loss_ce_7: 0.3042  loss_mask_7: 0.349  loss_dice_7: 1.132  loss_ce_8: 0.2269  loss_mask_8: 0.3463  loss_dice_8: 1.095    time: 0.3219  last_time: 0.3198  data_time: 0.0033  last_data_time: 0.0042   lr: 6.3533e-05  max_mem: 14757M
[12/05 00:03:31] d2.utils.events INFO:  eta: 0:31:58  iter: 3979  total_loss: 15.27  loss_ce: 0.1522  loss_mask: 0.3032  loss_dice: 0.8023  loss_contrastive: 0  loss_ce_0: 0.7551  loss_mask_0: 0.3685  loss_dice_0: 0.8711  loss_ce_1: 0.399  loss_mask_1: 0.3  loss_dice_1: 0.833  loss_ce_2: 0.2993  loss_mask_2: 0.295  loss_dice_2: 0.7517  loss_ce_3: 0.2253  loss_mask_3: 0.3692  loss_dice_3: 0.7594  loss_ce_4: 0.23  loss_mask_4: 0.3533  loss_dice_4: 0.6731  loss_ce_5: 0.1874  loss_mask_5: 0.3326  loss_dice_5: 0.7405  loss_ce_6: 0.1995  loss_mask_6: 0.3355  loss_dice_6: 0.6349  loss_ce_7: 0.1631  loss_mask_7: 0.3239  loss_dice_7: 0.7958  loss_ce_8: 0.1602  loss_mask_8: 0.3169  loss_dice_8: 0.7674    time: 0.3219  last_time: 0.3229  data_time: 0.0031  last_data_time: 0.0031   lr: 6.3343e-05  max_mem: 14757M
[12/05 00:03:37] d2.utils.events INFO:  eta: 0:31:52  iter: 3999  total_loss: 22.07  loss_ce: 0.5053  loss_mask: 0.298  loss_dice: 0.9904  loss_contrastive: 0  loss_ce_0: 1.502  loss_mask_0: 0.4098  loss_dice_0: 1.285  loss_ce_1: 1.079  loss_mask_1: 0.3326  loss_dice_1: 1.072  loss_ce_2: 0.9852  loss_mask_2: 0.3375  loss_dice_2: 1.09  loss_ce_3: 0.7168  loss_mask_3: 0.2794  loss_dice_3: 1.005  loss_ce_4: 0.6578  loss_mask_4: 0.2966  loss_dice_4: 1.008  loss_ce_5: 0.6348  loss_mask_5: 0.3213  loss_dice_5: 0.9791  loss_ce_6: 0.6524  loss_mask_6: 0.3164  loss_dice_6: 0.9511  loss_ce_7: 0.659  loss_mask_7: 0.3148  loss_dice_7: 0.9673  loss_ce_8: 0.5825  loss_mask_8: 0.3402  loss_dice_8: 0.9864    time: 0.3219  last_time: 0.3079  data_time: 0.0032  last_data_time: 0.0033   lr: 6.3154e-05  max_mem: 14757M
[12/05 00:03:44] d2.utils.events INFO:  eta: 0:31:46  iter: 4019  total_loss: 20.01  loss_ce: 0.276  loss_mask: 0.3818  loss_dice: 1.217  loss_contrastive: 0  loss_ce_0: 1.021  loss_mask_0: 0.3231  loss_dice_0: 1.493  loss_ce_1: 0.7681  loss_mask_1: 0.3426  loss_dice_1: 1.231  loss_ce_2: 0.6063  loss_mask_2: 0.3049  loss_dice_2: 1.155  loss_ce_3: 0.4356  loss_mask_3: 0.2914  loss_dice_3: 1.116  loss_ce_4: 0.3738  loss_mask_4: 0.2721  loss_dice_4: 1.19  loss_ce_5: 0.3257  loss_mask_5: 0.2768  loss_dice_5: 1.147  loss_ce_6: 0.3198  loss_mask_6: 0.2932  loss_dice_6: 1.066  loss_ce_7: 0.2755  loss_mask_7: 0.3385  loss_dice_7: 1.095  loss_ce_8: 0.3289  loss_mask_8: 0.3667  loss_dice_8: 1.132    time: 0.3219  last_time: 0.3064  data_time: 0.0034  last_data_time: 0.0026   lr: 6.2965e-05  max_mem: 14757M
[12/05 00:03:50] d2.utils.events INFO:  eta: 0:31:40  iter: 4039  total_loss: 23.52  loss_ce: 0.2371  loss_mask: 0.4429  loss_dice: 1.176  loss_contrastive: 0  loss_ce_0: 1.123  loss_mask_0: 0.4942  loss_dice_0: 1.311  loss_ce_1: 0.871  loss_mask_1: 0.4546  loss_dice_1: 1.259  loss_ce_2: 0.5883  loss_mask_2: 0.4772  loss_dice_2: 1.181  loss_ce_3: 0.5361  loss_mask_3: 0.4796  loss_dice_3: 1.173  loss_ce_4: 0.5199  loss_mask_4: 0.4944  loss_dice_4: 1.115  loss_ce_5: 0.3844  loss_mask_5: 0.4956  loss_dice_5: 1.185  loss_ce_6: 0.3683  loss_mask_6: 0.4739  loss_dice_6: 1.073  loss_ce_7: 0.3286  loss_mask_7: 0.4857  loss_dice_7: 1.172  loss_ce_8: 0.3123  loss_mask_8: 0.4538  loss_dice_8: 1.12    time: 0.3218  last_time: 0.3096  data_time: 0.0035  last_data_time: 0.0029   lr: 6.2775e-05  max_mem: 14757M
[12/05 00:03:56] d2.utils.events INFO:  eta: 0:31:33  iter: 4059  total_loss: 19.27  loss_ce: 0.3783  loss_mask: 0.3619  loss_dice: 0.9989  loss_contrastive: 0  loss_ce_0: 1.07  loss_mask_0: 0.5043  loss_dice_0: 1.184  loss_ce_1: 0.6957  loss_mask_1: 0.4148  loss_dice_1: 0.9397  loss_ce_2: 0.5683  loss_mask_2: 0.3991  loss_dice_2: 0.9691  loss_ce_3: 0.5531  loss_mask_3: 0.4191  loss_dice_3: 0.9808  loss_ce_4: 0.4858  loss_mask_4: 0.4136  loss_dice_4: 0.9416  loss_ce_5: 0.4308  loss_mask_5: 0.4016  loss_dice_5: 0.9034  loss_ce_6: 0.4564  loss_mask_6: 0.3598  loss_dice_6: 0.8019  loss_ce_7: 0.4272  loss_mask_7: 0.3659  loss_dice_7: 0.8527  loss_ce_8: 0.3988  loss_mask_8: 0.3502  loss_dice_8: 0.9187    time: 0.3218  last_time: 0.3196  data_time: 0.0031  last_data_time: 0.0029   lr: 6.2585e-05  max_mem: 14757M
[12/05 00:04:03] d2.utils.events INFO:  eta: 0:31:27  iter: 4079  total_loss: 16.17  loss_ce: 0.2108  loss_mask: 0.3544  loss_dice: 0.8563  loss_contrastive: 0  loss_ce_0: 1.087  loss_mask_0: 0.4336  loss_dice_0: 1.336  loss_ce_1: 0.6448  loss_mask_1: 0.3307  loss_dice_1: 1.001  loss_ce_2: 0.405  loss_mask_2: 0.3331  loss_dice_2: 0.9814  loss_ce_3: 0.2923  loss_mask_3: 0.3294  loss_dice_3: 0.9474  loss_ce_4: 0.3182  loss_mask_4: 0.3451  loss_dice_4: 0.8699  loss_ce_5: 0.2309  loss_mask_5: 0.3428  loss_dice_5: 0.8562  loss_ce_6: 0.2372  loss_mask_6: 0.3369  loss_dice_6: 0.8535  loss_ce_7: 0.2341  loss_mask_7: 0.3536  loss_dice_7: 0.8649  loss_ce_8: 0.2183  loss_mask_8: 0.3553  loss_dice_8: 0.838    time: 0.3218  last_time: 0.3166  data_time: 0.0037  last_data_time: 0.0029   lr: 6.2396e-05  max_mem: 14757M
[12/05 00:04:09] d2.utils.events INFO:  eta: 0:31:19  iter: 4099  total_loss: 19.94  loss_ce: 0.4689  loss_mask: 0.3196  loss_dice: 0.9173  loss_contrastive: 0  loss_ce_0: 1.06  loss_mask_0: 0.4365  loss_dice_0: 1.124  loss_ce_1: 0.7514  loss_mask_1: 0.3642  loss_dice_1: 1.087  loss_ce_2: 0.5581  loss_mask_2: 0.3218  loss_dice_2: 0.9299  loss_ce_3: 0.5331  loss_mask_3: 0.2596  loss_dice_3: 0.8969  loss_ce_4: 0.4949  loss_mask_4: 0.2833  loss_dice_4: 0.9747  loss_ce_5: 0.5227  loss_mask_5: 0.2519  loss_dice_5: 1.007  loss_ce_6: 0.4219  loss_mask_6: 0.287  loss_dice_6: 0.8918  loss_ce_7: 0.4635  loss_mask_7: 0.3018  loss_dice_7: 0.8637  loss_ce_8: 0.4359  loss_mask_8: 0.3197  loss_dice_8: 0.9588    time: 0.3218  last_time: 0.3147  data_time: 0.0038  last_data_time: 0.0032   lr: 6.2206e-05  max_mem: 14757M
[12/05 00:04:16] d2.utils.events INFO:  eta: 0:31:12  iter: 4119  total_loss: 19.96  loss_ce: 0.5186  loss_mask: 0.3342  loss_dice: 0.9073  loss_contrastive: 0  loss_ce_0: 1.085  loss_mask_0: 0.3682  loss_dice_0: 0.9769  loss_ce_1: 0.8208  loss_mask_1: 0.3369  loss_dice_1: 1.103  loss_ce_2: 0.6313  loss_mask_2: 0.3637  loss_dice_2: 1.054  loss_ce_3: 0.5312  loss_mask_3: 0.3485  loss_dice_3: 0.9314  loss_ce_4: 0.545  loss_mask_4: 0.3567  loss_dice_4: 0.9587  loss_ce_5: 0.4696  loss_mask_5: 0.3539  loss_dice_5: 0.9908  loss_ce_6: 0.5009  loss_mask_6: 0.3527  loss_dice_6: 0.882  loss_ce_7: 0.5104  loss_mask_7: 0.3492  loss_dice_7: 0.9142  loss_ce_8: 0.4478  loss_mask_8: 0.3407  loss_dice_8: 0.997    time: 0.3218  last_time: 0.3249  data_time: 0.0034  last_data_time: 0.0032   lr: 6.2016e-05  max_mem: 14757M
[12/05 00:04:22] d2.utils.events INFO:  eta: 0:31:06  iter: 4139  total_loss: 17.68  loss_ce: 0.3684  loss_mask: 0.2721  loss_dice: 0.9797  loss_contrastive: 0  loss_ce_0: 1.055  loss_mask_0: 0.3275  loss_dice_0: 1.021  loss_ce_1: 0.6233  loss_mask_1: 0.29  loss_dice_1: 0.9373  loss_ce_2: 0.5214  loss_mask_2: 0.2684  loss_dice_2: 0.96  loss_ce_3: 0.504  loss_mask_3: 0.2755  loss_dice_3: 0.9424  loss_ce_4: 0.4051  loss_mask_4: 0.3137  loss_dice_4: 0.9122  loss_ce_5: 0.3481  loss_mask_5: 0.277  loss_dice_5: 0.9588  loss_ce_6: 0.3315  loss_mask_6: 0.2776  loss_dice_6: 0.8633  loss_ce_7: 0.4064  loss_mask_7: 0.2735  loss_dice_7: 0.9536  loss_ce_8: 0.3441  loss_mask_8: 0.2866  loss_dice_8: 0.9903    time: 0.3218  last_time: 0.3259  data_time: 0.0039  last_data_time: 0.0025   lr: 6.1826e-05  max_mem: 14757M
[12/05 00:04:29] d2.utils.events INFO:  eta: 0:30:59  iter: 4159  total_loss: 19.24  loss_ce: 0.3534  loss_mask: 0.3095  loss_dice: 0.6657  loss_contrastive: 0  loss_ce_0: 1.028  loss_mask_0: 0.3468  loss_dice_0: 0.8776  loss_ce_1: 0.5426  loss_mask_1: 0.2966  loss_dice_1: 0.8227  loss_ce_2: 0.3599  loss_mask_2: 0.273  loss_dice_2: 0.7098  loss_ce_3: 0.365  loss_mask_3: 0.2856  loss_dice_3: 0.6471  loss_ce_4: 0.3646  loss_mask_4: 0.3059  loss_dice_4: 0.6633  loss_ce_5: 0.3396  loss_mask_5: 0.3146  loss_dice_5: 0.6946  loss_ce_6: 0.2636  loss_mask_6: 0.3111  loss_dice_6: 0.6974  loss_ce_7: 0.3239  loss_mask_7: 0.3145  loss_dice_7: 0.6542  loss_ce_8: 0.3337  loss_mask_8: 0.2738  loss_dice_8: 0.6502    time: 0.3218  last_time: 0.3208  data_time: 0.0035  last_data_time: 0.0027   lr: 6.1637e-05  max_mem: 14757M
[12/05 00:04:35] d2.utils.events INFO:  eta: 0:30:52  iter: 4179  total_loss: 21.94  loss_ce: 0.3496  loss_mask: 0.3533  loss_dice: 1.101  loss_contrastive: 0  loss_ce_0: 1.269  loss_mask_0: 0.4174  loss_dice_0: 1.492  loss_ce_1: 0.7373  loss_mask_1: 0.3218  loss_dice_1: 1.223  loss_ce_2: 0.5712  loss_mask_2: 0.3516  loss_dice_2: 1.037  loss_ce_3: 0.4775  loss_mask_3: 0.3499  loss_dice_3: 1.127  loss_ce_4: 0.4259  loss_mask_4: 0.3461  loss_dice_4: 1.058  loss_ce_5: 0.3356  loss_mask_5: 0.3488  loss_dice_5: 1.173  loss_ce_6: 0.3468  loss_mask_6: 0.3543  loss_dice_6: 1.212  loss_ce_7: 0.3478  loss_mask_7: 0.2926  loss_dice_7: 1.164  loss_ce_8: 0.3571  loss_mask_8: 0.3266  loss_dice_8: 1.233    time: 0.3218  last_time: 0.3350  data_time: 0.0035  last_data_time: 0.0027   lr: 6.1447e-05  max_mem: 14757M
[12/05 00:04:42] d2.utils.events INFO:  eta: 0:30:47  iter: 4199  total_loss: 23.75  loss_ce: 0.4403  loss_mask: 0.3574  loss_dice: 1.14  loss_contrastive: 0  loss_ce_0: 1.237  loss_mask_0: 0.4202  loss_dice_0: 1.566  loss_ce_1: 0.8897  loss_mask_1: 0.3398  loss_dice_1: 1.31  loss_ce_2: 0.7206  loss_mask_2: 0.3929  loss_dice_2: 1.257  loss_ce_3: 0.6123  loss_mask_3: 0.3719  loss_dice_3: 1.183  loss_ce_4: 0.4539  loss_mask_4: 0.3544  loss_dice_4: 1.1  loss_ce_5: 0.3998  loss_mask_5: 0.3371  loss_dice_5: 1.105  loss_ce_6: 0.5009  loss_mask_6: 0.3505  loss_dice_6: 1.057  loss_ce_7: 0.4833  loss_mask_7: 0.3738  loss_dice_7: 1.224  loss_ce_8: 0.416  loss_mask_8: 0.3671  loss_dice_8: 1.279    time: 0.3218  last_time: 0.3143  data_time: 0.0039  last_data_time: 0.0047   lr: 6.1257e-05  max_mem: 14757M
[12/05 00:04:48] d2.utils.events INFO:  eta: 0:30:41  iter: 4219  total_loss: 18.84  loss_ce: 0.2514  loss_mask: 0.3134  loss_dice: 1.007  loss_contrastive: 0  loss_ce_0: 1.092  loss_mask_0: 0.3442  loss_dice_0: 1.312  loss_ce_1: 0.6222  loss_mask_1: 0.2835  loss_dice_1: 1.175  loss_ce_2: 0.4179  loss_mask_2: 0.2935  loss_dice_2: 1.157  loss_ce_3: 0.3581  loss_mask_3: 0.3335  loss_dice_3: 1.124  loss_ce_4: 0.3299  loss_mask_4: 0.3129  loss_dice_4: 1.056  loss_ce_5: 0.3083  loss_mask_5: 0.319  loss_dice_5: 1.086  loss_ce_6: 0.2991  loss_mask_6: 0.3481  loss_dice_6: 1.041  loss_ce_7: 0.2835  loss_mask_7: 0.3248  loss_dice_7: 1.111  loss_ce_8: 0.2875  loss_mask_8: 0.2916  loss_dice_8: 1.085    time: 0.3218  last_time: 0.3144  data_time: 0.0036  last_data_time: 0.0029   lr: 6.1066e-05  max_mem: 14757M
[12/05 00:04:54] d2.utils.events INFO:  eta: 0:30:35  iter: 4239  total_loss: 22.12  loss_ce: 0.4177  loss_mask: 0.3294  loss_dice: 0.9625  loss_contrastive: 0  loss_ce_0: 1.213  loss_mask_0: 0.4115  loss_dice_0: 1.15  loss_ce_1: 0.7579  loss_mask_1: 0.3364  loss_dice_1: 1.072  loss_ce_2: 0.6147  loss_mask_2: 0.321  loss_dice_2: 0.9595  loss_ce_3: 0.54  loss_mask_3: 0.3481  loss_dice_3: 0.9586  loss_ce_4: 0.4892  loss_mask_4: 0.3258  loss_dice_4: 0.9761  loss_ce_5: 0.4944  loss_mask_5: 0.3714  loss_dice_5: 0.936  loss_ce_6: 0.4456  loss_mask_6: 0.328  loss_dice_6: 0.9843  loss_ce_7: 0.461  loss_mask_7: 0.3314  loss_dice_7: 0.9613  loss_ce_8: 0.4922  loss_mask_8: 0.3117  loss_dice_8: 0.9553    time: 0.3218  last_time: 0.3197  data_time: 0.0037  last_data_time: 0.0027   lr: 6.0876e-05  max_mem: 14757M
[12/05 00:05:01] d2.utils.events INFO:  eta: 0:30:30  iter: 4259  total_loss: 23.08  loss_ce: 0.4383  loss_mask: 0.3426  loss_dice: 1.209  loss_contrastive: 0  loss_ce_0: 1.137  loss_mask_0: 0.3292  loss_dice_0: 1.267  loss_ce_1: 0.7687  loss_mask_1: 0.3763  loss_dice_1: 1.315  loss_ce_2: 0.5421  loss_mask_2: 0.3039  loss_dice_2: 1.193  loss_ce_3: 0.5895  loss_mask_3: 0.3021  loss_dice_3: 1.139  loss_ce_4: 0.4407  loss_mask_4: 0.3496  loss_dice_4: 1.194  loss_ce_5: 0.4113  loss_mask_5: 0.3255  loss_dice_5: 1.216  loss_ce_6: 0.5057  loss_mask_6: 0.3585  loss_dice_6: 1.094  loss_ce_7: 0.5089  loss_mask_7: 0.3591  loss_dice_7: 1.074  loss_ce_8: 0.4447  loss_mask_8: 0.381  loss_dice_8: 1.24    time: 0.3218  last_time: 0.3079  data_time: 0.0036  last_data_time: 0.0033   lr: 6.0686e-05  max_mem: 14757M
[12/05 00:05:07] d2.utils.events INFO:  eta: 0:30:23  iter: 4279  total_loss: 21.53  loss_ce: 0.3297  loss_mask: 0.255  loss_dice: 0.9633  loss_contrastive: 0  loss_ce_0: 1.331  loss_mask_0: 0.312  loss_dice_0: 1.183  loss_ce_1: 0.7827  loss_mask_1: 0.2687  loss_dice_1: 1.252  loss_ce_2: 0.7141  loss_mask_2: 0.2664  loss_dice_2: 1.127  loss_ce_3: 0.4702  loss_mask_3: 0.2669  loss_dice_3: 1.057  loss_ce_4: 0.3608  loss_mask_4: 0.2732  loss_dice_4: 1.054  loss_ce_5: 0.3399  loss_mask_5: 0.2837  loss_dice_5: 1.095  loss_ce_6: 0.3581  loss_mask_6: 0.2469  loss_dice_6: 0.9965  loss_ce_7: 0.3512  loss_mask_7: 0.2471  loss_dice_7: 0.9132  loss_ce_8: 0.3226  loss_mask_8: 0.2463  loss_dice_8: 1.007    time: 0.3218  last_time: 0.3216  data_time: 0.0034  last_data_time: 0.0055   lr: 6.0496e-05  max_mem: 14757M
[12/05 00:05:14] d2.utils.events INFO:  eta: 0:30:16  iter: 4299  total_loss: 22.83  loss_ce: 0.3281  loss_mask: 0.3539  loss_dice: 1.195  loss_contrastive: 0  loss_ce_0: 1.211  loss_mask_0: 0.4207  loss_dice_0: 1.392  loss_ce_1: 0.7034  loss_mask_1: 0.3262  loss_dice_1: 1.318  loss_ce_2: 0.5441  loss_mask_2: 0.3219  loss_dice_2: 1.228  loss_ce_3: 0.4232  loss_mask_3: 0.3296  loss_dice_3: 1.241  loss_ce_4: 0.419  loss_mask_4: 0.3303  loss_dice_4: 1.155  loss_ce_5: 0.4681  loss_mask_5: 0.3694  loss_dice_5: 1.222  loss_ce_6: 0.5244  loss_mask_6: 0.3643  loss_dice_6: 1.234  loss_ce_7: 0.5136  loss_mask_7: 0.3453  loss_dice_7: 1.177  loss_ce_8: 0.3555  loss_mask_8: 0.36  loss_dice_8: 1.14    time: 0.3218  last_time: 0.3052  data_time: 0.0032  last_data_time: 0.0044   lr: 6.0305e-05  max_mem: 14757M
[12/05 00:05:20] d2.utils.events INFO:  eta: 0:30:09  iter: 4319  total_loss: 19.44  loss_ce: 0.4017  loss_mask: 0.3143  loss_dice: 0.9946  loss_contrastive: 0  loss_ce_0: 1.035  loss_mask_0: 0.3267  loss_dice_0: 1.256  loss_ce_1: 0.6941  loss_mask_1: 0.3271  loss_dice_1: 1.349  loss_ce_2: 0.5367  loss_mask_2: 0.3271  loss_dice_2: 1.155  loss_ce_3: 0.4159  loss_mask_3: 0.2771  loss_dice_3: 1.008  loss_ce_4: 0.3872  loss_mask_4: 0.3207  loss_dice_4: 1.042  loss_ce_5: 0.2897  loss_mask_5: 0.3193  loss_dice_5: 1.067  loss_ce_6: 0.2668  loss_mask_6: 0.3201  loss_dice_6: 0.9934  loss_ce_7: 0.3693  loss_mask_7: 0.3121  loss_dice_7: 1.045  loss_ce_8: 0.3389  loss_mask_8: 0.3137  loss_dice_8: 0.966    time: 0.3218  last_time: 0.3069  data_time: 0.0035  last_data_time: 0.0037   lr: 6.0115e-05  max_mem: 14757M
[12/05 00:05:27] d2.utils.events INFO:  eta: 0:30:02  iter: 4339  total_loss: 19.38  loss_ce: 0.3399  loss_mask: 0.2905  loss_dice: 0.8298  loss_contrastive: 0  loss_ce_0: 1.317  loss_mask_0: 0.2814  loss_dice_0: 1.174  loss_ce_1: 0.7601  loss_mask_1: 0.2783  loss_dice_1: 1.029  loss_ce_2: 0.6697  loss_mask_2: 0.2809  loss_dice_2: 0.9164  loss_ce_3: 0.4646  loss_mask_3: 0.2932  loss_dice_3: 0.7917  loss_ce_4: 0.5184  loss_mask_4: 0.2788  loss_dice_4: 0.891  loss_ce_5: 0.3927  loss_mask_5: 0.2889  loss_dice_5: 0.772  loss_ce_6: 0.3802  loss_mask_6: 0.2872  loss_dice_6: 0.7985  loss_ce_7: 0.335  loss_mask_7: 0.2918  loss_dice_7: 0.7897  loss_ce_8: 0.3206  loss_mask_8: 0.2812  loss_dice_8: 0.7681    time: 0.3218  last_time: 0.3281  data_time: 0.0031  last_data_time: 0.0030   lr: 5.9924e-05  max_mem: 14757M
[12/05 00:05:33] d2.utils.events INFO:  eta: 0:29:56  iter: 4359  total_loss: 20.98  loss_ce: 0.4826  loss_mask: 0.2773  loss_dice: 0.9255  loss_contrastive: 0  loss_ce_0: 1.152  loss_mask_0: 0.4383  loss_dice_0: 1.104  loss_ce_1: 0.7824  loss_mask_1: 0.3312  loss_dice_1: 0.988  loss_ce_2: 0.6332  loss_mask_2: 0.3508  loss_dice_2: 0.9768  loss_ce_3: 0.519  loss_mask_3: 0.311  loss_dice_3: 0.9206  loss_ce_4: 0.4166  loss_mask_4: 0.3217  loss_dice_4: 0.9323  loss_ce_5: 0.4922  loss_mask_5: 0.2858  loss_dice_5: 1.001  loss_ce_6: 0.4454  loss_mask_6: 0.2587  loss_dice_6: 0.9231  loss_ce_7: 0.4478  loss_mask_7: 0.2552  loss_dice_7: 0.9733  loss_ce_8: 0.4541  loss_mask_8: 0.2753  loss_dice_8: 0.8794    time: 0.3218  last_time: 0.3154  data_time: 0.0031  last_data_time: 0.0027   lr: 5.9734e-05  max_mem: 14757M
[12/05 00:05:39] d2.utils.events INFO:  eta: 0:29:47  iter: 4379  total_loss: 19.78  loss_ce: 0.41  loss_mask: 0.2109  loss_dice: 1.087  loss_contrastive: 0  loss_ce_0: 1.29  loss_mask_0: 0.24  loss_dice_0: 1.222  loss_ce_1: 0.7763  loss_mask_1: 0.2546  loss_dice_1: 1.218  loss_ce_2: 0.5905  loss_mask_2: 0.278  loss_dice_2: 1.141  loss_ce_3: 0.5251  loss_mask_3: 0.2254  loss_dice_3: 1.022  loss_ce_4: 0.4914  loss_mask_4: 0.2182  loss_dice_4: 1.04  loss_ce_5: 0.3808  loss_mask_5: 0.2403  loss_dice_5: 1.103  loss_ce_6: 0.3913  loss_mask_6: 0.2443  loss_dice_6: 1.032  loss_ce_7: 0.3937  loss_mask_7: 0.2322  loss_dice_7: 1.03  loss_ce_8: 0.3438  loss_mask_8: 0.2159  loss_dice_8: 1.059    time: 0.3218  last_time: 0.3033  data_time: 0.0035  last_data_time: 0.0027   lr: 5.9543e-05  max_mem: 14757M
[12/05 00:05:46] d2.utils.events INFO:  eta: 0:29:43  iter: 4399  total_loss: 17.15  loss_ce: 0.17  loss_mask: 0.3136  loss_dice: 0.9336  loss_contrastive: 0  loss_ce_0: 1.21  loss_mask_0: 0.4249  loss_dice_0: 0.9872  loss_ce_1: 0.568  loss_mask_1: 0.3335  loss_dice_1: 0.906  loss_ce_2: 0.2569  loss_mask_2: 0.3261  loss_dice_2: 0.8303  loss_ce_3: 0.2265  loss_mask_3: 0.3193  loss_dice_3: 0.8205  loss_ce_4: 0.2058  loss_mask_4: 0.312  loss_dice_4: 0.9467  loss_ce_5: 0.2317  loss_mask_5: 0.3036  loss_dice_5: 0.9242  loss_ce_6: 0.177  loss_mask_6: 0.3439  loss_dice_6: 0.8678  loss_ce_7: 0.2158  loss_mask_7: 0.2979  loss_dice_7: 0.8449  loss_ce_8: 0.2218  loss_mask_8: 0.3119  loss_dice_8: 0.8471    time: 0.3218  last_time: 0.3114  data_time: 0.0037  last_data_time: 0.0034   lr: 5.9352e-05  max_mem: 14757M
[12/05 00:05:52] d2.utils.events INFO:  eta: 0:29:37  iter: 4419  total_loss: 21.43  loss_ce: 0.3462  loss_mask: 0.2079  loss_dice: 0.8296  loss_contrastive: 0  loss_ce_0: 0.84  loss_mask_0: 0.2819  loss_dice_0: 1.152  loss_ce_1: 0.5621  loss_mask_1: 0.2224  loss_dice_1: 0.9408  loss_ce_2: 0.4519  loss_mask_2: 0.2203  loss_dice_2: 0.9204  loss_ce_3: 0.3547  loss_mask_3: 0.206  loss_dice_3: 0.8763  loss_ce_4: 0.3211  loss_mask_4: 0.239  loss_dice_4: 0.8814  loss_ce_5: 0.2775  loss_mask_5: 0.2449  loss_dice_5: 0.8864  loss_ce_6: 0.3789  loss_mask_6: 0.2229  loss_dice_6: 0.8875  loss_ce_7: 0.4224  loss_mask_7: 0.2242  loss_dice_7: 0.8478  loss_ce_8: 0.3673  loss_mask_8: 0.219  loss_dice_8: 0.8147    time: 0.3218  last_time: 0.3086  data_time: 0.0037  last_data_time: 0.0034   lr: 5.9162e-05  max_mem: 14757M
[12/05 00:05:59] d2.utils.events INFO:  eta: 0:29:31  iter: 4439  total_loss: 20.05  loss_ce: 0.3028  loss_mask: 0.4258  loss_dice: 0.9988  loss_contrastive: 0  loss_ce_0: 0.9826  loss_mask_0: 0.5375  loss_dice_0: 1.09  loss_ce_1: 0.6363  loss_mask_1: 0.4708  loss_dice_1: 1.016  loss_ce_2: 0.6197  loss_mask_2: 0.4321  loss_dice_2: 0.9553  loss_ce_3: 0.522  loss_mask_3: 0.4322  loss_dice_3: 0.9744  loss_ce_4: 0.477  loss_mask_4: 0.4258  loss_dice_4: 0.9162  loss_ce_5: 0.4139  loss_mask_5: 0.4274  loss_dice_5: 0.8388  loss_ce_6: 0.3023  loss_mask_6: 0.4172  loss_dice_6: 1.071  loss_ce_7: 0.3532  loss_mask_7: 0.4046  loss_dice_7: 0.9164  loss_ce_8: 0.3394  loss_mask_8: 0.4288  loss_dice_8: 0.8667    time: 0.3218  last_time: 0.3303  data_time: 0.0029  last_data_time: 0.0026   lr: 5.8971e-05  max_mem: 14757M
[12/05 00:06:05] d2.utils.events INFO:  eta: 0:29:24  iter: 4459  total_loss: 18.9  loss_ce: 0.2779  loss_mask: 0.3267  loss_dice: 0.897  loss_contrastive: 0  loss_ce_0: 1.187  loss_mask_0: 0.4781  loss_dice_0: 1.218  loss_ce_1: 0.6727  loss_mask_1: 0.3414  loss_dice_1: 0.9828  loss_ce_2: 0.4602  loss_mask_2: 0.3279  loss_dice_2: 0.9943  loss_ce_3: 0.3281  loss_mask_3: 0.3783  loss_dice_3: 0.911  loss_ce_4: 0.3987  loss_mask_4: 0.311  loss_dice_4: 0.9355  loss_ce_5: 0.2815  loss_mask_5: 0.3687  loss_dice_5: 0.9198  loss_ce_6: 0.3368  loss_mask_6: 0.3513  loss_dice_6: 0.8409  loss_ce_7: 0.2408  loss_mask_7: 0.3625  loss_dice_7: 0.877  loss_ce_8: 0.2336  loss_mask_8: 0.3523  loss_dice_8: 0.9125    time: 0.3218  last_time: 0.3459  data_time: 0.0033  last_data_time: 0.0029   lr: 5.878e-05  max_mem: 14757M
[12/05 00:06:12] d2.utils.events INFO:  eta: 0:29:17  iter: 4479  total_loss: 18.92  loss_ce: 0.2712  loss_mask: 0.301  loss_dice: 0.991  loss_contrastive: 0  loss_ce_0: 1.386  loss_mask_0: 0.3375  loss_dice_0: 1.315  loss_ce_1: 0.7635  loss_mask_1: 0.2713  loss_dice_1: 1.001  loss_ce_2: 0.5154  loss_mask_2: 0.272  loss_dice_2: 1.003  loss_ce_3: 0.3574  loss_mask_3: 0.2806  loss_dice_3: 1.058  loss_ce_4: 0.3106  loss_mask_4: 0.28  loss_dice_4: 1.029  loss_ce_5: 0.2521  loss_mask_5: 0.2809  loss_dice_5: 1.056  loss_ce_6: 0.3154  loss_mask_6: 0.2713  loss_dice_6: 0.9646  loss_ce_7: 0.4361  loss_mask_7: 0.2868  loss_dice_7: 0.9679  loss_ce_8: 0.2979  loss_mask_8: 0.305  loss_dice_8: 0.9725    time: 0.3218  last_time: 0.3181  data_time: 0.0039  last_data_time: 0.0062   lr: 5.8589e-05  max_mem: 14757M
[12/05 00:06:18] d2.utils.events INFO:  eta: 0:29:11  iter: 4499  total_loss: 18.76  loss_ce: 0.3898  loss_mask: 0.3182  loss_dice: 0.8118  loss_contrastive: 0  loss_ce_0: 1.062  loss_mask_0: 0.3702  loss_dice_0: 1.067  loss_ce_1: 0.6594  loss_mask_1: 0.3634  loss_dice_1: 0.9006  loss_ce_2: 0.636  loss_mask_2: 0.3208  loss_dice_2: 0.7527  loss_ce_3: 0.5342  loss_mask_3: 0.317  loss_dice_3: 0.7847  loss_ce_4: 0.4794  loss_mask_4: 0.2744  loss_dice_4: 0.7438  loss_ce_5: 0.4689  loss_mask_5: 0.2795  loss_dice_5: 0.7633  loss_ce_6: 0.3627  loss_mask_6: 0.2933  loss_dice_6: 0.7941  loss_ce_7: 0.3771  loss_mask_7: 0.3201  loss_dice_7: 0.7666  loss_ce_8: 0.4063  loss_mask_8: 0.3124  loss_dice_8: 0.7765    time: 0.3218  last_time: 0.3084  data_time: 0.0033  last_data_time: 0.0055   lr: 5.8398e-05  max_mem: 14757M
[12/05 00:06:25] d2.utils.events INFO:  eta: 0:29:05  iter: 4519  total_loss: 23.06  loss_ce: 0.3168  loss_mask: 0.2825  loss_dice: 1.109  loss_contrastive: 0  loss_ce_0: 1.093  loss_mask_0: 0.3771  loss_dice_0: 1.534  loss_ce_1: 0.8161  loss_mask_1: 0.2926  loss_dice_1: 1.25  loss_ce_2: 0.5264  loss_mask_2: 0.2974  loss_dice_2: 1.07  loss_ce_3: 0.3528  loss_mask_3: 0.3106  loss_dice_3: 1.192  loss_ce_4: 0.3874  loss_mask_4: 0.3107  loss_dice_4: 1.178  loss_ce_5: 0.4364  loss_mask_5: 0.2996  loss_dice_5: 1.174  loss_ce_6: 0.2642  loss_mask_6: 0.289  loss_dice_6: 1.222  loss_ce_7: 0.2249  loss_mask_7: 0.2815  loss_dice_7: 1.191  loss_ce_8: 0.242  loss_mask_8: 0.3022  loss_dice_8: 1.233    time: 0.3218  last_time: 0.3066  data_time: 0.0037  last_data_time: 0.0041   lr: 5.8207e-05  max_mem: 14757M
[12/05 00:06:31] d2.utils.events INFO:  eta: 0:28:59  iter: 4539  total_loss: 16.35  loss_ce: 0.1729  loss_mask: 0.2366  loss_dice: 0.7981  loss_contrastive: 0  loss_ce_0: 1.249  loss_mask_0: 0.2488  loss_dice_0: 1.063  loss_ce_1: 0.7044  loss_mask_1: 0.2282  loss_dice_1: 0.9063  loss_ce_2: 0.3855  loss_mask_2: 0.2316  loss_dice_2: 0.833  loss_ce_3: 0.3194  loss_mask_3: 0.2315  loss_dice_3: 0.9285  loss_ce_4: 0.2112  loss_mask_4: 0.2375  loss_dice_4: 0.8044  loss_ce_5: 0.2648  loss_mask_5: 0.248  loss_dice_5: 0.8204  loss_ce_6: 0.184  loss_mask_6: 0.2569  loss_dice_6: 0.773  loss_ce_7: 0.1876  loss_mask_7: 0.2469  loss_dice_7: 0.8016  loss_ce_8: 0.2304  loss_mask_8: 0.2358  loss_dice_8: 0.7263    time: 0.3218  last_time: 0.3360  data_time: 0.0031  last_data_time: 0.0030   lr: 5.8016e-05  max_mem: 14757M
[12/05 00:06:37] d2.utils.events INFO:  eta: 0:28:52  iter: 4559  total_loss: 21.92  loss_ce: 0.2858  loss_mask: 0.2346  loss_dice: 0.8628  loss_contrastive: 0  loss_ce_0: 1.629  loss_mask_0: 0.3236  loss_dice_0: 1.355  loss_ce_1: 0.9097  loss_mask_1: 0.2394  loss_dice_1: 1.046  loss_ce_2: 0.6696  loss_mask_2: 0.2275  loss_dice_2: 0.8876  loss_ce_3: 0.5194  loss_mask_3: 0.2427  loss_dice_3: 0.9401  loss_ce_4: 0.3344  loss_mask_4: 0.2213  loss_dice_4: 0.9704  loss_ce_5: 0.4519  loss_mask_5: 0.2287  loss_dice_5: 0.8613  loss_ce_6: 0.3232  loss_mask_6: 0.2338  loss_dice_6: 0.8616  loss_ce_7: 0.3323  loss_mask_7: 0.2193  loss_dice_7: 0.7822  loss_ce_8: 0.3898  loss_mask_8: 0.2241  loss_dice_8: 0.8856    time: 0.3218  last_time: 0.3173  data_time: 0.0037  last_data_time: 0.0026   lr: 5.7824e-05  max_mem: 14757M
[12/05 00:06:44] d2.utils.events INFO:  eta: 0:28:45  iter: 4579  total_loss: 24.16  loss_ce: 0.3493  loss_mask: 0.3263  loss_dice: 1.166  loss_contrastive: 0  loss_ce_0: 1.196  loss_mask_0: 0.3682  loss_dice_0: 1.572  loss_ce_1: 0.9276  loss_mask_1: 0.3111  loss_dice_1: 1.364  loss_ce_2: 0.7248  loss_mask_2: 0.345  loss_dice_2: 1.302  loss_ce_3: 0.601  loss_mask_3: 0.3618  loss_dice_3: 1.21  loss_ce_4: 0.4473  loss_mask_4: 0.3098  loss_dice_4: 1.211  loss_ce_5: 0.4747  loss_mask_5: 0.3172  loss_dice_5: 1.18  loss_ce_6: 0.4315  loss_mask_6: 0.3254  loss_dice_6: 1.128  loss_ce_7: 0.3562  loss_mask_7: 0.3198  loss_dice_7: 1.216  loss_ce_8: 0.3067  loss_mask_8: 0.3216  loss_dice_8: 1.302    time: 0.3218  last_time: 0.3047  data_time: 0.0035  last_data_time: 0.0028   lr: 5.7633e-05  max_mem: 14757M
[12/05 00:06:50] d2.utils.events INFO:  eta: 0:28:39  iter: 4599  total_loss: 18.57  loss_ce: 0.3016  loss_mask: 0.2839  loss_dice: 0.907  loss_contrastive: 0  loss_ce_0: 1.28  loss_mask_0: 0.3565  loss_dice_0: 1.247  loss_ce_1: 0.6786  loss_mask_1: 0.3213  loss_dice_1: 1.003  loss_ce_2: 0.5427  loss_mask_2: 0.302  loss_dice_2: 1.067  loss_ce_3: 0.3423  loss_mask_3: 0.2964  loss_dice_3: 0.9541  loss_ce_4: 0.3758  loss_mask_4: 0.265  loss_dice_4: 0.9572  loss_ce_5: 0.3146  loss_mask_5: 0.2885  loss_dice_5: 1.047  loss_ce_6: 0.3345  loss_mask_6: 0.3031  loss_dice_6: 0.9014  loss_ce_7: 0.4241  loss_mask_7: 0.2588  loss_dice_7: 0.8359  loss_ce_8: 0.3747  loss_mask_8: 0.2703  loss_dice_8: 0.9569    time: 0.3218  last_time: 0.3282  data_time: 0.0036  last_data_time: 0.0043   lr: 5.7442e-05  max_mem: 14757M
[12/05 00:06:57] d2.utils.events INFO:  eta: 0:28:33  iter: 4619  total_loss: 20.74  loss_ce: 0.4258  loss_mask: 0.3298  loss_dice: 1.167  loss_contrastive: 0  loss_ce_0: 1.294  loss_mask_0: 0.3845  loss_dice_0: 1.556  loss_ce_1: 0.8918  loss_mask_1: 0.3433  loss_dice_1: 1.308  loss_ce_2: 0.6289  loss_mask_2: 0.3296  loss_dice_2: 1.309  loss_ce_3: 0.5236  loss_mask_3: 0.323  loss_dice_3: 1.259  loss_ce_4: 0.5772  loss_mask_4: 0.3178  loss_dice_4: 1.188  loss_ce_5: 0.5335  loss_mask_5: 0.3511  loss_dice_5: 1.133  loss_ce_6: 0.5181  loss_mask_6: 0.296  loss_dice_6: 1.128  loss_ce_7: 0.505  loss_mask_7: 0.2996  loss_dice_7: 1.166  loss_ce_8: 0.469  loss_mask_8: 0.3425  loss_dice_8: 1.181    time: 0.3218  last_time: 0.3488  data_time: 0.0039  last_data_time: 0.0028   lr: 5.725e-05  max_mem: 14757M
[12/05 00:07:04] d2.utils.events INFO:  eta: 0:28:27  iter: 4639  total_loss: 21.21  loss_ce: 0.4359  loss_mask: 0.3265  loss_dice: 1.101  loss_contrastive: 0  loss_ce_0: 1.331  loss_mask_0: 0.3797  loss_dice_0: 1.394  loss_ce_1: 0.756  loss_mask_1: 0.3202  loss_dice_1: 1.111  loss_ce_2: 0.6258  loss_mask_2: 0.3637  loss_dice_2: 1.123  loss_ce_3: 0.4377  loss_mask_3: 0.3446  loss_dice_3: 1.025  loss_ce_4: 0.3738  loss_mask_4: 0.368  loss_dice_4: 1.149  loss_ce_5: 0.2769  loss_mask_5: 0.3954  loss_dice_5: 1.161  loss_ce_6: 0.3136  loss_mask_6: 0.3462  loss_dice_6: 1.134  loss_ce_7: 0.3523  loss_mask_7: 0.3376  loss_dice_7: 1.155  loss_ce_8: 0.2964  loss_mask_8: 0.3398  loss_dice_8: 1.146    time: 0.3219  last_time: 0.3207  data_time: 0.0038  last_data_time: 0.0030   lr: 5.7059e-05  max_mem: 14757M
[12/05 00:07:10] d2.utils.events INFO:  eta: 0:28:21  iter: 4659  total_loss: 20.22  loss_ce: 0.3657  loss_mask: 0.3445  loss_dice: 1.005  loss_contrastive: 0  loss_ce_0: 0.9163  loss_mask_0: 0.3904  loss_dice_0: 1.083  loss_ce_1: 0.5512  loss_mask_1: 0.4366  loss_dice_1: 0.9903  loss_ce_2: 0.4844  loss_mask_2: 0.3898  loss_dice_2: 0.934  loss_ce_3: 0.401  loss_mask_3: 0.3694  loss_dice_3: 0.9178  loss_ce_4: 0.3128  loss_mask_4: 0.3715  loss_dice_4: 0.9127  loss_ce_5: 0.339  loss_mask_5: 0.3658  loss_dice_5: 0.9625  loss_ce_6: 0.3202  loss_mask_6: 0.3498  loss_dice_6: 1.01  loss_ce_7: 0.3768  loss_mask_7: 0.361  loss_dice_7: 0.9926  loss_ce_8: 0.3605  loss_mask_8: 0.3354  loss_dice_8: 0.9406    time: 0.3219  last_time: 0.3076  data_time: 0.0034  last_data_time: 0.0028   lr: 5.6867e-05  max_mem: 14757M
[12/05 00:07:17] d2.utils.events INFO:  eta: 0:28:15  iter: 4679  total_loss: 18.46  loss_ce: 0.2287  loss_mask: 0.241  loss_dice: 0.8133  loss_contrastive: 0  loss_ce_0: 1.168  loss_mask_0: 0.2761  loss_dice_0: 1.212  loss_ce_1: 0.6615  loss_mask_1: 0.2443  loss_dice_1: 1.088  loss_ce_2: 0.4434  loss_mask_2: 0.2358  loss_dice_2: 1.025  loss_ce_3: 0.3524  loss_mask_3: 0.2548  loss_dice_3: 0.8953  loss_ce_4: 0.2784  loss_mask_4: 0.2574  loss_dice_4: 1.023  loss_ce_5: 0.2409  loss_mask_5: 0.2507  loss_dice_5: 0.8663  loss_ce_6: 0.2658  loss_mask_6: 0.2519  loss_dice_6: 0.8393  loss_ce_7: 0.2404  loss_mask_7: 0.2551  loss_dice_7: 0.7693  loss_ce_8: 0.245  loss_mask_8: 0.2483  loss_dice_8: 0.9245    time: 0.3219  last_time: 0.3260  data_time: 0.0034  last_data_time: 0.0030   lr: 5.6675e-05  max_mem: 14757M
[12/05 00:07:23] d2.utils.events INFO:  eta: 0:28:08  iter: 4699  total_loss: 20.73  loss_ce: 0.3652  loss_mask: 0.2485  loss_dice: 1.067  loss_contrastive: 0  loss_ce_0: 1.11  loss_mask_0: 0.2987  loss_dice_0: 1.259  loss_ce_1: 0.6764  loss_mask_1: 0.2581  loss_dice_1: 1.066  loss_ce_2: 0.5296  loss_mask_2: 0.2492  loss_dice_2: 1.041  loss_ce_3: 0.4978  loss_mask_3: 0.2387  loss_dice_3: 1.104  loss_ce_4: 0.4545  loss_mask_4: 0.2471  loss_dice_4: 0.9521  loss_ce_5: 0.3759  loss_mask_5: 0.3014  loss_dice_5: 1.091  loss_ce_6: 0.4948  loss_mask_6: 0.2656  loss_dice_6: 1.008  loss_ce_7: 0.4341  loss_mask_7: 0.2362  loss_dice_7: 1.059  loss_ce_8: 0.326  loss_mask_8: 0.2437  loss_dice_8: 1.059    time: 0.3219  last_time: 0.3345  data_time: 0.0034  last_data_time: 0.0034   lr: 5.6484e-05  max_mem: 14757M
[12/05 00:07:30] d2.utils.events INFO:  eta: 0:28:04  iter: 4719  total_loss: 19.48  loss_ce: 0.3835  loss_mask: 0.2586  loss_dice: 1.119  loss_contrastive: 0  loss_ce_0: 1.129  loss_mask_0: 0.2722  loss_dice_0: 1.426  loss_ce_1: 0.6646  loss_mask_1: 0.2551  loss_dice_1: 1.18  loss_ce_2: 0.5384  loss_mask_2: 0.284  loss_dice_2: 1.133  loss_ce_3: 0.4928  loss_mask_3: 0.2424  loss_dice_3: 1.037  loss_ce_4: 0.4382  loss_mask_4: 0.2494  loss_dice_4: 0.9945  loss_ce_5: 0.3448  loss_mask_5: 0.2373  loss_dice_5: 1.074  loss_ce_6: 0.4124  loss_mask_6: 0.2344  loss_dice_6: 1.019  loss_ce_7: 0.3369  loss_mask_7: 0.2577  loss_dice_7: 1.041  loss_ce_8: 0.3091  loss_mask_8: 0.2557  loss_dice_8: 1.049    time: 0.3219  last_time: 0.3383  data_time: 0.0040  last_data_time: 0.0028   lr: 5.6292e-05  max_mem: 14757M
[12/05 00:07:36] d2.utils.events INFO:  eta: 0:27:58  iter: 4739  total_loss: 19.01  loss_ce: 0.4346  loss_mask: 0.242  loss_dice: 0.9553  loss_contrastive: 0  loss_ce_0: 0.9857  loss_mask_0: 0.2959  loss_dice_0: 1.184  loss_ce_1: 0.6706  loss_mask_1: 0.2399  loss_dice_1: 0.933  loss_ce_2: 0.4963  loss_mask_2: 0.2926  loss_dice_2: 0.9252  loss_ce_3: 0.5041  loss_mask_3: 0.2937  loss_dice_3: 0.9458  loss_ce_4: 0.4287  loss_mask_4: 0.2432  loss_dice_4: 0.8802  loss_ce_5: 0.3854  loss_mask_5: 0.1995  loss_dice_5: 0.9315  loss_ce_6: 0.3839  loss_mask_6: 0.2126  loss_dice_6: 0.9844  loss_ce_7: 0.4266  loss_mask_7: 0.2591  loss_dice_7: 0.9312  loss_ce_8: 0.4661  loss_mask_8: 0.2191  loss_dice_8: 1.016    time: 0.3219  last_time: 0.3235  data_time: 0.0036  last_data_time: 0.0038   lr: 5.61e-05  max_mem: 14757M
[12/05 00:07:43] d2.utils.events INFO:  eta: 0:27:51  iter: 4759  total_loss: 17.8  loss_ce: 0.3973  loss_mask: 0.3208  loss_dice: 0.8931  loss_contrastive: 0  loss_ce_0: 1.156  loss_mask_0: 0.3903  loss_dice_0: 0.9703  loss_ce_1: 0.7928  loss_mask_1: 0.3683  loss_dice_1: 0.836  loss_ce_2: 0.6267  loss_mask_2: 0.3424  loss_dice_2: 0.6574  loss_ce_3: 0.4535  loss_mask_3: 0.3185  loss_dice_3: 0.8653  loss_ce_4: 0.4417  loss_mask_4: 0.2999  loss_dice_4: 0.7755  loss_ce_5: 0.4171  loss_mask_5: 0.3652  loss_dice_5: 0.7238  loss_ce_6: 0.392  loss_mask_6: 0.3369  loss_dice_6: 0.7425  loss_ce_7: 0.5233  loss_mask_7: 0.3272  loss_dice_7: 0.8321  loss_ce_8: 0.4668  loss_mask_8: 0.3355  loss_dice_8: 0.6657    time: 0.3219  last_time: 0.3145  data_time: 0.0033  last_data_time: 0.0062   lr: 5.5908e-05  max_mem: 14757M
[12/05 00:07:49] d2.utils.events INFO:  eta: 0:27:45  iter: 4779  total_loss: 19.74  loss_ce: 0.4416  loss_mask: 0.2733  loss_dice: 0.9172  loss_contrastive: 0  loss_ce_0: 1.202  loss_mask_0: 0.3779  loss_dice_0: 1.337  loss_ce_1: 0.8307  loss_mask_1: 0.3524  loss_dice_1: 1.145  loss_ce_2: 0.7136  loss_mask_2: 0.294  loss_dice_2: 1.035  loss_ce_3: 0.4853  loss_mask_3: 0.2432  loss_dice_3: 1.019  loss_ce_4: 0.5268  loss_mask_4: 0.2706  loss_dice_4: 0.9192  loss_ce_5: 0.454  loss_mask_5: 0.2425  loss_dice_5: 1.013  loss_ce_6: 0.4926  loss_mask_6: 0.2849  loss_dice_6: 0.9289  loss_ce_7: 0.4775  loss_mask_7: 0.2774  loss_dice_7: 0.8939  loss_ce_8: 0.467  loss_mask_8: 0.256  loss_dice_8: 0.9243    time: 0.3219  last_time: 0.3149  data_time: 0.0035  last_data_time: 0.0056   lr: 5.5716e-05  max_mem: 14757M
[12/05 00:07:55] d2.utils.events INFO:  eta: 0:27:39  iter: 4799  total_loss: 21.05  loss_ce: 0.3112  loss_mask: 0.3906  loss_dice: 0.8929  loss_contrastive: 0  loss_ce_0: 1.218  loss_mask_0: 0.3671  loss_dice_0: 1.141  loss_ce_1: 0.8463  loss_mask_1: 0.3372  loss_dice_1: 0.8944  loss_ce_2: 0.6222  loss_mask_2: 0.3502  loss_dice_2: 0.9994  loss_ce_3: 0.4451  loss_mask_3: 0.3348  loss_dice_3: 0.8522  loss_ce_4: 0.3818  loss_mask_4: 0.317  loss_dice_4: 0.9413  loss_ce_5: 0.3035  loss_mask_5: 0.3507  loss_dice_5: 0.8826  loss_ce_6: 0.3398  loss_mask_6: 0.3398  loss_dice_6: 0.8212  loss_ce_7: 0.2979  loss_mask_7: 0.3535  loss_dice_7: 0.9048  loss_ce_8: 0.3272  loss_mask_8: 0.371  loss_dice_8: 0.8803    time: 0.3219  last_time: 0.3012  data_time: 0.0034  last_data_time: 0.0034   lr: 5.5524e-05  max_mem: 14757M
[12/05 00:08:02] d2.utils.events INFO:  eta: 0:27:32  iter: 4819  total_loss: 18.71  loss_ce: 0.3723  loss_mask: 0.3534  loss_dice: 0.862  loss_contrastive: 0  loss_ce_0: 1.074  loss_mask_0: 0.3368  loss_dice_0: 1.113  loss_ce_1: 0.606  loss_mask_1: 0.3357  loss_dice_1: 0.9898  loss_ce_2: 0.5229  loss_mask_2: 0.3283  loss_dice_2: 0.9477  loss_ce_3: 0.4526  loss_mask_3: 0.3256  loss_dice_3: 0.9066  loss_ce_4: 0.3956  loss_mask_4: 0.3156  loss_dice_4: 0.9331  loss_ce_5: 0.4024  loss_mask_5: 0.3256  loss_dice_5: 0.9116  loss_ce_6: 0.3249  loss_mask_6: 0.3306  loss_dice_6: 0.8701  loss_ce_7: 0.3567  loss_mask_7: 0.3259  loss_dice_7: 0.8636  loss_ce_8: 0.344  loss_mask_8: 0.3155  loss_dice_8: 0.8473    time: 0.3219  last_time: 0.3087  data_time: 0.0034  last_data_time: 0.0029   lr: 5.5331e-05  max_mem: 14757M
[12/05 00:08:09] d2.utils.events INFO:  eta: 0:27:28  iter: 4839  total_loss: 27.23  loss_ce: 0.4249  loss_mask: 0.4435  loss_dice: 1.167  loss_contrastive: 0  loss_ce_0: 2.173  loss_mask_0: 0.5712  loss_dice_0: 1.64  loss_ce_1: 1.089  loss_mask_1: 0.4585  loss_dice_1: 1.367  loss_ce_2: 0.7935  loss_mask_2: 0.4368  loss_dice_2: 1.17  loss_ce_3: 0.5992  loss_mask_3: 0.4434  loss_dice_3: 1.047  loss_ce_4: 0.5706  loss_mask_4: 0.4373  loss_dice_4: 1.184  loss_ce_5: 0.4533  loss_mask_5: 0.4316  loss_dice_5: 1.122  loss_ce_6: 0.4858  loss_mask_6: 0.4304  loss_dice_6: 1.114  loss_ce_7: 0.482  loss_mask_7: 0.4496  loss_dice_7: 1.1  loss_ce_8: 0.402  loss_mask_8: 0.4563  loss_dice_8: 1.16    time: 0.3219  last_time: 0.3301  data_time: 0.0038  last_data_time: 0.0038   lr: 5.5139e-05  max_mem: 14757M
[12/05 00:08:15] d2.utils.events INFO:  eta: 0:27:21  iter: 4859  total_loss: 20.13  loss_ce: 0.3959  loss_mask: 0.2232  loss_dice: 1.066  loss_contrastive: 0  loss_ce_0: 1.259  loss_mask_0: 0.3344  loss_dice_0: 1.309  loss_ce_1: 0.6974  loss_mask_1: 0.2633  loss_dice_1: 1.112  loss_ce_2: 0.6619  loss_mask_2: 0.2639  loss_dice_2: 1.037  loss_ce_3: 0.5192  loss_mask_3: 0.243  loss_dice_3: 1.059  loss_ce_4: 0.3756  loss_mask_4: 0.2213  loss_dice_4: 1.132  loss_ce_5: 0.4769  loss_mask_5: 0.2126  loss_dice_5: 1.147  loss_ce_6: 0.4468  loss_mask_6: 0.2074  loss_dice_6: 1.049  loss_ce_7: 0.4236  loss_mask_7: 0.2159  loss_dice_7: 1.185  loss_ce_8: 0.3982  loss_mask_8: 0.2177  loss_dice_8: 1.133    time: 0.3219  last_time: 0.3274  data_time: 0.0037  last_data_time: 0.0026   lr: 5.4947e-05  max_mem: 14757M
[12/05 00:08:22] d2.utils.events INFO:  eta: 0:27:14  iter: 4879  total_loss: 19.85  loss_ce: 0.2523  loss_mask: 0.316  loss_dice: 0.7922  loss_contrastive: 0  loss_ce_0: 1.286  loss_mask_0: 0.3636  loss_dice_0: 1.015  loss_ce_1: 0.6647  loss_mask_1: 0.3089  loss_dice_1: 0.9434  loss_ce_2: 0.5115  loss_mask_2: 0.346  loss_dice_2: 0.8574  loss_ce_3: 0.3671  loss_mask_3: 0.358  loss_dice_3: 0.8465  loss_ce_4: 0.283  loss_mask_4: 0.3146  loss_dice_4: 0.7388  loss_ce_5: 0.2854  loss_mask_5: 0.3185  loss_dice_5: 0.869  loss_ce_6: 0.3282  loss_mask_6: 0.324  loss_dice_6: 0.7632  loss_ce_7: 0.241  loss_mask_7: 0.3097  loss_dice_7: 0.8505  loss_ce_8: 0.3536  loss_mask_8: 0.3347  loss_dice_8: 0.8406    time: 0.3219  last_time: 0.3089  data_time: 0.0034  last_data_time: 0.0036   lr: 5.4754e-05  max_mem: 14757M
[12/05 00:08:28] d2.utils.events INFO:  eta: 0:27:08  iter: 4899  total_loss: 16.46  loss_ce: 0.2168  loss_mask: 0.4193  loss_dice: 0.9613  loss_contrastive: 0  loss_ce_0: 1.136  loss_mask_0: 0.4002  loss_dice_0: 1.213  loss_ce_1: 0.5699  loss_mask_1: 0.3955  loss_dice_1: 1.068  loss_ce_2: 0.3795  loss_mask_2: 0.3584  loss_dice_2: 0.9444  loss_ce_3: 0.2587  loss_mask_3: 0.3596  loss_dice_3: 0.8786  loss_ce_4: 0.2994  loss_mask_4: 0.367  loss_dice_4: 0.9283  loss_ce_5: 0.2489  loss_mask_5: 0.3565  loss_dice_5: 0.9741  loss_ce_6: 0.2603  loss_mask_6: 0.3557  loss_dice_6: 0.9166  loss_ce_7: 0.2231  loss_mask_7: 0.3908  loss_dice_7: 0.9381  loss_ce_8: 0.2124  loss_mask_8: 0.4009  loss_dice_8: 1.019    time: 0.3219  last_time: 0.3243  data_time: 0.0035  last_data_time: 0.0054   lr: 5.4562e-05  max_mem: 14757M
[12/05 00:08:35] d2.utils.events INFO:  eta: 0:27:02  iter: 4919  total_loss: 22.28  loss_ce: 0.4693  loss_mask: 0.2774  loss_dice: 1.003  loss_contrastive: 0  loss_ce_0: 1.599  loss_mask_0: 0.3614  loss_dice_0: 1.355  loss_ce_1: 0.8476  loss_mask_1: 0.2841  loss_dice_1: 1.084  loss_ce_2: 0.6232  loss_mask_2: 0.3019  loss_dice_2: 1.069  loss_ce_3: 0.4646  loss_mask_3: 0.2517  loss_dice_3: 1.06  loss_ce_4: 0.4093  loss_mask_4: 0.2453  loss_dice_4: 1.08  loss_ce_5: 0.4709  loss_mask_5: 0.2687  loss_dice_5: 0.9845  loss_ce_6: 0.4502  loss_mask_6: 0.2712  loss_dice_6: 0.9658  loss_ce_7: 0.4165  loss_mask_7: 0.267  loss_dice_7: 0.9349  loss_ce_8: 0.3638  loss_mask_8: 0.2675  loss_dice_8: 1.031    time: 0.3220  last_time: 0.3331  data_time: 0.0036  last_data_time: 0.0040   lr: 5.4369e-05  max_mem: 14757M
[12/05 00:08:41] d2.utils.events INFO:  eta: 0:26:57  iter: 4939  total_loss: 17.84  loss_ce: 0.1758  loss_mask: 0.3234  loss_dice: 0.9919  loss_contrastive: 0  loss_ce_0: 0.9079  loss_mask_0: 0.3599  loss_dice_0: 1.057  loss_ce_1: 0.4868  loss_mask_1: 0.3242  loss_dice_1: 1.071  loss_ce_2: 0.2859  loss_mask_2: 0.3284  loss_dice_2: 0.9866  loss_ce_3: 0.3418  loss_mask_3: 0.3307  loss_dice_3: 1.013  loss_ce_4: 0.238  loss_mask_4: 0.3136  loss_dice_4: 1.042  loss_ce_5: 0.204  loss_mask_5: 0.3321  loss_dice_5: 1.008  loss_ce_6: 0.1771  loss_mask_6: 0.3289  loss_dice_6: 0.9714  loss_ce_7: 0.1573  loss_mask_7: 0.3095  loss_dice_7: 0.9928  loss_ce_8: 0.1871  loss_mask_8: 0.3062  loss_dice_8: 1.07    time: 0.3220  last_time: 0.3348  data_time: 0.0033  last_data_time: 0.0030   lr: 5.4177e-05  max_mem: 14757M
[12/05 00:08:48] d2.utils.events INFO:  eta: 0:26:51  iter: 4959  total_loss: 18.46  loss_ce: 0.3304  loss_mask: 0.4624  loss_dice: 0.7768  loss_contrastive: 0  loss_ce_0: 0.7113  loss_mask_0: 0.4994  loss_dice_0: 0.9489  loss_ce_1: 0.4608  loss_mask_1: 0.4867  loss_dice_1: 0.9773  loss_ce_2: 0.4319  loss_mask_2: 0.4684  loss_dice_2: 0.8278  loss_ce_3: 0.3397  loss_mask_3: 0.4931  loss_dice_3: 0.8801  loss_ce_4: 0.3958  loss_mask_4: 0.4527  loss_dice_4: 0.8286  loss_ce_5: 0.2717  loss_mask_5: 0.4647  loss_dice_5: 0.8681  loss_ce_6: 0.264  loss_mask_6: 0.4649  loss_dice_6: 0.8548  loss_ce_7: 0.2762  loss_mask_7: 0.451  loss_dice_7: 0.8426  loss_ce_8: 0.2369  loss_mask_8: 0.4582  loss_dice_8: 0.8238    time: 0.3220  last_time: 0.3306  data_time: 0.0032  last_data_time: 0.0031   lr: 5.3984e-05  max_mem: 14757M
[12/05 00:08:54] d2.utils.events INFO:  eta: 0:26:45  iter: 4979  total_loss: 14.15  loss_ce: 0.1329  loss_mask: 0.1494  loss_dice: 0.6874  loss_contrastive: 0  loss_ce_0: 1.11  loss_mask_0: 0.2907  loss_dice_0: 1.13  loss_ce_1: 0.5793  loss_mask_1: 0.1926  loss_dice_1: 1.141  loss_ce_2: 0.3487  loss_mask_2: 0.2112  loss_dice_2: 0.7925  loss_ce_3: 0.1996  loss_mask_3: 0.1482  loss_dice_3: 0.8318  loss_ce_4: 0.1615  loss_mask_4: 0.1418  loss_dice_4: 0.7882  loss_ce_5: 0.167  loss_mask_5: 0.145  loss_dice_5: 0.7199  loss_ce_6: 0.1541  loss_mask_6: 0.1453  loss_dice_6: 0.6998  loss_ce_7: 0.1297  loss_mask_7: 0.1387  loss_dice_7: 0.6848  loss_ce_8: 0.1439  loss_mask_8: 0.1501  loss_dice_8: 0.7697    time: 0.3220  last_time: 0.3343  data_time: 0.0035  last_data_time: 0.0026   lr: 5.3791e-05  max_mem: 14757M
[12/05 00:09:01] fvcore.common.checkpoint INFO: Saving checkpoint to outputs/ceymo2/model_0004999.pth
[12/05 00:09:06] oneformer.data.dataset_mappers.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1024, 1024), max_size=2048, sample_style='choice')]
[12/05 00:09:06] oneformer.data.datasets.register_cityscapes_panoptic INFO: 1 cities found in '/mnt/source/datasets/cityscapes/leftImg8bit/val'.
[12/05 00:09:06] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[12/05 00:09:06] d2.data.common INFO: Serializing 267 elements to byte tensors and concatenating them all ...
[12/05 00:09:06] d2.data.common INFO: Serialized dataset takes 0.44 MiB
[12/05 00:09:06] d2.evaluation.evaluator INFO: Start inference on 267 batches
[12/05 00:09:06] d2.evaluation.cityscapes_evaluation INFO: Writing cityscapes results to temporary directory /tmp/cityscapes_eval_axe47umm ...
[12/05 00:09:06] oneformer.evaluation.cityscapes_evaluation INFO: Writing cityscapes results to temporary directory /tmp/cityscapes_eval_a0cl4gjb ...
[12/05 00:09:51] d2.evaluation.evaluator INFO: Inference done 11/267. Dataloading: 0.0021 s/iter. Inference: 0.4055 s/iter. Eval: 3.8043 s/iter. Total: 4.2118 s/iter. ETA=0:17:58
[12/05 00:10:00] d2.evaluation.evaluator INFO: Inference done 13/267. Dataloading: 0.0023 s/iter. Inference: 0.4077 s/iter. Eval: 3.7949 s/iter. Total: 4.2049 s/iter. ETA=0:17:48
[12/05 00:10:08] d2.evaluation.evaluator INFO: Inference done 15/267. Dataloading: 0.0023 s/iter. Inference: 0.4049 s/iter. Eval: 3.8024 s/iter. Total: 4.2097 s/iter. ETA=0:17:40
[12/05 00:10:13] d2.evaluation.evaluator INFO: Inference done 16/267. Dataloading: 0.0024 s/iter. Inference: 0.4112 s/iter. Eval: 3.8693 s/iter. Total: 4.2829 s/iter. ETA=0:17:55
[12/05 00:10:21] d2.evaluation.evaluator INFO: Inference done 18/267. Dataloading: 0.0024 s/iter. Inference: 0.4085 s/iter. Eval: 3.8526 s/iter. Total: 4.2636 s/iter. ETA=0:17:41
[12/05 00:10:29] d2.evaluation.evaluator INFO: Inference done 20/267. Dataloading: 0.0025 s/iter. Inference: 0.4066 s/iter. Eval: 3.8157 s/iter. Total: 4.2248 s/iter. ETA=0:17:23
[12/05 00:10:37] d2.evaluation.evaluator INFO: Inference done 22/267. Dataloading: 0.0025 s/iter. Inference: 0.4045 s/iter. Eval: 3.7502 s/iter. Total: 4.1573 s/iter. ETA=0:16:58
[12/05 00:10:45] d2.evaluation.evaluator INFO: Inference done 24/267. Dataloading: 0.0026 s/iter. Inference: 0.4033 s/iter. Eval: 3.7299 s/iter. Total: 4.1358 s/iter. ETA=0:16:44
[12/05 00:10:52] d2.evaluation.evaluator INFO: Inference done 26/267. Dataloading: 0.0026 s/iter. Inference: 0.4018 s/iter. Eval: 3.6969 s/iter. Total: 4.1014 s/iter. ETA=0:16:28
[12/05 00:11:01] d2.evaluation.evaluator INFO: Inference done 28/267. Dataloading: 0.0026 s/iter. Inference: 0.4009 s/iter. Eval: 3.7388 s/iter. Total: 4.1425 s/iter. ETA=0:16:30
[12/05 00:11:10] d2.evaluation.evaluator INFO: Inference done 30/267. Dataloading: 0.0026 s/iter. Inference: 0.4000 s/iter. Eval: 3.7594 s/iter. Total: 4.1622 s/iter. ETA=0:16:26
[12/05 00:11:18] d2.evaluation.evaluator INFO: Inference done 32/267. Dataloading: 0.0028 s/iter. Inference: 0.3998 s/iter. Eval: 3.7665 s/iter. Total: 4.1691 s/iter. ETA=0:16:19
[12/05 00:11:28] d2.evaluation.evaluator INFO: Inference done 34/267. Dataloading: 0.0028 s/iter. Inference: 0.3993 s/iter. Eval: 3.7938 s/iter. Total: 4.1960 s/iter. ETA=0:16:17
[12/05 00:11:35] d2.evaluation.evaluator INFO: Inference done 36/267. Dataloading: 0.0028 s/iter. Inference: 0.3990 s/iter. Eval: 3.7746 s/iter. Total: 4.1764 s/iter. ETA=0:16:04
[12/05 00:11:44] d2.evaluation.evaluator INFO: Inference done 38/267. Dataloading: 0.0028 s/iter. Inference: 0.3989 s/iter. Eval: 3.7762 s/iter. Total: 4.1779 s/iter. ETA=0:15:56
[12/05 00:11:52] d2.evaluation.evaluator INFO: Inference done 40/267. Dataloading: 0.0028 s/iter. Inference: 0.3986 s/iter. Eval: 3.7633 s/iter. Total: 4.1648 s/iter. ETA=0:15:45
[12/05 00:12:01] d2.evaluation.evaluator INFO: Inference done 42/267. Dataloading: 0.0028 s/iter. Inference: 0.3983 s/iter. Eval: 3.7782 s/iter. Total: 4.1794 s/iter. ETA=0:15:40
[12/05 00:12:09] d2.evaluation.evaluator INFO: Inference done 44/267. Dataloading: 0.0028 s/iter. Inference: 0.3979 s/iter. Eval: 3.7926 s/iter. Total: 4.1934 s/iter. ETA=0:15:35
[12/05 00:12:18] d2.evaluation.evaluator INFO: Inference done 46/267. Dataloading: 0.0029 s/iter. Inference: 0.4031 s/iter. Eval: 3.8029 s/iter. Total: 4.2090 s/iter. ETA=0:15:30
[12/05 00:12:27] d2.evaluation.evaluator INFO: Inference done 48/267. Dataloading: 0.0029 s/iter. Inference: 0.4025 s/iter. Eval: 3.7965 s/iter. Total: 4.2020 s/iter. ETA=0:15:20
[12/05 00:12:35] d2.evaluation.evaluator INFO: Inference done 50/267. Dataloading: 0.0030 s/iter. Inference: 0.4020 s/iter. Eval: 3.7958 s/iter. Total: 4.2009 s/iter. ETA=0:15:11
[12/05 00:12:42] d2.evaluation.evaluator INFO: Inference done 52/267. Dataloading: 0.0030 s/iter. Inference: 0.4015 s/iter. Eval: 3.7767 s/iter. Total: 4.1813 s/iter. ETA=0:14:58
[12/05 00:12:51] d2.evaluation.evaluator INFO: Inference done 54/267. Dataloading: 0.0030 s/iter. Inference: 0.4008 s/iter. Eval: 3.7888 s/iter. Total: 4.1927 s/iter. ETA=0:14:53
[12/05 00:13:00] d2.evaluation.evaluator INFO: Inference done 56/267. Dataloading: 0.0030 s/iter. Inference: 0.4005 s/iter. Eval: 3.7944 s/iter. Total: 4.1980 s/iter. ETA=0:14:45
[12/05 00:13:08] d2.evaluation.evaluator INFO: Inference done 58/267. Dataloading: 0.0030 s/iter. Inference: 0.4002 s/iter. Eval: 3.7864 s/iter. Total: 4.1897 s/iter. ETA=0:14:35
[12/05 00:13:17] d2.evaluation.evaluator INFO: Inference done 60/267. Dataloading: 0.0030 s/iter. Inference: 0.3998 s/iter. Eval: 3.7985 s/iter. Total: 4.2014 s/iter. ETA=0:14:29
[12/05 00:13:26] d2.evaluation.evaluator INFO: Inference done 62/267. Dataloading: 0.0030 s/iter. Inference: 0.3995 s/iter. Eval: 3.8039 s/iter. Total: 4.2066 s/iter. ETA=0:14:22
[12/05 00:13:34] d2.evaluation.evaluator INFO: Inference done 64/267. Dataloading: 0.0030 s/iter. Inference: 0.3991 s/iter. Eval: 3.8020 s/iter. Total: 4.2042 s/iter. ETA=0:14:13
[12/05 00:13:42] d2.evaluation.evaluator INFO: Inference done 66/267. Dataloading: 0.0030 s/iter. Inference: 0.3989 s/iter. Eval: 3.7954 s/iter. Total: 4.1974 s/iter. ETA=0:14:03
[12/05 00:13:50] d2.evaluation.evaluator INFO: Inference done 68/267. Dataloading: 0.0030 s/iter. Inference: 0.3987 s/iter. Eval: 3.7958 s/iter. Total: 4.1976 s/iter. ETA=0:13:55
[12/05 00:14:00] d2.evaluation.evaluator INFO: Inference done 70/267. Dataloading: 0.0030 s/iter. Inference: 0.3992 s/iter. Eval: 3.8084 s/iter. Total: 4.2108 s/iter. ETA=0:13:49
[12/05 00:14:09] d2.evaluation.evaluator INFO: Inference done 72/267. Dataloading: 0.0030 s/iter. Inference: 0.3990 s/iter. Eval: 3.8203 s/iter. Total: 4.2224 s/iter. ETA=0:13:43
[12/05 00:14:18] d2.evaluation.evaluator INFO: Inference done 74/267. Dataloading: 0.0030 s/iter. Inference: 0.3986 s/iter. Eval: 3.8283 s/iter. Total: 4.2301 s/iter. ETA=0:13:36
[12/05 00:14:26] d2.evaluation.evaluator INFO: Inference done 76/267. Dataloading: 0.0030 s/iter. Inference: 0.3982 s/iter. Eval: 3.8279 s/iter. Total: 4.2293 s/iter. ETA=0:13:27
[12/05 00:14:35] d2.evaluation.evaluator INFO: Inference done 78/267. Dataloading: 0.0030 s/iter. Inference: 0.3978 s/iter. Eval: 3.8280 s/iter. Total: 4.2290 s/iter. ETA=0:13:19
[12/05 00:14:44] d2.evaluation.evaluator INFO: Inference done 80/267. Dataloading: 0.0030 s/iter. Inference: 0.3974 s/iter. Eval: 3.8347 s/iter. Total: 4.2353 s/iter. ETA=0:13:11
[12/05 00:14:53] d2.evaluation.evaluator INFO: Inference done 82/267. Dataloading: 0.0030 s/iter. Inference: 0.3971 s/iter. Eval: 3.8460 s/iter. Total: 4.2463 s/iter. ETA=0:13:05
[12/05 00:15:02] d2.evaluation.evaluator INFO: Inference done 84/267. Dataloading: 0.0030 s/iter. Inference: 0.3968 s/iter. Eval: 3.8562 s/iter. Total: 4.2563 s/iter. ETA=0:12:58
[12/05 00:15:11] d2.evaluation.evaluator INFO: Inference done 86/267. Dataloading: 0.0031 s/iter. Inference: 0.3966 s/iter. Eval: 3.8607 s/iter. Total: 4.2604 s/iter. ETA=0:12:51
[12/05 00:15:20] d2.evaluation.evaluator INFO: Inference done 88/267. Dataloading: 0.0031 s/iter. Inference: 0.3964 s/iter. Eval: 3.8724 s/iter. Total: 4.2720 s/iter. ETA=0:12:44
[12/05 00:15:30] d2.evaluation.evaluator INFO: Inference done 90/267. Dataloading: 0.0031 s/iter. Inference: 0.3961 s/iter. Eval: 3.8841 s/iter. Total: 4.2834 s/iter. ETA=0:12:38
[12/05 00:15:39] d2.evaluation.evaluator INFO: Inference done 92/267. Dataloading: 0.0031 s/iter. Inference: 0.3959 s/iter. Eval: 3.8912 s/iter. Total: 4.2902 s/iter. ETA=0:12:30
[12/05 00:15:48] d2.evaluation.evaluator INFO: Inference done 94/267. Dataloading: 0.0031 s/iter. Inference: 0.3956 s/iter. Eval: 3.8990 s/iter. Total: 4.2978 s/iter. ETA=0:12:23
[12/05 00:15:56] d2.evaluation.evaluator INFO: Inference done 96/267. Dataloading: 0.0031 s/iter. Inference: 0.3954 s/iter. Eval: 3.8922 s/iter. Total: 4.2909 s/iter. ETA=0:12:13
[12/05 00:16:05] d2.evaluation.evaluator INFO: Inference done 98/267. Dataloading: 0.0031 s/iter. Inference: 0.3953 s/iter. Eval: 3.8943 s/iter. Total: 4.2927 s/iter. ETA=0:12:05
[12/05 00:16:14] d2.evaluation.evaluator INFO: Inference done 100/267. Dataloading: 0.0031 s/iter. Inference: 0.3952 s/iter. Eval: 3.8962 s/iter. Total: 4.2945 s/iter. ETA=0:11:57
[12/05 00:16:23] d2.evaluation.evaluator INFO: Inference done 102/267. Dataloading: 0.0031 s/iter. Inference: 0.3950 s/iter. Eval: 3.8982 s/iter. Total: 4.2965 s/iter. ETA=0:11:48
[12/05 00:16:31] d2.evaluation.evaluator INFO: Inference done 104/267. Dataloading: 0.0031 s/iter. Inference: 0.3950 s/iter. Eval: 3.8990 s/iter. Total: 4.2972 s/iter. ETA=0:11:40
[12/05 00:16:37] d2.evaluation.evaluator INFO: Inference done 105/267. Dataloading: 0.0031 s/iter. Inference: 0.3950 s/iter. Eval: 3.9082 s/iter. Total: 4.3065 s/iter. ETA=0:11:37
[12/05 00:16:45] d2.evaluation.evaluator INFO: Inference done 107/267. Dataloading: 0.0031 s/iter. Inference: 0.3950 s/iter. Eval: 3.9108 s/iter. Total: 4.3090 s/iter. ETA=0:11:29
[12/05 00:16:54] d2.evaluation.evaluator INFO: Inference done 109/267. Dataloading: 0.0031 s/iter. Inference: 0.3949 s/iter. Eval: 3.9090 s/iter. Total: 4.3072 s/iter. ETA=0:11:20
[12/05 00:17:02] d2.evaluation.evaluator INFO: Inference done 111/267. Dataloading: 0.0031 s/iter. Inference: 0.3949 s/iter. Eval: 3.9080 s/iter. Total: 4.3061 s/iter. ETA=0:11:11
[12/05 00:17:10] d2.evaluation.evaluator INFO: Inference done 113/267. Dataloading: 0.0031 s/iter. Inference: 0.3948 s/iter. Eval: 3.8968 s/iter. Total: 4.2949 s/iter. ETA=0:11:01
[12/05 00:17:18] d2.evaluation.evaluator INFO: Inference done 115/267. Dataloading: 0.0031 s/iter. Inference: 0.3948 s/iter. Eval: 3.8964 s/iter. Total: 4.2945 s/iter. ETA=0:10:52
[12/05 00:17:26] d2.evaluation.evaluator INFO: Inference done 117/267. Dataloading: 0.0031 s/iter. Inference: 0.3948 s/iter. Eval: 3.8928 s/iter. Total: 4.2908 s/iter. ETA=0:10:43
[12/05 00:17:34] d2.evaluation.evaluator INFO: Inference done 119/267. Dataloading: 0.0031 s/iter. Inference: 0.3947 s/iter. Eval: 3.8846 s/iter. Total: 4.2826 s/iter. ETA=0:10:33
[12/05 00:17:42] d2.evaluation.evaluator INFO: Inference done 121/267. Dataloading: 0.0031 s/iter. Inference: 0.3946 s/iter. Eval: 3.8754 s/iter. Total: 4.2733 s/iter. ETA=0:10:23
[12/05 00:17:50] d2.evaluation.evaluator INFO: Inference done 123/267. Dataloading: 0.0031 s/iter. Inference: 0.3946 s/iter. Eval: 3.8729 s/iter. Total: 4.2708 s/iter. ETA=0:10:14
[12/05 00:17:59] d2.evaluation.evaluator INFO: Inference done 125/267. Dataloading: 0.0031 s/iter. Inference: 0.3946 s/iter. Eval: 3.8800 s/iter. Total: 4.2778 s/iter. ETA=0:10:07
[12/05 00:18:08] d2.evaluation.evaluator INFO: Inference done 127/267. Dataloading: 0.0031 s/iter. Inference: 0.3945 s/iter. Eval: 3.8791 s/iter. Total: 4.2769 s/iter. ETA=0:09:58
[12/05 00:18:17] d2.evaluation.evaluator INFO: Inference done 129/267. Dataloading: 0.0031 s/iter. Inference: 0.3946 s/iter. Eval: 3.8836 s/iter. Total: 4.2815 s/iter. ETA=0:09:50
[12/05 00:18:26] d2.evaluation.evaluator INFO: Inference done 131/267. Dataloading: 0.0031 s/iter. Inference: 0.3945 s/iter. Eval: 3.8858 s/iter. Total: 4.2836 s/iter. ETA=0:09:42
[12/05 00:18:34] d2.evaluation.evaluator INFO: Inference done 133/267. Dataloading: 0.0031 s/iter. Inference: 0.3943 s/iter. Eval: 3.8868 s/iter. Total: 4.2844 s/iter. ETA=0:09:34
[12/05 00:18:43] d2.evaluation.evaluator INFO: Inference done 135/267. Dataloading: 0.0031 s/iter. Inference: 0.3942 s/iter. Eval: 3.8882 s/iter. Total: 4.2856 s/iter. ETA=0:09:25
[12/05 00:18:52] d2.evaluation.evaluator INFO: Inference done 137/267. Dataloading: 0.0031 s/iter. Inference: 0.3941 s/iter. Eval: 3.8894 s/iter. Total: 4.2868 s/iter. ETA=0:09:17
[12/05 00:19:00] d2.evaluation.evaluator INFO: Inference done 139/267. Dataloading: 0.0031 s/iter. Inference: 0.3940 s/iter. Eval: 3.8861 s/iter. Total: 4.2833 s/iter. ETA=0:09:08
[12/05 00:19:08] d2.evaluation.evaluator INFO: Inference done 141/267. Dataloading: 0.0031 s/iter. Inference: 0.3940 s/iter. Eval: 3.8845 s/iter. Total: 4.2817 s/iter. ETA=0:08:59
[12/05 00:19:13] d2.evaluation.evaluator INFO: Inference done 142/267. Dataloading: 0.0031 s/iter. Inference: 0.3940 s/iter. Eval: 3.8910 s/iter. Total: 4.2882 s/iter. ETA=0:08:56
[12/05 00:19:22] d2.evaluation.evaluator INFO: Inference done 144/267. Dataloading: 0.0031 s/iter. Inference: 0.3939 s/iter. Eval: 3.8931 s/iter. Total: 4.2903 s/iter. ETA=0:08:47
[12/05 00:19:30] d2.evaluation.evaluator INFO: Inference done 146/267. Dataloading: 0.0031 s/iter. Inference: 0.3940 s/iter. Eval: 3.8875 s/iter. Total: 4.2847 s/iter. ETA=0:08:38
[12/05 00:19:38] d2.evaluation.evaluator INFO: Inference done 148/267. Dataloading: 0.0031 s/iter. Inference: 0.3940 s/iter. Eval: 3.8839 s/iter. Total: 4.2811 s/iter. ETA=0:08:29
[12/05 00:19:47] d2.evaluation.evaluator INFO: Inference done 150/267. Dataloading: 0.0031 s/iter. Inference: 0.3940 s/iter. Eval: 3.8862 s/iter. Total: 4.2835 s/iter. ETA=0:08:21
[12/05 00:19:55] d2.evaluation.evaluator INFO: Inference done 152/267. Dataloading: 0.0032 s/iter. Inference: 0.3940 s/iter. Eval: 3.8836 s/iter. Total: 4.2809 s/iter. ETA=0:08:12
[12/05 00:20:04] d2.evaluation.evaluator INFO: Inference done 154/267. Dataloading: 0.0032 s/iter. Inference: 0.3940 s/iter. Eval: 3.8869 s/iter. Total: 4.2843 s/iter. ETA=0:08:04
[12/05 00:20:13] d2.evaluation.evaluator INFO: Inference done 156/267. Dataloading: 0.0032 s/iter. Inference: 0.3941 s/iter. Eval: 3.8851 s/iter. Total: 4.2825 s/iter. ETA=0:07:55
[12/05 00:20:20] d2.evaluation.evaluator INFO: Inference done 158/267. Dataloading: 0.0032 s/iter. Inference: 0.3940 s/iter. Eval: 3.8775 s/iter. Total: 4.2749 s/iter. ETA=0:07:45
[12/05 00:20:28] d2.evaluation.evaluator INFO: Inference done 160/267. Dataloading: 0.0032 s/iter. Inference: 0.3940 s/iter. Eval: 3.8735 s/iter. Total: 4.2708 s/iter. ETA=0:07:36
[12/05 00:20:36] d2.evaluation.evaluator INFO: Inference done 162/267. Dataloading: 0.0032 s/iter. Inference: 0.3941 s/iter. Eval: 3.8738 s/iter. Total: 4.2712 s/iter. ETA=0:07:28
[12/05 00:20:44] d2.evaluation.evaluator INFO: Inference done 164/267. Dataloading: 0.0032 s/iter. Inference: 0.3940 s/iter. Eval: 3.8698 s/iter. Total: 4.2671 s/iter. ETA=0:07:19
[12/05 00:20:52] d2.evaluation.evaluator INFO: Inference done 166/267. Dataloading: 0.0032 s/iter. Inference: 0.3939 s/iter. Eval: 3.8664 s/iter. Total: 4.2637 s/iter. ETA=0:07:10
[12/05 00:21:00] d2.evaluation.evaluator INFO: Inference done 168/267. Dataloading: 0.0032 s/iter. Inference: 0.3938 s/iter. Eval: 3.8607 s/iter. Total: 4.2579 s/iter. ETA=0:07:01
[12/05 00:21:06] d2.evaluation.evaluator INFO: Inference done 170/267. Dataloading: 0.0032 s/iter. Inference: 0.3938 s/iter. Eval: 3.8485 s/iter. Total: 4.2457 s/iter. ETA=0:06:51
[12/05 00:21:15] d2.evaluation.evaluator INFO: Inference done 172/267. Dataloading: 0.0032 s/iter. Inference: 0.3938 s/iter. Eval: 3.8480 s/iter. Total: 4.2451 s/iter. ETA=0:06:43
[12/05 00:21:24] d2.evaluation.evaluator INFO: Inference done 174/267. Dataloading: 0.0032 s/iter. Inference: 0.3938 s/iter. Eval: 3.8512 s/iter. Total: 4.2483 s/iter. ETA=0:06:35
[12/05 00:21:32] d2.evaluation.evaluator INFO: Inference done 176/267. Dataloading: 0.0032 s/iter. Inference: 0.3937 s/iter. Eval: 3.8516 s/iter. Total: 4.2487 s/iter. ETA=0:06:26
[12/05 00:21:40] d2.evaluation.evaluator INFO: Inference done 178/267. Dataloading: 0.0032 s/iter. Inference: 0.3937 s/iter. Eval: 3.8471 s/iter. Total: 4.2441 s/iter. ETA=0:06:17
[12/05 00:21:48] d2.evaluation.evaluator INFO: Inference done 180/267. Dataloading: 0.0032 s/iter. Inference: 0.3937 s/iter. Eval: 3.8462 s/iter. Total: 4.2433 s/iter. ETA=0:06:09
[12/05 00:21:56] d2.evaluation.evaluator INFO: Inference done 182/267. Dataloading: 0.0032 s/iter. Inference: 0.3936 s/iter. Eval: 3.8435 s/iter. Total: 4.2405 s/iter. ETA=0:06:00
[12/05 00:22:05] d2.evaluation.evaluator INFO: Inference done 184/267. Dataloading: 0.0032 s/iter. Inference: 0.3935 s/iter. Eval: 3.8424 s/iter. Total: 4.2393 s/iter. ETA=0:05:51
[12/05 00:22:14] d2.evaluation.evaluator INFO: Inference done 186/267. Dataloading: 0.0032 s/iter. Inference: 0.3935 s/iter. Eval: 3.8456 s/iter. Total: 4.2425 s/iter. ETA=0:05:43
[12/05 00:22:23] d2.evaluation.evaluator INFO: Inference done 188/267. Dataloading: 0.0032 s/iter. Inference: 0.3935 s/iter. Eval: 3.8489 s/iter. Total: 4.2458 s/iter. ETA=0:05:35
[12/05 00:22:32] d2.evaluation.evaluator INFO: Inference done 190/267. Dataloading: 0.0032 s/iter. Inference: 0.3934 s/iter. Eval: 3.8497 s/iter. Total: 4.2465 s/iter. ETA=0:05:26
[12/05 00:22:40] d2.evaluation.evaluator INFO: Inference done 192/267. Dataloading: 0.0032 s/iter. Inference: 0.3933 s/iter. Eval: 3.8489 s/iter. Total: 4.2455 s/iter. ETA=0:05:18
[12/05 00:22:48] d2.evaluation.evaluator INFO: Inference done 194/267. Dataloading: 0.0032 s/iter. Inference: 0.3932 s/iter. Eval: 3.8452 s/iter. Total: 4.2418 s/iter. ETA=0:05:09
[12/05 00:22:56] d2.evaluation.evaluator INFO: Inference done 196/267. Dataloading: 0.0032 s/iter. Inference: 0.3931 s/iter. Eval: 3.8451 s/iter. Total: 4.2416 s/iter. ETA=0:05:01
[12/05 00:23:05] d2.evaluation.evaluator INFO: Inference done 198/267. Dataloading: 0.0032 s/iter. Inference: 0.3931 s/iter. Eval: 3.8452 s/iter. Total: 4.2416 s/iter. ETA=0:04:52
[12/05 00:23:14] d2.evaluation.evaluator INFO: Inference done 200/267. Dataloading: 0.0032 s/iter. Inference: 0.3929 s/iter. Eval: 3.8479 s/iter. Total: 4.2442 s/iter. ETA=0:04:44
[12/05 00:23:23] d2.evaluation.evaluator INFO: Inference done 202/267. Dataloading: 0.0032 s/iter. Inference: 0.3929 s/iter. Eval: 3.8508 s/iter. Total: 4.2470 s/iter. ETA=0:04:36
[12/05 00:23:31] d2.evaluation.evaluator INFO: Inference done 204/267. Dataloading: 0.0032 s/iter. Inference: 0.3928 s/iter. Eval: 3.8517 s/iter. Total: 4.2479 s/iter. ETA=0:04:27
[12/05 00:23:40] d2.evaluation.evaluator INFO: Inference done 206/267. Dataloading: 0.0032 s/iter. Inference: 0.3927 s/iter. Eval: 3.8528 s/iter. Total: 4.2490 s/iter. ETA=0:04:19
[12/05 00:23:48] d2.evaluation.evaluator INFO: Inference done 208/267. Dataloading: 0.0032 s/iter. Inference: 0.3927 s/iter. Eval: 3.8496 s/iter. Total: 4.2456 s/iter. ETA=0:04:10
[12/05 00:23:56] d2.evaluation.evaluator INFO: Inference done 210/267. Dataloading: 0.0032 s/iter. Inference: 0.3926 s/iter. Eval: 3.8489 s/iter. Total: 4.2449 s/iter. ETA=0:04:01
[12/05 00:24:04] d2.evaluation.evaluator INFO: Inference done 212/267. Dataloading: 0.0032 s/iter. Inference: 0.3926 s/iter. Eval: 3.8480 s/iter. Total: 4.2440 s/iter. ETA=0:03:53
[12/05 00:24:13] d2.evaluation.evaluator INFO: Inference done 214/267. Dataloading: 0.0032 s/iter. Inference: 0.3926 s/iter. Eval: 3.8471 s/iter. Total: 4.2431 s/iter. ETA=0:03:44
[12/05 00:24:22] d2.evaluation.evaluator INFO: Inference done 216/267. Dataloading: 0.0032 s/iter. Inference: 0.3927 s/iter. Eval: 3.8512 s/iter. Total: 4.2472 s/iter. ETA=0:03:36
[12/05 00:24:31] d2.evaluation.evaluator INFO: Inference done 218/267. Dataloading: 0.0032 s/iter. Inference: 0.3927 s/iter. Eval: 3.8551 s/iter. Total: 4.2512 s/iter. ETA=0:03:28
[12/05 00:24:40] d2.evaluation.evaluator INFO: Inference done 220/267. Dataloading: 0.0032 s/iter. Inference: 0.3927 s/iter. Eval: 3.8565 s/iter. Total: 4.2525 s/iter. ETA=0:03:19
[12/05 00:24:49] d2.evaluation.evaluator INFO: Inference done 222/267. Dataloading: 0.0032 s/iter. Inference: 0.3926 s/iter. Eval: 3.8562 s/iter. Total: 4.2522 s/iter. ETA=0:03:11
[12/05 00:24:57] d2.evaluation.evaluator INFO: Inference done 224/267. Dataloading: 0.0032 s/iter. Inference: 0.3926 s/iter. Eval: 3.8539 s/iter. Total: 4.2499 s/iter. ETA=0:03:02
[12/05 00:25:04] d2.evaluation.evaluator INFO: Inference done 226/267. Dataloading: 0.0032 s/iter. Inference: 0.3925 s/iter. Eval: 3.8508 s/iter. Total: 4.2467 s/iter. ETA=0:02:54
[12/05 00:25:12] d2.evaluation.evaluator INFO: Inference done 228/267. Dataloading: 0.0032 s/iter. Inference: 0.3925 s/iter. Eval: 3.8467 s/iter. Total: 4.2426 s/iter. ETA=0:02:45
[12/05 00:25:19] d2.evaluation.evaluator INFO: Inference done 230/267. Dataloading: 0.0032 s/iter. Inference: 0.3925 s/iter. Eval: 3.8418 s/iter. Total: 4.2377 s/iter. ETA=0:02:36
[12/05 00:25:27] d2.evaluation.evaluator INFO: Inference done 232/267. Dataloading: 0.0032 s/iter. Inference: 0.3925 s/iter. Eval: 3.8400 s/iter. Total: 4.2359 s/iter. ETA=0:02:28
[12/05 00:25:36] d2.evaluation.evaluator INFO: Inference done 234/267. Dataloading: 0.0032 s/iter. Inference: 0.3925 s/iter. Eval: 3.8411 s/iter. Total: 4.2369 s/iter. ETA=0:02:19
[12/05 00:25:45] d2.evaluation.evaluator INFO: Inference done 236/267. Dataloading: 0.0032 s/iter. Inference: 0.3925 s/iter. Eval: 3.8444 s/iter. Total: 4.2403 s/iter. ETA=0:02:11
[12/05 00:25:54] d2.evaluation.evaluator INFO: Inference done 238/267. Dataloading: 0.0032 s/iter. Inference: 0.3925 s/iter. Eval: 3.8447 s/iter. Total: 4.2405 s/iter. ETA=0:02:02
[12/05 00:26:02] d2.evaluation.evaluator INFO: Inference done 240/267. Dataloading: 0.0032 s/iter. Inference: 0.3924 s/iter. Eval: 3.8445 s/iter. Total: 4.2403 s/iter. ETA=0:01:54
[12/05 00:26:11] d2.evaluation.evaluator INFO: Inference done 242/267. Dataloading: 0.0032 s/iter. Inference: 0.3924 s/iter. Eval: 3.8452 s/iter. Total: 4.2409 s/iter. ETA=0:01:46
[12/05 00:26:19] d2.evaluation.evaluator INFO: Inference done 244/267. Dataloading: 0.0032 s/iter. Inference: 0.3924 s/iter. Eval: 3.8440 s/iter. Total: 4.2398 s/iter. ETA=0:01:37
[12/05 00:26:27] d2.evaluation.evaluator INFO: Inference done 246/267. Dataloading: 0.0032 s/iter. Inference: 0.3924 s/iter. Eval: 3.8412 s/iter. Total: 4.2369 s/iter. ETA=0:01:28
[12/05 00:26:34] d2.evaluation.evaluator INFO: Inference done 248/267. Dataloading: 0.0032 s/iter. Inference: 0.3924 s/iter. Eval: 3.8360 s/iter. Total: 4.2317 s/iter. ETA=0:01:20
[12/05 00:26:42] d2.evaluation.evaluator INFO: Inference done 250/267. Dataloading: 0.0032 s/iter. Inference: 0.3923 s/iter. Eval: 3.8348 s/iter. Total: 4.2305 s/iter. ETA=0:01:11
[12/05 00:26:51] d2.evaluation.evaluator INFO: Inference done 252/267. Dataloading: 0.0032 s/iter. Inference: 0.3924 s/iter. Eval: 3.8371 s/iter. Total: 4.2328 s/iter. ETA=0:01:03
[12/05 00:27:00] d2.evaluation.evaluator INFO: Inference done 254/267. Dataloading: 0.0032 s/iter. Inference: 0.3923 s/iter. Eval: 3.8377 s/iter. Total: 4.2334 s/iter. ETA=0:00:55
[12/05 00:27:09] d2.evaluation.evaluator INFO: Inference done 256/267. Dataloading: 0.0032 s/iter. Inference: 0.3923 s/iter. Eval: 3.8378 s/iter. Total: 4.2335 s/iter. ETA=0:00:46
[12/05 00:27:16] d2.evaluation.evaluator INFO: Inference done 258/267. Dataloading: 0.0032 s/iter. Inference: 0.3923 s/iter. Eval: 3.8334 s/iter. Total: 4.2290 s/iter. ETA=0:00:38
[12/05 00:27:23] d2.evaluation.evaluator INFO: Inference done 260/267. Dataloading: 0.0032 s/iter. Inference: 0.3923 s/iter. Eval: 3.8273 s/iter. Total: 4.2230 s/iter. ETA=0:00:29
[12/05 00:27:30] d2.evaluation.evaluator INFO: Inference done 262/267. Dataloading: 0.0032 s/iter. Inference: 0.3923 s/iter. Eval: 3.8241 s/iter. Total: 4.2198 s/iter. ETA=0:00:21
[12/05 00:27:39] d2.evaluation.evaluator INFO: Inference done 264/267. Dataloading: 0.0032 s/iter. Inference: 0.3923 s/iter. Eval: 3.8233 s/iter. Total: 4.2189 s/iter. ETA=0:00:12
[12/05 00:27:48] d2.evaluation.evaluator INFO: Inference done 266/267. Dataloading: 0.0032 s/iter. Inference: 0.3923 s/iter. Eval: 3.8270 s/iter. Total: 4.2226 s/iter. ETA=0:00:04
[12/05 00:27:52] d2.evaluation.evaluator INFO: Total inference time: 0:18:26.566546 (4.223536 s / iter per device, on 1 devices)
[12/05 00:27:52] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:01:42 (0.392305 s / iter per device, on 1 devices)
[12/05 00:27:52] d2.evaluation.panoptic_evaluation INFO: Writing all panoptic predictions to /tmp/panoptic_evalpy9naa4f ...
[12/05 00:28:00] d2.evaluation.panoptic_evaluation INFO: Panoptic Evaluation Results:
|        |   PQ   |   SQ   |   RQ   |  #categories  |
|:------:|:------:|:------:|:------:|:-------------:|
|  All   | 33.594 | 50.928 | 41.261 |      30       |
| Things | 14.603 | 32.721 | 18.792 |      19       |
| Stuff  | 66.397 | 82.375 | 80.070 |      11       |
[12/05 00:28:00] d2.evaluation.cityscapes_evaluation INFO: Evaluating results under /tmp/cityscapes_eval_axe47umm ...
[12/05 00:28:38] oneformer.evaluation.cityscapes_evaluation INFO: Evaluating results under /tmp/cityscapes_eval_a0cl4gjb ...
[12/05 00:29:10] d2.engine.hooks INFO: Overall training speed: 4997 iterations in 0:26:49 (0.3221 s / it)
[12/05 00:29:10] d2.engine.hooks INFO: Total training time: 0:47:03 (0:20:13 on hooks)
[12/05 00:29:10] d2.utils.events INFO:  eta: 0:26:38  iter: 4999  total_loss: 16.98  loss_ce: 0.1565  loss_mask: 0.3374  loss_dice: 0.8112  loss_contrastive: 0  loss_ce_0: 0.9453  loss_mask_0: 0.3912  loss_dice_0: 0.893  loss_ce_1: 0.5266  loss_mask_1: 0.3375  loss_dice_1: 0.8388  loss_ce_2: 0.2712  loss_mask_2: 0.3424  loss_dice_2: 0.7448  loss_ce_3: 0.2705  loss_mask_3: 0.3689  loss_dice_3: 0.8821  loss_ce_4: 0.2143  loss_mask_4: 0.3714  loss_dice_4: 0.9572  loss_ce_5: 0.176  loss_mask_5: 0.3855  loss_dice_5: 0.9798  loss_ce_6: 0.1839  loss_mask_6: 0.3547  loss_dice_6: 0.9782  loss_ce_7: 0.1685  loss_mask_7: 0.3263  loss_dice_7: 1.034  loss_ce_8: 0.1599  loss_mask_8: 0.3294  loss_dice_8: 0.7892    time: 0.3220  last_time: 0.3104  data_time: 0.0032  last_data_time: 0.0026   lr: 5.3598e-05  max_mem: 19574M
