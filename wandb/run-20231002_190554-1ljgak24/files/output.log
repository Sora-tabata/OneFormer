[32m[10/02 19:06:01 d2.engine.defaults]: [39mModel:
OneFormer(
  (backbone): D2ConvNeXt(
    (downsample_layers): ModuleList(
      (0): Sequential(
        (0): Conv2d(3, 256, kernel_size=(4, 4), stride=(4, 4))
        (1): LayerNorm()
      )
      (1): Sequential(
        (0): LayerNorm()
        (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))
      )
      (2): Sequential(
        (0): LayerNorm()
        (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))
      )
      (3): Sequential(
        (0): LayerNorm()
        (1): Conv2d(1024, 2048, kernel_size=(2, 2), stride=(2, 2))
      )
    )
    (stages): ModuleList(
      (0): Sequential(
        (0): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): Identity()
        )
        (1): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): DropPath()
        )
      )
      (1): Sequential(
        (0): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
          (drop_path): DropPath()
        )
      )
      (2): Sequential(
        (0): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (3): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (4): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (5): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (6): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (7): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (8): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (9): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (10): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (11): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (12): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (13): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (14): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (15): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (16): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (17): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (18): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (19): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (20): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (21): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (22): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (23): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (24): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (25): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
        (26): Block(
          (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
          (drop_path): DropPath()
        )
      )
      (3): Sequential(
        (0): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
        (1): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
        (2): Block(
          (dwconv): Conv2d(2048, 2048, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=2048)
          (norm): LayerNorm()
          (pwconv1): Linear(in_features=2048, out_features=8192, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=8192, out_features=2048, bias=True)
          (drop_path): DropPath()
        )
      )
    )
    (norm0): LayerNorm()
    (norm1): LayerNorm()
    (norm2): LayerNorm()
    (norm3): LayerNorm()
  )
  (sem_seg_head): OneFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.1, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.1, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): ContrastiveMultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (class_transformer): Transformer(
        (encoder): TransformerEncoder(
          (layers): ModuleList()
        )
        (decoder): TransformerDecoder(
          (layers): ModuleList(
            (0-1): 2 x TransformerDecoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (dropout3): Dropout(p=0.1, inplace=False)
            )
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (transformer_self_attention_layers): ModuleList(
        (0-8): 9 x SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0-8): 9 x CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0-8): 9 x FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_embed): Embedding(250, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0-2): 3 x Sequential()
      )
      (class_input_proj): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (class_embed): Linear(in_features=256, out_features=20, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (task_mlp): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=77, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
    )
  )
  (text_encoder): TextTransformer(
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=256, out_features=1024, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (token_embedding): Embedding(49408, 256)
  )
  (text_projector): MLP(
    (layers): ModuleList(
      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
    )
  )
  (prompt_ctx): Embedding(16, 256)
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks', 'contrastive']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_contrastive': 0.5, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_contrastive_0': 0.5, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_contrastive_1': 0.5, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_contrastive_2': 0.5, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_contrastive_3': 0.5, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_contrastive_4': 0.5, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_contrastive_5': 0.5, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_contrastive_6': 0.5, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_contrastive_7': 0.5, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0, 'loss_contrastive_8': 0.5}
      num_classes: 19
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[32m[10/02 19:06:01 oneformer.data.dataset_mappers.oneformer_unified_dataset_mapper]: [39m[OneFormerUnifiedDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=..., max_size=4096, sample_style='choice'), RandomCrop_CategoryAreaConstraint(crop_type='absolute', crop_size=[512, 1024], single_category_max_area=1.0, ignored_category=255), <detectron2.projects.point_rend.color_augmentation.ColorAugSSDTransform object at 0x7fbfd95e9e40>, RandomFlip()]
[32m[10/02 19:06:01 oneformer.data.datasets.register_cityscapes_panoptic]: [39m2 cities found in '/mnt/source/datasets/cityscapes/leftImg8bit/train'.
[32m[10/02 19:06:01 d2.data.build]: [39mUsing training sampler TrainingSampler
[32m[10/02 19:06:01 d2.data.common]: [39mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[32m[10/02 19:06:01 d2.data.common]: [39mSerializing 181 elements to byte tensors and concatenating them all ...
[32m[10/02 19:06:01 d2.data.common]: [39mSerialized dataset takes 0.26 MiB
[32m[10/02 19:06:01 d2.checkpoint.detection_checkpoint]: [39m[DetectionCheckpointer] Loading from detectron2://ImageNetPretrained/torchvision/R-50.pkl ...
[32m[10/02 19:06:01 fvcore.common.checkpoint]: [39m[Checkpointer] Loading from /root/.torch/iopath_cache/detectron2/ImageNetPretrained/torchvision/R-50.pkl ...
[32m[10/02 19:06:01 fvcore.common.checkpoint]: [39mReading a file from 'torchvision'
[31m[5mWARNING[39m[25m [32m[10/02 19:06:01 d2.checkpoint.c2_model_loading]: [39mNo weights in checkpoint matched with model.
[31m[5mWARNING[39m[25m [32m[10/02 19:06:01 fvcore.common.checkpoint]: [39mSome model parameters or buffers are not found in the checkpoint:
[34mbackbone.downsample_layers.0.0.{bias, weight}
[34mbackbone.downsample_layers.0.1.{bias, weight}
[34mbackbone.downsample_layers.1.0.{bias, weight}
[34mbackbone.downsample_layers.1.1.{bias, weight}
[34mbackbone.downsample_layers.2.0.{bias, weight}
[34mbackbone.downsample_layers.2.1.{bias, weight}
[34mbackbone.downsample_layers.3.0.{bias, weight}
[34mbackbone.downsample_layers.3.1.{bias, weight}
[34mbackbone.norm0.{bias, weight}
[34mbackbone.norm1.{bias, weight}
[34mbackbone.norm2.{bias, weight}
[34mbackbone.norm3.{bias, weight}
[34mbackbone.stages.0.0.dwconv.{bias, weight}
[34mbackbone.stages.0.0.gamma
[34mbackbone.stages.0.0.norm.{bias, weight}
[34mbackbone.stages.0.0.pwconv1.{bias, weight}
[34mbackbone.stages.0.0.pwconv2.{bias, weight}
[34mbackbone.stages.0.1.dwconv.{bias, weight}
[34mbackbone.stages.0.1.gamma
[34mbackbone.stages.0.1.norm.{bias, weight}
[34mbackbone.stages.0.1.pwconv1.{bias, weight}
[34mbackbone.stages.0.1.pwconv2.{bias, weight}
[34mbackbone.stages.0.2.dwconv.{bias, weight}
[34mbackbone.stages.0.2.gamma
[34mbackbone.stages.0.2.norm.{bias, weight}
[34mbackbone.stages.0.2.pwconv1.{bias, weight}
[34mbackbone.stages.0.2.pwconv2.{bias, weight}
[34mbackbone.stages.1.0.dwconv.{bias, weight}
[34mbackbone.stages.1.0.gamma
[34mbackbone.stages.1.0.norm.{bias, weight}
[34mbackbone.stages.1.0.pwconv1.{bias, weight}
[34mbackbone.stages.1.0.pwconv2.{bias, weight}
[34mbackbone.stages.1.1.dwconv.{bias, weight}
[34mbackbone.stages.1.1.gamma
[34mbackbone.stages.1.1.norm.{bias, weight}
[34mbackbone.stages.1.1.pwconv1.{bias, weight}
[34mbackbone.stages.1.1.pwconv2.{bias, weight}
[34mbackbone.stages.1.2.dwconv.{bias, weight}
[34mbackbone.stages.1.2.gamma
[34mbackbone.stages.1.2.norm.{bias, weight}
[34mbackbone.stages.1.2.pwconv1.{bias, weight}
[34mbackbone.stages.1.2.pwconv2.{bias, weight}
[34mbackbone.stages.2.0.dwconv.{bias, weight}
[34mbackbone.stages.2.0.gamma
[34mbackbone.stages.2.0.norm.{bias, weight}
[34mbackbone.stages.2.0.pwconv1.{bias, weight}
[34mbackbone.stages.2.0.pwconv2.{bias, weight}
[34mbackbone.stages.2.1.dwconv.{bias, weight}
[34mbackbone.stages.2.1.gamma
[34mbackbone.stages.2.1.norm.{bias, weight}
[34mbackbone.stages.2.1.pwconv1.{bias, weight}
[34mbackbone.stages.2.1.pwconv2.{bias, weight}
[34mbackbone.stages.2.10.dwconv.{bias, weight}
[34mbackbone.stages.2.10.gamma
[34mbackbone.stages.2.10.norm.{bias, weight}
[34mbackbone.stages.2.10.pwconv1.{bias, weight}
[34mbackbone.stages.2.10.pwconv2.{bias, weight}
[34mbackbone.stages.2.11.dwconv.{bias, weight}
[34mbackbone.stages.2.11.gamma
[34mbackbone.stages.2.11.norm.{bias, weight}
[34mbackbone.stages.2.11.pwconv1.{bias, weight}
[34mbackbone.stages.2.11.pwconv2.{bias, weight}
[34mbackbone.stages.2.12.dwconv.{bias, weight}
[34mbackbone.stages.2.12.gamma
[34mbackbone.stages.2.12.norm.{bias, weight}
[34mbackbone.stages.2.12.pwconv1.{bias, weight}
[34mbackbone.stages.2.12.pwconv2.{bias, weight}
[34mbackbone.stages.2.13.dwconv.{bias, weight}
[34mbackbone.stages.2.13.gamma
[34mbackbone.stages.2.13.norm.{bias, weight}
[34mbackbone.stages.2.13.pwconv1.{bias, weight}
[34mbackbone.stages.2.13.pwconv2.{bias, weight}
[34mbackbone.stages.2.14.dwconv.{bias, weight}
[34mbackbone.stages.2.14.gamma
[34mbackbone.stages.2.14.norm.{bias, weight}
[34mbackbone.stages.2.14.pwconv1.{bias, weight}
[34mbackbone.stages.2.14.pwconv2.{bias, weight}
[34mbackbone.stages.2.15.dwconv.{bias, weight}
[34mbackbone.stages.2.15.gamma
[34mbackbone.stages.2.15.norm.{bias, weight}
[34mbackbone.stages.2.15.pwconv1.{bias, weight}
[34mbackbone.stages.2.15.pwconv2.{bias, weight}
[34mbackbone.stages.2.16.dwconv.{bias, weight}
[34mbackbone.stages.2.16.gamma
[34mbackbone.stages.2.16.norm.{bias, weight}
[34mbackbone.stages.2.16.pwconv1.{bias, weight}
[34mbackbone.stages.2.16.pwconv2.{bias, weight}
[34mbackbone.stages.2.17.dwconv.{bias, weight}
[34mbackbone.stages.2.17.gamma
[34mbackbone.stages.2.17.norm.{bias, weight}
[34mbackbone.stages.2.17.pwconv1.{bias, weight}
[34mbackbone.stages.2.17.pwconv2.{bias, weight}
[34mbackbone.stages.2.18.dwconv.{bias, weight}
[34mbackbone.stages.2.18.gamma
[34mbackbone.stages.2.18.norm.{bias, weight}
[34mbackbone.stages.2.18.pwconv1.{bias, weight}
[34mbackbone.stages.2.18.pwconv2.{bias, weight}
[34mbackbone.stages.2.19.dwconv.{bias, weight}
[34mbackbone.stages.2.19.gamma
[34mbackbone.stages.2.19.norm.{bias, weight}
[34mbackbone.stages.2.19.pwconv1.{bias, weight}
[34mbackbone.stages.2.19.pwconv2.{bias, weight}
[34mbackbone.stages.2.2.dwconv.{bias, weight}
[34mbackbone.stages.2.2.gamma
[34mbackbone.stages.2.2.norm.{bias, weight}
[34mbackbone.stages.2.2.pwconv1.{bias, weight}
[34mbackbone.stages.2.2.pwconv2.{bias, weight}
[34mbackbone.stages.2.20.dwconv.{bias, weight}
[34mbackbone.stages.2.20.gamma
[34mbackbone.stages.2.20.norm.{bias, weight}
[34mbackbone.stages.2.20.pwconv1.{bias, weight}
[34mbackbone.stages.2.20.pwconv2.{bias, weight}
[34mbackbone.stages.2.21.dwconv.{bias, weight}
[34mbackbone.stages.2.21.gamma
[34mbackbone.stages.2.21.norm.{bias, weight}
[34mbackbone.stages.2.21.pwconv1.{bias, weight}
[34mbackbone.stages.2.21.pwconv2.{bias, weight}
[34mbackbone.stages.2.22.dwconv.{bias, weight}
[34mbackbone.stages.2.22.gamma
[34mbackbone.stages.2.22.norm.{bias, weight}
[34mbackbone.stages.2.22.pwconv1.{bias, weight}
[34mbackbone.stages.2.22.pwconv2.{bias, weight}
[34mbackbone.stages.2.23.dwconv.{bias, weight}
[34mbackbone.stages.2.23.gamma
[34mbackbone.stages.2.23.norm.{bias, weight}
[34mbackbone.stages.2.23.pwconv1.{bias, weight}
[34mbackbone.stages.2.23.pwconv2.{bias, weight}
[34mbackbone.stages.2.24.dwconv.{bias, weight}
[34mbackbone.stages.2.24.gamma
[34mbackbone.stages.2.24.norm.{bias, weight}
[34mbackbone.stages.2.24.pwconv1.{bias, weight}
[34mbackbone.stages.2.24.pwconv2.{bias, weight}
[34mbackbone.stages.2.25.dwconv.{bias, weight}
[34mbackbone.stages.2.25.gamma
[34mbackbone.stages.2.25.norm.{bias, weight}
[34mbackbone.stages.2.25.pwconv1.{bias, weight}
[34mbackbone.stages.2.25.pwconv2.{bias, weight}
[34mbackbone.stages.2.26.dwconv.{bias, weight}
[34mbackbone.stages.2.26.gamma
[34mbackbone.stages.2.26.norm.{bias, weight}
[34mbackbone.stages.2.26.pwconv1.{bias, weight}
[34mbackbone.stages.2.26.pwconv2.{bias, weight}
[34mbackbone.stages.2.3.dwconv.{bias, weight}
[34mbackbone.stages.2.3.gamma
[34mbackbone.stages.2.3.norm.{bias, weight}
[34mbackbone.stages.2.3.pwconv1.{bias, weight}
[34mbackbone.stages.2.3.pwconv2.{bias, weight}
[34mbackbone.stages.2.4.dwconv.{bias, weight}
[34mbackbone.stages.2.4.gamma
[34mbackbone.stages.2.4.norm.{bias, weight}
[34mbackbone.stages.2.4.pwconv1.{bias, weight}
[34mbackbone.stages.2.4.pwconv2.{bias, weight}
[34mbackbone.stages.2.5.dwconv.{bias, weight}
[34mbackbone.stages.2.5.gamma
[34mbackbone.stages.2.5.norm.{bias, weight}
[34mbackbone.stages.2.5.pwconv1.{bias, weight}
[34mbackbone.stages.2.5.pwconv2.{bias, weight}
[34mbackbone.stages.2.6.dwconv.{bias, weight}
[34mbackbone.stages.2.6.gamma
[34mbackbone.stages.2.6.norm.{bias, weight}
[34mbackbone.stages.2.6.pwconv1.{bias, weight}
[34mbackbone.stages.2.6.pwconv2.{bias, weight}
[34mbackbone.stages.2.7.dwconv.{bias, weight}
[34mbackbone.stages.2.7.gamma
[34mbackbone.stages.2.7.norm.{bias, weight}
[34mbackbone.stages.2.7.pwconv1.{bias, weight}
[34mbackbone.stages.2.7.pwconv2.{bias, weight}
[34mbackbone.stages.2.8.dwconv.{bias, weight}
[34mbackbone.stages.2.8.gamma
[34mbackbone.stages.2.8.norm.{bias, weight}
[34mbackbone.stages.2.8.pwconv1.{bias, weight}
[34mbackbone.stages.2.8.pwconv2.{bias, weight}
[34mbackbone.stages.2.9.dwconv.{bias, weight}
[34mbackbone.stages.2.9.gamma
[34mbackbone.stages.2.9.norm.{bias, weight}
[34mbackbone.stages.2.9.pwconv1.{bias, weight}
[34mbackbone.stages.2.9.pwconv2.{bias, weight}
[34mbackbone.stages.3.0.dwconv.{bias, weight}
[34mbackbone.stages.3.0.gamma
[34mbackbone.stages.3.0.norm.{bias, weight}
[34mbackbone.stages.3.0.pwconv1.{bias, weight}
[34mbackbone.stages.3.0.pwconv2.{bias, weight}
[34mbackbone.stages.3.1.dwconv.{bias, weight}
[34mbackbone.stages.3.1.gamma
[34mbackbone.stages.3.1.norm.{bias, weight}
[34mbackbone.stages.3.1.pwconv1.{bias, weight}
[34mbackbone.stages.3.1.pwconv2.{bias, weight}
[34mbackbone.stages.3.2.dwconv.{bias, weight}
[34mbackbone.stages.3.2.gamma
[34mbackbone.stages.3.2.norm.{bias, weight}
[34mbackbone.stages.3.2.pwconv1.{bias, weight}
[34mbackbone.stages.3.2.pwconv2.{bias, weight}
[34mcriterion.{empty_weight, logit_scale}
[34mprompt_ctx.weight
[34msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}
[34msem_seg_head.pixel_decoder.adapter_1.weight
[34msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}
[34msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}
[34msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}
[34msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}
[34msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}
[34msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}
[34msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}
[34msem_seg_head.pixel_decoder.layer_1.weight
[34msem_seg_head.pixel_decoder.mask_features.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}
[34msem_seg_head.pixel_decoder.transformer.level_embed
[34msem_seg_head.predictor.class_embed.{bias, weight}
[34msem_seg_head.predictor.class_input_proj.{bias, weight}
[34msem_seg_head.predictor.class_transformer.decoder.layers.0.linear1.{bias, weight}
[34msem_seg_head.predictor.class_transformer.decoder.layers.0.linear2.{bias, weight}
[34msem_seg_head.predictor.class_transformer.decoder.layers.0.multihead_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.class_transformer.decoder.layers.0.multihead_attn.{in_proj_bias, in_proj_weight}
[34msem_seg_head.predictor.class_transformer.decoder.layers.0.norm1.{bias, weight}
[34msem_seg_head.predictor.class_transformer.decoder.layers.0.norm2.{bias, weight}
[34msem_seg_head.predictor.class_transformer.decoder.layers.0.norm3.{bias, weight}
[34msem_seg_head.predictor.class_transformer.decoder.layers.0.self_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.class_transformer.decoder.layers.0.self_attn.{in_proj_bias, in_proj_weight}
[34msem_seg_head.predictor.class_transformer.decoder.layers.1.linear1.{bias, weight}
[34msem_seg_head.predictor.class_transformer.decoder.layers.1.linear2.{bias, weight}
[34msem_seg_head.predictor.class_transformer.decoder.layers.1.multihead_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.class_transformer.decoder.layers.1.multihead_attn.{in_proj_bias, in_proj_weight}
[34msem_seg_head.predictor.class_transformer.decoder.layers.1.norm1.{bias, weight}
[34msem_seg_head.predictor.class_transformer.decoder.layers.1.norm2.{bias, weight}
[34msem_seg_head.predictor.class_transformer.decoder.layers.1.norm3.{bias, weight}
[34msem_seg_head.predictor.class_transformer.decoder.layers.1.self_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.class_transformer.decoder.layers.1.self_attn.{in_proj_bias, in_proj_weight}
[34msem_seg_head.predictor.class_transformer.decoder.norm.{bias, weight}
[34msem_seg_head.predictor.decoder_norm.{bias, weight}
[34msem_seg_head.predictor.level_embed.weight
[34msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}
[34msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}
[34msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}
[34msem_seg_head.predictor.query_embed.weight
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}
[34msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}
[34msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}
[34msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}
[34mtask_mlp.layers.0.{bias, weight}
[34mtask_mlp.layers.1.{bias, weight}
[34mtext_encoder.ln_final.{bias, weight}
[34mtext_encoder.positional_embedding
[34mtext_encoder.token_embedding.weight
[34mtext_encoder.transformer.resblocks.0.attn.out_proj.{bias, weight}
[34mtext_encoder.transformer.resblocks.0.attn.{in_proj_bias, in_proj_weight}
[34mtext_encoder.transformer.resblocks.0.ln_1.{bias, weight}
[34mtext_encoder.transformer.resblocks.0.ln_2.{bias, weight}
[34mtext_encoder.transformer.resblocks.0.mlp.c_fc.{bias, weight}
[34mtext_encoder.transformer.resblocks.0.mlp.c_proj.{bias, weight}
[34mtext_encoder.transformer.resblocks.1.attn.out_proj.{bias, weight}
[34mtext_encoder.transformer.resblocks.1.attn.{in_proj_bias, in_proj_weight}
[34mtext_encoder.transformer.resblocks.1.ln_1.{bias, weight}
[34mtext_encoder.transformer.resblocks.1.ln_2.{bias, weight}
[34mtext_encoder.transformer.resblocks.1.mlp.c_fc.{bias, weight}
[34mtext_encoder.transformer.resblocks.1.mlp.c_proj.{bias, weight}
[34mtext_encoder.transformer.resblocks.2.attn.out_proj.{bias, weight}
[34mtext_encoder.transformer.resblocks.2.attn.{in_proj_bias, in_proj_weight}
[34mtext_encoder.transformer.resblocks.2.ln_1.{bias, weight}
[34mtext_encoder.transformer.resblocks.2.ln_2.{bias, weight}
[34mtext_encoder.transformer.resblocks.2.mlp.c_fc.{bias, weight}
[34mtext_encoder.transformer.resblocks.2.mlp.c_proj.{bias, weight}
[34mtext_encoder.transformer.resblocks.3.attn.out_proj.{bias, weight}
[34mtext_encoder.transformer.resblocks.3.attn.{in_proj_bias, in_proj_weight}
[34mtext_encoder.transformer.resblocks.3.ln_1.{bias, weight}
[34mtext_encoder.transformer.resblocks.3.ln_2.{bias, weight}
[34mtext_encoder.transformer.resblocks.3.mlp.c_fc.{bias, weight}
[34mtext_encoder.transformer.resblocks.3.mlp.c_proj.{bias, weight}
[34mtext_encoder.transformer.resblocks.4.attn.out_proj.{bias, weight}
[34mtext_encoder.transformer.resblocks.4.attn.{in_proj_bias, in_proj_weight}
[34mtext_encoder.transformer.resblocks.4.ln_1.{bias, weight}
[34mtext_encoder.transformer.resblocks.4.ln_2.{bias, weight}
[34mtext_encoder.transformer.resblocks.4.mlp.c_fc.{bias, weight}
[34mtext_encoder.transformer.resblocks.4.mlp.c_proj.{bias, weight}
[34mtext_encoder.transformer.resblocks.5.attn.out_proj.{bias, weight}
[34mtext_encoder.transformer.resblocks.5.attn.{in_proj_bias, in_proj_weight}
[34mtext_encoder.transformer.resblocks.5.ln_1.{bias, weight}
[34mtext_encoder.transformer.resblocks.5.ln_2.{bias, weight}
[34mtext_encoder.transformer.resblocks.5.mlp.c_fc.{bias, weight}
[34mtext_encoder.transformer.resblocks.5.mlp.c_proj.{bias, weight}
[34mtext_projector.layers.0.{bias, weight}
[34mtext_projector.layers.1.{bias, weight}
[31m[5mWARNING[39m[25m [32m[10/02 19:06:01 fvcore.common.checkpoint]: [39mThe checkpoint state_dict contains keys that are not used by the model:
  [35mstem.conv1.weight
  [35mstem.conv1.norm.{bias, running_mean, running_var, weight}
  [35mres2.0.conv1.weight
  [35mres2.0.conv1.norm.{bias, running_mean, running_var, weight}
  [35mres2.0.conv2.weight
  [35mres2.0.conv2.norm.{bias, running_mean, running_var, weight}
  [35mres2.0.conv3.weight
  [35mres2.0.conv3.norm.{bias, running_mean, running_var, weight}
  [35mres2.0.shortcut.weight
  [35mres2.0.shortcut.norm.{bias, running_mean, running_var, weight}
  [35mres2.1.conv1.weight
  [35mres2.1.conv1.norm.{bias, running_mean, running_var, weight}
  [35mres2.1.conv2.weight
  [35mres2.1.conv2.norm.{bias, running_mean, running_var, weight}
  [35mres2.1.conv3.weight
  [35mres2.1.conv3.norm.{bias, running_mean, running_var, weight}
  [35mres2.2.conv1.weight
  [35mres2.2.conv1.norm.{bias, running_mean, running_var, weight}
  [35mres2.2.conv2.weight
  [35mres2.2.conv2.norm.{bias, running_mean, running_var, weight}
  [35mres2.2.conv3.weight
  [35mres2.2.conv3.norm.{bias, running_mean, running_var, weight}
  [35mres3.0.conv1.weight
  [35mres3.0.conv1.norm.{bias, running_mean, running_var, weight}
  [35mres3.0.conv2.weight
  [35mres3.0.conv2.norm.{bias, running_mean, running_var, weight}
  [35mres3.0.conv3.weight
  [35mres3.0.conv3.norm.{bias, running_mean, running_var, weight}
  [35mres3.0.shortcut.weight
  [35mres3.0.shortcut.norm.{bias, running_mean, running_var, weight}
  [35mres3.1.conv1.weight
  [35mres3.1.conv1.norm.{bias, running_mean, running_var, weight}
  [35mres3.1.conv2.weight
  [35mres3.1.conv2.norm.{bias, running_mean, running_var, weight}
  [35mres3.1.conv3.weight
  [35mres3.1.conv3.norm.{bias, running_mean, running_var, weight}
  [35mres3.2.conv1.weight
  [35mres3.2.conv1.norm.{bias, running_mean, running_var, weight}
  [35mres3.2.conv2.weight
  [35mres3.2.conv2.norm.{bias, running_mean, running_var, weight}
  [35mres3.2.conv3.weight
  [35mres3.2.conv3.norm.{bias, running_mean, running_var, weight}
  [35mres3.3.conv1.weight
  [35mres3.3.conv1.norm.{bias, running_mean, running_var, weight}
  [35mres3.3.conv2.weight
  [35mres3.3.conv2.norm.{bias, running_mean, running_var, weight}
  [35mres3.3.conv3.weight
  [35mres3.3.conv3.norm.{bias, running_mean, running_var, weight}
  [35mres4.0.conv1.weight
  [35mres4.0.conv1.norm.{bias, running_mean, running_var, weight}
  [35mres4.0.conv2.weight
  [35mres4.0.conv2.norm.{bias, running_mean, running_var, weight}
  [35mres4.0.conv3.weight
  [35mres4.0.conv3.norm.{bias, running_mean, running_var, weight}
  [35mres4.0.shortcut.weight
  [35mres4.0.shortcut.norm.{bias, running_mean, running_var, weight}
  [35mres4.1.conv1.weight
  [35mres4.1.conv1.norm.{bias, running_mean, running_var, weight}
  [35mres4.1.conv2.weight
  [35mres4.1.conv2.norm.{bias, running_mean, running_var, weight}
  [35mres4.1.conv3.weight
  [35mres4.1.conv3.norm.{bias, running_mean, running_var, weight}
  [35mres4.2.conv1.weight
  [35mres4.2.conv1.norm.{bias, running_mean, running_var, weight}
  [35mres4.2.conv2.weight
  [35mres4.2.conv2.norm.{bias, running_mean, running_var, weight}
  [35mres4.2.conv3.weight
  [35mres4.2.conv3.norm.{bias, running_mean, running_var, weight}
  [35mres4.3.conv1.weight
  [35mres4.3.conv1.norm.{bias, running_mean, running_var, weight}
  [35mres4.3.conv2.weight
  [35mres4.3.conv2.norm.{bias, running_mean, running_var, weight}
  [35mres4.3.conv3.weight
  [35mres4.3.conv3.norm.{bias, running_mean, running_var, weight}
  [35mres4.4.conv1.weight
  [35mres4.4.conv1.norm.{bias, running_mean, running_var, weight}
  [35mres4.4.conv2.weight
  [35mres4.4.conv2.norm.{bias, running_mean, running_var, weight}
  [35mres4.4.conv3.weight
  [35mres4.4.conv3.norm.{bias, running_mean, running_var, weight}
  [35mres4.5.conv1.weight
  [35mres4.5.conv1.norm.{bias, running_mean, running_var, weight}
  [35mres4.5.conv2.weight
  [35mres4.5.conv2.norm.{bias, running_mean, running_var, weight}
  [35mres4.5.conv3.weight
  [35mres4.5.conv3.norm.{bias, running_mean, running_var, weight}
  [35mres5.0.conv1.weight
  [35mres5.0.conv1.norm.{bias, running_mean, running_var, weight}
  [35mres5.0.conv2.weight
  [35mres5.0.conv2.norm.{bias, running_mean, running_var, weight}
  [35mres5.0.conv3.weight
  [35mres5.0.conv3.norm.{bias, running_mean, running_var, weight}
  [35mres5.0.shortcut.weight
  [35mres5.0.shortcut.norm.{bias, running_mean, running_var, weight}
  [35mres5.1.conv1.weight
  [35mres5.1.conv1.norm.{bias, running_mean, running_var, weight}
  [35mres5.1.conv2.weight
  [35mres5.1.conv2.norm.{bias, running_mean, running_var, weight}
  [35mres5.1.conv3.weight
  [35mres5.1.conv3.norm.{bias, running_mean, running_var, weight}
  [35mres5.2.conv1.weight
  [35mres5.2.conv1.norm.{bias, running_mean, running_var, weight}
  [35mres5.2.conv2.weight
  [35mres5.2.conv2.norm.{bias, running_mean, running_var, weight}
  [35mres5.2.conv3.weight
  [35mres5.2.conv3.norm.{bias, running_mean, running_var, weight}
  [35mstem.fc.{bias, weight}
Total Params: 389.516501 M
[32m[10/02 19:06:04 d2.engine.train_loop]: [39mStarting training from iteration 0
/opt/miniconda3/envs/oneformer/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/opt/miniconda3/envs/oneformer/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/opt/miniconda3/envs/oneformer/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/opt/miniconda3/envs/oneformer/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/opt/miniconda3/envs/oneformer/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/miniconda3/envs/oneformer/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/opt/miniconda3/envs/oneformer/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/miniconda3/envs/oneformer/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
[31m[4m[5mERROR[39m[24m[25m [32m[10/02 19:06:09 d2.engine.train_loop]: [39mException during training:
Traceback (most recent call last):
  File "/OneFormer/detectron2/detectron2/engine/train_loop.py", line 155, in train
    self.run_step()
  File "/OneFormer/detectron2/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/OneFormer/detectron2/detectron2/engine/train_loop.py", line 492, in run_step
    loss_dict = self.model(data)
  File "/opt/miniconda3/envs/oneformer/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/OneFormer/oneformer/oneformer_model.py", line 296, in forward
    losses = self.criterion(outputs, targets)
  File "/opt/miniconda3/envs/oneformer/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/OneFormer/oneformer/modeling/criterion.py", line 287, in forward
    indices = self.matcher(outputs_without_aux, targets)
  File "/opt/miniconda3/envs/oneformer/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/miniconda3/envs/oneformer/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/OneFormer/oneformer/modeling/matcher.py", line 202, in forward
    return self.memory_efficient_forward(outputs, targets)
  File "/opt/miniconda3/envs/oneformer/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/OneFormer/oneformer/modeling/matcher.py", line 171, in memory_efficient_forward
    C = C.reshape(num_queries, -1).cpu()
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[32m[10/02 19:06:09 d2.engine.hooks]: [39mOverall training speed: 2 iterations in 0:00:01 (0.6349 s / it)
[32m[10/02 19:06:09 d2.engine.hooks]: [39mTotal training time: 0:00:01 (0:00:00 on hooks)
[32m[10/02 19:06:09 d2.utils.events]: [39m eta: 1:36:40  iter: 4  total_loss: 121.8  loss_ce: 4.972  loss_mask: 2.62  loss_dice: 4.182  loss_contrastive: 0  loss_ce_0: 5.163  loss_mask_0: 1.645  loss_dice_0: 4.257  loss_ce_1: 5.6  loss_mask_1: 1.797  loss_dice_1: 4.219  loss_ce_2: 5.538  loss_mask_2: 2.122  loss_dice_2: 4.298  loss_ce_3: 5.061  loss_mask_3: 2.143  loss_dice_3: 4.207  loss_ce_4: 5.122  loss_mask_4: 2.398  loss_dice_4: 4.302  loss_ce_5: 7.164  loss_mask_5: 2.066  loss_dice_5: 4.333  loss_ce_6: 6.912  loss_mask_6: 2.343  loss_dice_6: 4.318  loss_ce_7: 4.718  loss_mask_7: 3.16  loss_dice_7: 4.159  loss_ce_8: 5.183  loss_mask_8: 3.014  loss_dice_8: 4.266    time: 0.5804  last_time: 0.2289  data_time: 0.1218  last_data_time: 0.0013   lr: 9.9973e-05  max_mem: 11455M
/opt/miniconda3/envs/oneformer/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [37,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [38,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [46,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [47,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [55,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [56,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [2,0,0], thread: [64,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [2,0,0], thread: [72,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [2,0,0], thread: [73,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [2,0,0], thread: [81,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [2,0,0], thread: [82,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [2,0,0], thread: [90,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [2,0,0], thread: [91,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [64,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [65,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [73,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [74,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [82,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [83,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [91,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [92,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [2,0,0], thread: [0,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [2,0,0], thread: [1,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [2,0,0], thread: [9,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [2,0,0], thread: [10,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [2,0,0], thread: [18,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [2,0,0], thread: [19,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [2,0,0], thread: [27,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [2,0,0], thread: [28,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [1,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [2,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [10,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [11,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [19,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [20,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [28,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [29,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [100,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [101,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [109,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [110,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [118,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [119,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [127,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [2,0,0], thread: [36,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [2,0,0], thread: [37,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [2,0,0], thread: [45,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [2,0,0], thread: [46,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [2,0,0], thread: [54,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [2,0,0], thread: [55,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [2,0,0], thread: [63,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [2,0,0], thread: [99,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [2,0,0], thread: [100,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [2,0,0], thread: [108,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [2,0,0], thread: [109,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [2,0,0], thread: [117,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [2,0,0], thread: [118,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [2,0,0], thread: [126,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [2,0,0], thread: [127,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [38,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [39,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [47,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [48,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [56,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [57,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [101,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [102,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [110,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [111,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [119,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [120,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [1,0,0], thread: [71,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [1,0,0], thread: [72,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [1,0,0], thread: [80,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [1,0,0], thread: [81,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [1,0,0], thread: [89,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [1,0,0], thread: [90,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [1,0,0], thread: [0,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [1,0,0], thread: [8,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [1,0,0], thread: [9,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [1,0,0], thread: [17,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [1,0,0], thread: [18,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [1,0,0], thread: [26,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [1,0,0], thread: [27,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [2,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [3,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [11,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [12,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [20,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [21,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [29,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [30,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [65,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [66,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [74,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [75,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [83,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [84,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [92,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [4,0,0], thread: [93,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [1,0,0], thread: [98,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [1,0,0], thread: [99,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [1,0,0], thread: [107,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [1,0,0], thread: [108,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [1,0,0], thread: [116,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [1,0,0], thread: [117,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [1,0,0], thread: [125,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [1,0,0], thread: [126,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [1,0,0], thread: [35,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [1,0,0], thread: [36,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [1,0,0], thread: [44,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [1,0,0], thread: [45,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [1,0,0], thread: [53,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [1,0,0], thread: [54,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [1,0,0], thread: [62,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [1,0,0], thread: [63,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [70,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [71,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [79,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [80,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [88,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [89,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [7,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [8,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [16,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [17,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [25,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [26,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [97,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [98,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [106,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [107,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [115,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [116,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [124,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [125,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [34,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [35,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [43,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [44,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [52,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [53,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [61,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
/opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [0,0,0], thread: [62,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.